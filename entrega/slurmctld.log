[2024-05-14T13:18:55.079] error: chdir(/var/log): Permission denied
[2024-05-14T13:18:55.079] Job accounting information stored, but details not gathered
[2024-05-14T13:18:55.080] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T13:18:55.106] No memory enforcing mechanism configured.
[2024-05-14T13:18:55.106] layouts: no layout to initialize
[2024-05-14T13:18:55.137] layouts: loading entities/relations information
[2024-05-14T13:18:55.137] error: Could not open node state file /var/spool/slurm/ctld/node_state: No such file or directory
[2024-05-14T13:18:55.137] error: NOTE: Trying backup state save file. Information may be lost!
[2024-05-14T13:18:55.137] No node state file (/var/spool/slurm/ctld/node_state.old) to recover
[2024-05-14T13:18:55.138] error: Could not open job state file /var/spool/slurm/ctld/job_state: No such file or directory
[2024-05-14T13:18:55.138] error: NOTE: Trying backup state save file. Jobs may be lost!
[2024-05-14T13:18:55.138] No job state file (/var/spool/slurm/ctld/job_state.old) to recover
[2024-05-14T13:18:55.138] error: Could not open reservation state file /var/spool/slurm/ctld/resv_state: No such file or directory
[2024-05-14T13:18:55.138] error: NOTE: Trying backup state save file. Reservations may be lost
[2024-05-14T13:18:55.138] No reservation state file (/var/spool/slurm/ctld/resv_state.old) to recover
[2024-05-14T13:18:55.138] error: Could not open trigger state file /var/spool/slurm/ctld/trigger_state: No such file or directory
[2024-05-14T13:18:55.138] error: NOTE: Trying backup state save file. Triggers may be lost!
[2024-05-14T13:18:55.138] No trigger state file (/var/spool/slurm/ctld/trigger_state.old) to recover
[2024-05-14T13:18:55.138] _preserve_plugins: backup_controller not specified
[2024-05-14T13:18:55.138] Reinitializing job accounting state
[2024-05-14T13:18:55.138] Running as primary controller
[2024-05-14T13:18:55.139] No parameter for mcs plugin, default values set
[2024-05-14T13:18:55.139] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T13:19:47.578] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T13:19:47.633] Saving all slurm state
[2024-05-14T13:19:47.636] error: Could not open job state file /var/spool/slurm/ctld/job_state: No such file or directory
[2024-05-14T13:19:47.636] error: NOTE: Trying backup state save file. Jobs may be lost!
[2024-05-14T13:19:47.636] No job state file (/var/spool/slurm/ctld/job_state.old) found
[2024-05-14T13:19:48.175] layouts: all layouts are now unloaded.
[2024-05-14T13:20:25.335] error: chdir(/var/log): Permission denied
[2024-05-14T13:20:25.344] Job accounting information stored, but details not gathered
[2024-05-14T13:20:25.346] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T13:20:25.424] No memory enforcing mechanism configured.
[2024-05-14T13:20:25.424] layouts: no layout to initialize
[2024-05-14T13:20:25.441] layouts: loading entities/relations information
[2024-05-14T13:20:25.442] Recovered state of 2 nodes
[2024-05-14T13:20:25.443] Recovered information about 0 jobs
[2024-05-14T13:20:25.444] Recovered state of 0 reservations
[2024-05-14T13:20:25.445] _preserve_plugins: backup_controller not specified
[2024-05-14T13:20:25.445] Running as primary controller
[2024-05-14T13:20:25.446] No parameter for mcs plugin, default values set
[2024-05-14T13:20:25.446] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T13:21:25.490] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T13:25:25.640] error: Nodes compute[00-01] not responding
[2024-05-14T13:27:09.665] error: Nodes compute[00-01] not responding, setting DOWN
[2024-05-14T14:15:55.506] killing old slurmctld[1212]
[2024-05-14T14:15:55.506] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T14:15:55.506] Saving all slurm state
[2024-05-14T14:15:55.592] layouts: all layouts are now unloaded.
[2024-05-14T14:15:55.594] error: chdir(/var/log): Permission denied
[2024-05-14T14:15:55.594] Job accounting information stored, but details not gathered
[2024-05-14T14:15:55.594] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:15:55.612] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:15:55.612] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:15:55.612] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:16:16.624] error: chdir(/var/log): Permission denied
[2024-05-14T14:16:16.624] Job accounting information stored, but details not gathered
[2024-05-14T14:16:16.624] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:16:16.626] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:16:16.639] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:16:16.639] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:22:21.294] error: chdir(/var/log): Permission denied
[2024-05-14T14:22:21.294] Job accounting information stored, but details not gathered
[2024-05-14T14:22:21.294] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:22:21.295] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:22:21.296] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:22:21.296] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:22:45.864] error: chdir(/var/log): Permission denied
[2024-05-14T14:22:45.864] Job accounting information stored, but details not gathered
[2024-05-14T14:22:45.864] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:22:45.864] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:22:45.865] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:22:45.865] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:26:20.048] error: chdir(/var/log): Permission denied
[2024-05-14T14:26:20.050] Job accounting information stored, but details not gathered
[2024-05-14T14:26:20.051] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:26:20.055] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:26:20.069] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:26:20.069] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:43:04.782] error: chdir(/var/log): Permission denied
[2024-05-14T14:43:04.783] Job accounting information stored, but details not gathered
[2024-05-14T14:43:04.783] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:43:04.811] No memory enforcing mechanism configured.
[2024-05-14T14:43:04.811] layouts: no layout to initialize
[2024-05-14T14:43:04.821] layouts: loading entities/relations information
[2024-05-14T14:43:04.822] Recovered state of 2 nodes
[2024-05-14T14:43:04.822] Down nodes: compute[00-01]
[2024-05-14T14:43:04.822] Recovered information about 0 jobs
[2024-05-14T14:43:04.823] Recovered state of 0 reservations
[2024-05-14T14:43:04.824] _preserve_plugins: backup_controller not specified
[2024-05-14T14:43:04.824] Running as primary controller
[2024-05-14T14:43:04.825] No parameter for mcs plugin, default values set
[2024-05-14T14:43:04.825] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T14:44:04.483] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T14:49:35.061] killing old slurmctld[2739]
[2024-05-14T14:49:35.061] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T14:49:35.064] Saving all slurm state
[2024-05-14T14:49:35.101] layouts: all layouts are now unloaded.
[2024-05-14T14:49:35.103] error: chdir(/var/log): Permission denied
[2024-05-14T14:49:35.103] Job accounting information stored, but details not gathered
[2024-05-14T14:49:35.103] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:49:35.103] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:49:35.104] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:49:35.104] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:51:08.374] error: chdir(/var/log): Permission denied
[2024-05-14T14:51:08.375] Job accounting information stored, but details not gathered
[2024-05-14T14:51:08.375] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:51:08.375] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:51:08.377] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:51:08.377] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:51:19.924] error: chdir(/var/log): Permission denied
[2024-05-14T14:51:19.924] Job accounting information stored, but details not gathered
[2024-05-14T14:51:19.924] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:51:19.925] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:51:19.925] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:51:19.925] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:52:47.531] error: chdir(/var/log): Permission denied
[2024-05-14T14:52:47.531] Job accounting information stored, but details not gathered
[2024-05-14T14:52:47.531] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:52:47.531] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:52:47.532] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:52:47.532] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:53:14.592] error: chdir(/var/log): Permission denied
[2024-05-14T14:53:14.592] Job accounting information stored, but details not gathered
[2024-05-14T14:53:14.592] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:53:14.592] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:53:14.593] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:53:14.593] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:54:21.497] error: chdir(/var/log): Permission denied
[2024-05-14T14:54:21.498] Job accounting information stored, but details not gathered
[2024-05-14T14:54:21.498] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:54:21.498] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:54:21.498] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:54:21.498] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T14:58:29.070] error: chdir(/var/log): Permission denied
[2024-05-14T14:58:29.070] Job accounting information stored, but details not gathered
[2024-05-14T14:58:29.070] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T14:58:29.070] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T14:58:29.071] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T14:58:29.071] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T15:01:14.152] error: chdir(/var/log): Permission denied
[2024-05-14T15:01:14.152] Job accounting information stored, but details not gathered
[2024-05-14T15:01:14.152] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T15:01:14.152] error: Couldn't find the specified plugin name for select/CR_Core_Memory looking at all files
[2024-05-14T15:01:14.153] error: cannot find select plugin for select/CR_Core_Memory
[2024-05-14T15:01:14.153] fatal: Can't find plugin for select/CR_Core_Memory
[2024-05-14T15:05:49.428] error: chdir(/var/log): Permission denied
[2024-05-14T15:05:49.428] Job accounting information stored, but details not gathered
[2024-05-14T15:05:49.428] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T15:05:49.431] No memory enforcing mechanism configured.
[2024-05-14T15:05:49.431] layouts: no layout to initialize
[2024-05-14T15:05:49.439] layouts: loading entities/relations information
[2024-05-14T15:05:49.439] Recovered state of 2 nodes
[2024-05-14T15:05:49.439] Down nodes: compute[00-01]
[2024-05-14T15:05:49.439] Recovered information about 0 jobs
[2024-05-14T15:05:49.439] cons_res: select_p_node_init
[2024-05-14T15:05:49.439] cons_res: preparing for 2 partitions
[2024-05-14T15:05:49.439] Recovered state of 0 reservations
[2024-05-14T15:05:49.439] _preserve_plugins: backup_controller not specified
[2024-05-14T15:05:49.439] cons_res: select_p_reconfigure
[2024-05-14T15:05:49.439] cons_res: select_p_node_init
[2024-05-14T15:05:49.439] cons_res: preparing for 2 partitions
[2024-05-14T15:05:49.439] Running as primary controller
[2024-05-14T15:05:49.439] No parameter for mcs plugin, default values set
[2024-05-14T15:05:49.439] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T15:05:56.663] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T15:05:56.664] Saving all slurm state
[2024-05-14T15:05:56.714] layouts: all layouts are now unloaded.
[2024-05-14T15:05:59.755] Dropped 1 hung communications to shutdown
[2024-05-14T15:05:59.755] Slurmctld shutdown completing with 1 active agent thread
[2024-05-14T15:06:07.223] error: chdir(/var/log): Permission denied
[2024-05-14T15:06:07.224] Job accounting information stored, but details not gathered
[2024-05-14T15:06:07.224] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T15:06:07.226] No memory enforcing mechanism configured.
[2024-05-14T15:06:07.226] layouts: no layout to initialize
[2024-05-14T15:06:07.235] layouts: loading entities/relations information
[2024-05-14T15:06:07.235] Recovered state of 2 nodes
[2024-05-14T15:06:07.235] Down nodes: compute[00-01]
[2024-05-14T15:06:07.235] Recovered information about 0 jobs
[2024-05-14T15:06:07.235] cons_res: select_p_node_init
[2024-05-14T15:06:07.235] cons_res: preparing for 2 partitions
[2024-05-14T15:06:07.235] Recovered state of 0 reservations
[2024-05-14T15:06:07.235] _preserve_plugins: backup_controller not specified
[2024-05-14T15:06:07.235] cons_res: select_p_reconfigure
[2024-05-14T15:06:07.235] cons_res: select_p_node_init
[2024-05-14T15:06:07.235] cons_res: preparing for 2 partitions
[2024-05-14T15:06:07.235] Running as primary controller
[2024-05-14T15:06:07.235] No parameter for mcs plugin, default values set
[2024-05-14T15:06:07.236] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T15:07:07.630] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T15:07:34.296] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:07:34.304] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:07:34.501] Node compute00 now responding
[2024-05-14T15:07:34.501] node compute00 returned to service
[2024-05-14T15:07:34.883] Node compute01 now responding
[2024-05-14T15:07:34.883] node compute01 returned to service
[2024-05-14T15:11:08.029] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:11:08.033] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:16:07.914] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:16:07.918] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:17:46.849] _slurm_rpc_submit_batch_job: JobId=2 InitPrio=4294901759 usec=399
[2024-05-14T15:17:47.407] backfill: Started JobId=2 in normal on compute00
[2024-05-14T15:17:47.451] _job_complete: JobId=2 WEXITSTATUS 1
[2024-05-14T15:17:47.456] _job_complete: JobId=2 done
[2024-05-14T15:21:08.160] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:21:08.160] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:26:07.388] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:26:07.389] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:31:07.203] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:31:07.213] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:36:07.919] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:36:07.927] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:38:07.831] _slurm_rpc_submit_batch_job: JobId=3 InitPrio=4294901758 usec=1698
[2024-05-14T15:38:07.950] sched: Allocate JobId=3 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T15:38:07.991] _job_complete: JobId=3 WEXITSTATUS 1
[2024-05-14T15:38:07.999] _job_complete: JobId=3 done
[2024-05-14T15:41:08.109] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:41:08.112] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:44:07.492] _slurm_rpc_submit_batch_job: JobId=4 InitPrio=4294901757 usec=319
[2024-05-14T15:44:07.872] backfill: Started JobId=4 in normal on compute00
[2024-05-14T15:44:07.922] _job_complete: JobId=4 WEXITSTATUS 1
[2024-05-14T15:44:07.928] _job_complete: JobId=4 done
[2024-05-14T15:46:07.400] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:46:07.414] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:51:07.244] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T15:51:07.248] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:56:08.042] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T15:56:08.043] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:01:07.881] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:01:07.884] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:06:08.131] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:06:08.139] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:11:07.852] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:11:07.856] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:16:07.488] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:16:07.491] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:16:21.283] _slurm_rpc_submit_batch_job: JobId=5 InitPrio=4294901756 usec=432
[2024-05-14T16:16:21.779] sched: Allocate JobId=5 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:16:21.819] _job_complete: JobId=5 WEXITSTATUS 1
[2024-05-14T16:16:21.826] _job_complete: JobId=5 done
[2024-05-14T16:21:07.704] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:21:07.707] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:26:08.104] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:26:08.113] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:27:06.833] _slurm_rpc_submit_batch_job: JobId=6 InitPrio=4294901755 usec=1606
[2024-05-14T16:27:07.135] sched: Allocate JobId=6 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:27:07.177] _job_complete: JobId=6 WEXITSTATUS 1
[2024-05-14T16:27:07.185] _job_complete: JobId=6 done
[2024-05-14T16:29:17.105] _slurm_rpc_submit_batch_job: JobId=7 InitPrio=4294901754 usec=1074
[2024-05-14T16:29:17.667] sched: Allocate JobId=7 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:29:17.707] _job_complete: JobId=7 WEXITSTATUS 1
[2024-05-14T16:29:17.720] _job_complete: JobId=7 done
[2024-05-14T16:31:07.591] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:31:07.591] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:35:21.778] _slurm_rpc_submit_batch_job: JobId=8 InitPrio=4294901753 usec=1753
[2024-05-14T16:35:22.236] sched: Allocate JobId=8 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:35:22.279] _job_complete: JobId=8 WEXITSTATUS 1
[2024-05-14T16:35:22.285] _job_complete: JobId=8 done
[2024-05-14T16:36:07.859] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:36:07.869] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:36:21.857] _slurm_rpc_submit_batch_job: JobId=9 InitPrio=4294901752 usec=1041
[2024-05-14T16:36:21.932] sched: Allocate JobId=9 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:36:21.972] _job_complete: JobId=9 WEXITSTATUS 1
[2024-05-14T16:36:21.979] _job_complete: JobId=9 done
[2024-05-14T16:41:07.915] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:41:07.922] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:41:55.029] _build_node_list: No nodes satisfy JobId=10 requirements in partition normal
[2024-05-14T16:41:55.075] _slurm_rpc_allocate_resources: Requested node configuration is not available 
[2024-05-14T16:42:22.990] sched: _slurm_rpc_allocate_resources JobId=11 NodeList=compute00 usec=14525
[2024-05-14T16:42:42.790] _job_complete: JobId=11 WTERMSIG 126
[2024-05-14T16:42:42.790] _job_complete: JobId=11 cancelled by interactive user
[2024-05-14T16:42:42.858] _job_complete: JobId=11 done
[2024-05-14T16:42:50.433] _build_node_list: No nodes satisfy JobId=12 requirements in partition normal
[2024-05-14T16:42:50.442] _slurm_rpc_allocate_resources: Requested node configuration is not available 
[2024-05-14T16:43:07.948] sched: _slurm_rpc_allocate_resources JobId=13 NodeList=compute00 usec=11520
[2024-05-14T16:44:08.414] _job_complete: JobId=13 WEXITSTATUS 1
[2024-05-14T16:44:08.424] _job_complete: JobId=13 done
[2024-05-14T16:45:01.760] _slurm_rpc_submit_batch_job: JobId=14 InitPrio=4294901747 usec=2803
[2024-05-14T16:45:01.930] sched: Allocate JobId=14 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:45:02.026] _job_complete: JobId=14 WEXITSTATUS 0
[2024-05-14T16:45:02.033] _job_complete: JobId=14 done
[2024-05-14T16:46:08.088] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:46:08.093] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:46:14.788] _slurm_rpc_submit_batch_job: JobId=15 InitPrio=4294901746 usec=770
[2024-05-14T16:46:16.574] sched: Allocate JobId=15 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:46:16.652] _job_complete: JobId=15 WEXITSTATUS 0
[2024-05-14T16:46:16.659] _job_complete: JobId=15 done
[2024-05-14T16:48:36.361] _slurm_rpc_submit_batch_job: JobId=16 InitPrio=4294901745 usec=1060
[2024-05-14T16:48:36.669] sched: Allocate JobId=16 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:48:36.781] _job_complete: JobId=16 WEXITSTATUS 0
[2024-05-14T16:48:36.787] _job_complete: JobId=16 done
[2024-05-14T16:50:48.417] _slurm_rpc_submit_batch_job: JobId=17 InitPrio=4294901744 usec=861
[2024-05-14T16:50:49.392] sched: Allocate JobId=17 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:51:08.144] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:51:08.185] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:53:14.374] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=17 uid 0
[2024-05-14T16:54:23.152] _slurm_rpc_submit_batch_job: JobId=18 InitPrio=4294901743 usec=1273
[2024-05-14T16:54:23.999] sched: Allocate JobId=18 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T16:54:26.567] _job_complete: JobId=18 WEXITSTATUS 0
[2024-05-14T16:54:26.575] _job_complete: JobId=18 done
[2024-05-14T16:56:07.641] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T16:56:07.645] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T16:59:02.865] killing old slurmctld[4337]
[2024-05-14T16:59:02.865] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T16:59:02.891] Saving all slurm state
[2024-05-14T16:59:02.927] layouts: all layouts are now unloaded.
[2024-05-14T16:59:02.929] error: chdir(/var/log): Permission denied
[2024-05-14T16:59:02.929] Job accounting information stored, but details not gathered
[2024-05-14T16:59:02.929] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T16:59:02.946] No memory enforcing mechanism configured.
[2024-05-14T16:59:02.946] layouts: no layout to initialize
[2024-05-14T16:59:02.953] layouts: loading entities/relations information
[2024-05-14T16:59:02.953] Recovered state of 2 nodes
[2024-05-14T16:59:02.953] Recovered JobId=18 Assoc=0
[2024-05-14T16:59:02.958] Recovered information about 1 jobs
[2024-05-14T16:59:02.958] cons_res: select_p_node_init
[2024-05-14T16:59:02.958] cons_res: preparing for 2 partitions
[2024-05-14T16:59:02.959] Recovered state of 0 reservations
[2024-05-14T16:59:02.959] _preserve_plugins: backup_controller not specified
[2024-05-14T16:59:02.959] cons_res: select_p_reconfigure
[2024-05-14T16:59:02.959] cons_res: select_p_node_init
[2024-05-14T16:59:02.959] cons_res: preparing for 2 partitions
[2024-05-14T16:59:02.959] Running as primary controller
[2024-05-14T16:59:02.960] No parameter for mcs plugin, default values set
[2024-05-14T16:59:02.960] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T16:59:06.388] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T16:59:06.389] Saving all slurm state
[2024-05-14T16:59:06.626] layouts: all layouts are now unloaded.
[2024-05-14T16:59:10.382] error: chdir(/var/log): Permission denied
[2024-05-14T16:59:10.382] Job accounting information stored, but details not gathered
[2024-05-14T16:59:10.382] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T16:59:10.393] No memory enforcing mechanism configured.
[2024-05-14T16:59:10.393] layouts: no layout to initialize
[2024-05-14T16:59:10.413] layouts: loading entities/relations information
[2024-05-14T16:59:10.413] Recovered state of 2 nodes
[2024-05-14T16:59:10.414] Recovered JobId=18 Assoc=0
[2024-05-14T16:59:10.420] Recovered information about 1 jobs
[2024-05-14T16:59:10.421] cons_res: select_p_node_init
[2024-05-14T16:59:10.421] cons_res: preparing for 2 partitions
[2024-05-14T16:59:10.432] Recovered state of 0 reservations
[2024-05-14T16:59:10.432] _preserve_plugins: backup_controller not specified
[2024-05-14T16:59:10.432] cons_res: select_p_reconfigure
[2024-05-14T16:59:10.432] cons_res: select_p_node_init
[2024-05-14T16:59:10.432] cons_res: preparing for 2 partitions
[2024-05-14T16:59:10.432] Running as primary controller
[2024-05-14T16:59:10.432] No parameter for mcs plugin, default values set
[2024-05-14T16:59:10.432] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T17:00:10.080] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T17:02:16.172] error: Security violation, UPDATE_NODE RPC from uid=1001
[2024-05-14T17:02:16.172] _slurm_rpc_update_node for compute00: Invalid user id
[2024-05-14T17:02:32.843] Invalid node state transition requested for node compute00 from=UNKNOWN* to=RESUME
[2024-05-14T17:02:32.843] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-05-14T17:04:10.771] error: Nodes compute[00-01] not responding
[2024-05-14T17:04:16.654] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:04:16.780] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:04:17.011] Node compute00 now responding
[2024-05-14T17:04:17.021] Node compute01 now responding
[2024-05-14T17:05:07.495] _slurm_rpc_submit_batch_job: JobId=19 InitPrio=4294901742 usec=1802
[2024-05-14T17:05:07.997] sched: Allocate JobId=19 NodeList=compute00 #CPUs=1 Partition=normal
[2024-05-14T17:05:10.592] _job_complete: JobId=19 WEXITSTATUS 0
[2024-05-14T17:05:10.603] _job_complete: JobId=19 done
[2024-05-14T17:09:10.410] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:09:10.430] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:14:10.407] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:14:10.414] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:19:10.900] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:19:10.902] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:24:10.683] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:24:10.687] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:29:11.022] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:29:11.024] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:34:10.654] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:34:10.658] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:39:10.747] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:39:10.747] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:44:10.982] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:44:10.985] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:49:10.628] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:49:10.670] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:54:10.372] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T17:54:10.374] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:59:10.357] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T17:59:10.359] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T18:04:11.167] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T18:04:11.168] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T18:09:10.716] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T18:09:10.719] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T18:17:34.631] error: Nodes compute[00-01] not responding, setting DOWN
[2024-05-14T18:36:34.757] killing old slurmctld[23894]
[2024-05-14T18:36:34.757] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T18:36:34.851] Saving all slurm state
[2024-05-14T18:36:34.916] layouts: all layouts are now unloaded.
[2024-05-14T18:36:34.926] error: chdir(/var/log): Permission denied
[2024-05-14T18:36:34.926] Job accounting information stored, but details not gathered
[2024-05-14T18:36:34.926] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T18:36:34.965] No memory enforcing mechanism configured.
[2024-05-14T18:36:34.965] layouts: no layout to initialize
[2024-05-14T18:36:34.982] layouts: loading entities/relations information
[2024-05-14T18:36:34.982] Recovered state of 2 nodes
[2024-05-14T18:36:34.982] Down nodes: compute[00-01]
[2024-05-14T18:36:34.982] Recovered information about 0 jobs
[2024-05-14T18:36:34.982] cons_res: select_p_node_init
[2024-05-14T18:36:34.982] cons_res: preparing for 2 partitions
[2024-05-14T18:36:34.983] Recovered state of 0 reservations
[2024-05-14T18:36:34.983] _preserve_plugins: backup_controller not specified
[2024-05-14T18:36:34.983] cons_res: select_p_reconfigure
[2024-05-14T18:36:34.983] cons_res: select_p_node_init
[2024-05-14T18:36:34.983] cons_res: preparing for 2 partitions
[2024-05-14T18:36:34.983] Running as primary controller
[2024-05-14T18:36:34.987] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T18:36:34.987] No parameter for mcs plugin, default values set
[2024-05-14T18:36:34.987] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T18:37:16.517] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T18:37:16.616] Saving all slurm state
[2024-05-14T18:37:16.693] layouts: all layouts are now unloaded.
[2024-05-14T18:37:34.279] error: chdir(/var/log): Permission denied
[2024-05-14T18:37:34.279] Job accounting information stored, but details not gathered
[2024-05-14T18:37:34.279] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T18:37:34.282] No memory enforcing mechanism configured.
[2024-05-14T18:37:34.282] layouts: no layout to initialize
[2024-05-14T18:37:34.293] layouts: loading entities/relations information
[2024-05-14T18:37:34.293] Recovered state of 2 nodes
[2024-05-14T18:37:34.293] Down nodes: compute[00-01]
[2024-05-14T18:37:34.293] Recovered information about 0 jobs
[2024-05-14T18:37:34.293] cons_res: select_p_node_init
[2024-05-14T18:37:34.293] cons_res: preparing for 2 partitions
[2024-05-14T18:37:34.293] Recovered state of 0 reservations
[2024-05-14T18:37:34.293] _preserve_plugins: backup_controller not specified
[2024-05-14T18:37:34.293] cons_res: select_p_reconfigure
[2024-05-14T18:37:34.294] cons_res: select_p_node_init
[2024-05-14T18:37:34.294] cons_res: preparing for 2 partitions
[2024-05-14T18:37:34.294] Running as primary controller
[2024-05-14T18:37:34.294] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T18:37:34.294] No parameter for mcs plugin, default values set
[2024-05-14T18:37:34.294] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T18:38:34.759] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T18:40:57.051] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T18:40:57.199] Node compute01 now responding
[2024-05-14T18:40:57.199] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1715711963 last response=1715711854
[2024-05-14T18:40:57.202] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T18:40:57.376] Node compute00 now responding
[2024-05-14T18:40:57.376] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1715711964 last response=1715711854
[2024-05-14T18:42:34.339] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T18:42:34.343] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T18:47:27.152] update_node: node compute00 state set to IDLE
[2024-05-14T18:47:27.649] Node compute00 now responding
[2024-05-14T18:47:33.548] update_node: node compute01 state set to IDLE
[2024-05-14T18:47:33.684] Node compute01 now responding
[2024-05-14T18:47:34.989] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T18:47:34.989] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T18:48:28.158] _slurm_rpc_submit_batch_job: JobId=20 InitPrio=1037 usec=4393
[2024-05-14T18:48:28.648] sched: Allocate JobId=20 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:28.756] _job_complete: JobId=20 WEXITSTATUS 0
[2024-05-14T18:48:28.763] _job_complete: JobId=20 done
[2024-05-14T18:48:48.380] _slurm_rpc_submit_batch_job: JobId=21 InitPrio=1037 usec=499
[2024-05-14T18:48:48.769] _slurm_rpc_submit_batch_job: JobId=22 InitPrio=1037 usec=855
[2024-05-14T18:48:49.121] sched: Allocate JobId=21 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:49.126] sched: Allocate JobId=22 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:49.167] _slurm_rpc_submit_batch_job: JobId=23 InitPrio=1037 usec=358
[2024-05-14T18:48:49.243] _job_complete: JobId=21 WEXITSTATUS 0
[2024-05-14T18:48:49.250] _job_complete: JobId=21 done
[2024-05-14T18:48:49.250] _job_complete: JobId=22 WEXITSTATUS 0
[2024-05-14T18:48:49.255] _job_complete: JobId=22 done
[2024-05-14T18:48:49.269] sched: Allocate JobId=23 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:49.362] _job_complete: JobId=23 WEXITSTATUS 0
[2024-05-14T18:48:49.372] _job_complete: JobId=23 done
[2024-05-14T18:48:49.514] _slurm_rpc_submit_batch_job: JobId=24 InitPrio=1037 usec=401
[2024-05-14T18:48:49.877] _slurm_rpc_submit_batch_job: JobId=25 InitPrio=1037 usec=1389
[2024-05-14T18:48:50.181] _slurm_rpc_submit_batch_job: JobId=26 InitPrio=1037 usec=804
[2024-05-14T18:48:50.524] _slurm_rpc_submit_batch_job: JobId=27 InitPrio=1037 usec=1128
[2024-05-14T18:48:50.898] _slurm_rpc_submit_batch_job: JobId=28 InitPrio=1037 usec=1232
[2024-05-14T18:48:51.270] _slurm_rpc_submit_batch_job: JobId=29 InitPrio=1037 usec=909
[2024-05-14T18:48:51.631] _slurm_rpc_submit_batch_job: JobId=30 InitPrio=1037 usec=1159
[2024-05-14T18:48:52.032] _slurm_rpc_submit_batch_job: JobId=31 InitPrio=1037 usec=1116
[2024-05-14T18:48:52.302] sched: Allocate JobId=24 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.308] sched: Allocate JobId=25 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.314] sched: Allocate JobId=26 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.319] sched: Allocate JobId=27 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.410] _slurm_rpc_submit_batch_job: JobId=32 InitPrio=1037 usec=693
[2024-05-14T18:48:52.493] _job_complete: JobId=25 WEXITSTATUS 0
[2024-05-14T18:48:52.501] _job_complete: JobId=25 done
[2024-05-14T18:48:52.501] _job_complete: JobId=24 WEXITSTATUS 0
[2024-05-14T18:48:52.520] _job_complete: JobId=24 done
[2024-05-14T18:48:52.520] _job_complete: JobId=26 WEXITSTATUS 0
[2024-05-14T18:48:52.528] _job_complete: JobId=26 done
[2024-05-14T18:48:52.529] _job_complete: JobId=27 WEXITSTATUS 0
[2024-05-14T18:48:52.536] _job_complete: JobId=27 done
[2024-05-14T18:48:52.562] sched: Allocate JobId=28 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.565] sched: Allocate JobId=29 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.608] _job_complete: JobId=28 WEXITSTATUS 0
[2024-05-14T18:48:52.619] _job_complete: JobId=28 done
[2024-05-14T18:48:52.620] _job_complete: JobId=29 WEXITSTATUS 0
[2024-05-14T18:48:52.628] _job_complete: JobId=29 done
[2024-05-14T18:48:52.630] sched: Allocate JobId=30 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.633] sched: Allocate JobId=31 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.715] _job_complete: JobId=31 WEXITSTATUS 0
[2024-05-14T18:48:52.723] _job_complete: JobId=31 done
[2024-05-14T18:48:52.724] _job_complete: JobId=30 WEXITSTATUS 0
[2024-05-14T18:48:52.731] _job_complete: JobId=30 done
[2024-05-14T18:48:52.733] sched: Allocate JobId=32 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.749] _slurm_rpc_submit_batch_job: JobId=33 InitPrio=1037 usec=397
[2024-05-14T18:48:52.755] sched: Allocate JobId=33 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:52.818] _job_complete: JobId=32 WEXITSTATUS 0
[2024-05-14T18:48:52.830] _job_complete: JobId=32 done
[2024-05-14T18:48:52.831] _job_complete: JobId=33 WEXITSTATUS 0
[2024-05-14T18:48:52.838] _job_complete: JobId=33 done
[2024-05-14T18:48:53.315] _slurm_rpc_submit_batch_job: JobId=34 InitPrio=1037 usec=1290
[2024-05-14T18:48:53.811] _slurm_rpc_submit_batch_job: JobId=35 InitPrio=1037 usec=1878
[2024-05-14T18:48:54.281] _slurm_rpc_submit_batch_job: JobId=36 InitPrio=1037 usec=1978
[2024-05-14T18:48:54.707] _slurm_rpc_submit_batch_job: JobId=37 InitPrio=1037 usec=1123
[2024-05-14T18:48:55.105] _slurm_rpc_submit_batch_job: JobId=38 InitPrio=1037 usec=1085
[2024-05-14T18:48:55.970] sched: Allocate JobId=34 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:55.975] sched: Allocate JobId=35 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:55.979] sched: Allocate JobId=36 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:55.984] sched: Allocate JobId=37 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:56.106] _job_complete: JobId=37 WEXITSTATUS 0
[2024-05-14T18:48:56.120] _job_complete: JobId=37 done
[2024-05-14T18:48:56.120] _job_complete: JobId=35 WEXITSTATUS 0
[2024-05-14T18:48:56.128] _job_complete: JobId=35 done
[2024-05-14T18:48:56.129] _job_complete: JobId=36 WEXITSTATUS 0
[2024-05-14T18:48:56.138] _job_complete: JobId=36 done
[2024-05-14T18:48:56.138] _job_complete: JobId=34 WEXITSTATUS 0
[2024-05-14T18:48:56.146] _job_complete: JobId=34 done
[2024-05-14T18:48:56.221] sched: Allocate JobId=38 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T18:48:56.283] _job_complete: JobId=38 WEXITSTATUS 0
[2024-05-14T18:48:56.290] _job_complete: JobId=38 done
[2024-05-14T18:49:23.656] _slurm_rpc_submit_batch_job: JobId=39 InitPrio=10100 usec=1031
[2024-05-14T18:49:24.347] sched: Allocate JobId=39 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T18:49:28.650] _slurm_rpc_submit_batch_job: JobId=40 InitPrio=1037 usec=445
[2024-05-14T18:50:31.017] _slurm_rpc_submit_batch_job: JobId=41 InitPrio=1037 usec=310
[2024-05-14T18:50:32.298] _slurm_rpc_submit_batch_job: JobId=42 InitPrio=1037 usec=283
[2024-05-14T18:50:32.937] _slurm_rpc_submit_batch_job: JobId=43 InitPrio=1037 usec=315
[2024-05-14T18:50:33.449] _slurm_rpc_submit_batch_job: JobId=44 InitPrio=1037 usec=373
[2024-05-14T18:50:33.849] _slurm_rpc_submit_batch_job: JobId=45 InitPrio=1037 usec=290
[2024-05-14T18:50:34.105] _slurm_rpc_submit_batch_job: JobId=46 InitPrio=1037 usec=323
[2024-05-14T18:50:34.314] _slurm_rpc_submit_batch_job: JobId=47 InitPrio=1037 usec=296
[2024-05-14T18:50:34.505] _slurm_rpc_submit_batch_job: JobId=48 InitPrio=1037 usec=355
[2024-05-14T18:50:34.681] _slurm_rpc_submit_batch_job: JobId=49 InitPrio=1037 usec=300
[2024-05-14T18:50:34.874] _slurm_rpc_submit_batch_job: JobId=50 InitPrio=1037 usec=313
[2024-05-14T18:50:35.050] _slurm_rpc_submit_batch_job: JobId=51 InitPrio=1037 usec=337
[2024-05-14T18:50:36.281] _slurm_rpc_submit_batch_job: JobId=52 InitPrio=1037 usec=358
[2024-05-14T18:51:08.313] _slurm_rpc_submit_batch_job: JobId=53 InitPrio=10100 usec=291
[2024-05-14T18:51:12.489] _slurm_rpc_submit_batch_job: JobId=54 InitPrio=1037 usec=290
[2024-05-14T18:51:13.129] _slurm_rpc_submit_batch_job: JobId=55 InitPrio=1037 usec=366
[2024-05-14T18:51:13.441] _slurm_rpc_submit_batch_job: JobId=56 InitPrio=1037 usec=329
[2024-05-14T18:51:13.641] _slurm_rpc_submit_batch_job: JobId=57 InitPrio=1037 usec=338
[2024-05-14T18:51:13.849] _slurm_rpc_submit_batch_job: JobId=58 InitPrio=1037 usec=339
[2024-05-14T18:51:14.027] _slurm_rpc_submit_batch_job: JobId=59 InitPrio=10100 usec=327
[2024-05-14T18:51:14.216] _slurm_rpc_submit_batch_job: JobId=60 InitPrio=10100 usec=347
[2024-05-14T18:51:14.426] _slurm_rpc_submit_batch_job: JobId=61 InitPrio=10100 usec=298
[2024-05-14T18:51:14.633] _slurm_rpc_submit_batch_job: JobId=62 InitPrio=10100 usec=354
[2024-05-14T18:51:14.808] _slurm_rpc_submit_batch_job: JobId=63 InitPrio=10100 usec=355
[2024-05-14T18:51:15.001] _slurm_rpc_submit_batch_job: JobId=64 InitPrio=10100 usec=334
[2024-05-14T18:51:15.194] _slurm_rpc_submit_batch_job: JobId=65 InitPrio=10100 usec=1728
[2024-05-14T18:51:15.371] _slurm_rpc_submit_batch_job: JobId=66 InitPrio=1037 usec=325
[2024-05-14T18:51:15.545] _slurm_rpc_submit_batch_job: JobId=67 InitPrio=1037 usec=319
[2024-05-14T18:51:15.721] _slurm_rpc_submit_batch_job: JobId=68 InitPrio=1037 usec=413
[2024-05-14T18:51:15.913] _slurm_rpc_submit_batch_job: JobId=69 InitPrio=1037 usec=322
[2024-05-14T18:51:16.106] _slurm_rpc_submit_batch_job: JobId=70 InitPrio=1037 usec=1641
[2024-05-14T18:51:16.267] _slurm_rpc_submit_batch_job: JobId=71 InitPrio=1037 usec=315
[2024-05-14T18:51:16.461] _slurm_rpc_submit_batch_job: JobId=72 InitPrio=1037 usec=980
[2024-05-14T18:51:16.633] _slurm_rpc_submit_batch_job: JobId=73 InitPrio=1037 usec=397
[2024-05-14T18:51:16.794] _slurm_rpc_submit_batch_job: JobId=74 InitPrio=1037 usec=1671
[2024-05-14T18:51:18.090] _slurm_rpc_submit_batch_job: JobId=75 InitPrio=1037 usec=341
[2024-05-14T18:51:30.141] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=39 uid 1001
[2024-05-14T18:51:32.102] sched: Allocate JobId=53 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T18:52:34.306] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T18:52:34.541] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T18:57:34.631] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T18:57:34.659] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:00:13.087] killing old slurmctld[29332]
[2024-05-14T19:00:13.088] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T19:00:13.130] Saving all slurm state
[2024-05-14T19:00:13.191] layouts: all layouts are now unloaded.
[2024-05-14T19:00:13.201] error: chdir(/var/log): Permission denied
[2024-05-14T19:00:13.201] Job accounting information stored, but details not gathered
[2024-05-14T19:00:13.201] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T19:00:13.209] No memory enforcing mechanism configured.
[2024-05-14T19:00:13.209] layouts: no layout to initialize
[2024-05-14T19:00:13.215] layouts: loading entities/relations information
[2024-05-14T19:00:13.215] Recovered state of 2 nodes
[2024-05-14T19:00:13.216] Recovered JobId=40 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=41 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=42 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=43 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=44 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=45 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=46 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=47 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=48 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=49 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=50 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=51 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=52 Assoc=0
[2024-05-14T19:00:13.216] Recovered JobId=53 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=54 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=55 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=56 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=57 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=58 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=59 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=60 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=61 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=62 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=63 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=64 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=65 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=66 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=67 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=68 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=69 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=70 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=71 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=72 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=73 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=74 Assoc=0
[2024-05-14T19:00:13.219] Recovered JobId=75 Assoc=0
[2024-05-14T19:00:13.219] Recovered information about 36 jobs
[2024-05-14T19:00:13.219] cons_res: select_p_node_init
[2024-05-14T19:00:13.219] cons_res: preparing for 2 partitions
[2024-05-14T19:00:13.220] Recovered state of 0 reservations
[2024-05-14T19:00:13.220] _preserve_plugins: backup_controller not specified
[2024-05-14T19:00:13.220] cons_res: select_p_reconfigure
[2024-05-14T19:00:13.220] cons_res: select_p_node_init
[2024-05-14T19:00:13.220] cons_res: preparing for 2 partitions
[2024-05-14T19:00:13.220] Running as primary controller
[2024-05-14T19:00:13.221] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T19:00:13.221] No parameter for mcs plugin, default values set
[2024-05-14T19:00:13.221] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T19:00:20.350] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T19:00:20.389] Saving all slurm state
[2024-05-14T19:00:20.478] layouts: all layouts are now unloaded.
[2024-05-14T19:00:28.986] error: chdir(/var/log): Permission denied
[2024-05-14T19:00:28.986] Job accounting information stored, but details not gathered
[2024-05-14T19:00:28.986] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T19:00:28.988] No memory enforcing mechanism configured.
[2024-05-14T19:00:28.988] layouts: no layout to initialize
[2024-05-14T19:00:29.193] layouts: loading entities/relations information
[2024-05-14T19:00:29.193] Recovered state of 2 nodes
[2024-05-14T19:00:29.193] Recovered JobId=40 Assoc=0
[2024-05-14T19:00:29.193] Recovered JobId=41 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=42 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=43 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=44 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=45 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=46 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=47 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=48 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=49 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=50 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=51 Assoc=0
[2024-05-14T19:00:29.194] Recovered JobId=52 Assoc=0
[2024-05-14T19:00:29.195] Recovered JobId=53 Assoc=0
[2024-05-14T19:00:29.200] Recovered JobId=54 Assoc=0
[2024-05-14T19:00:29.200] Recovered JobId=55 Assoc=0
[2024-05-14T19:00:29.200] Recovered JobId=56 Assoc=0
[2024-05-14T19:00:29.200] Recovered JobId=57 Assoc=0
[2024-05-14T19:00:29.200] Recovered JobId=58 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=59 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=60 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=61 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=62 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=63 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=64 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=65 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=66 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=67 Assoc=0
[2024-05-14T19:00:29.201] Recovered JobId=68 Assoc=0
[2024-05-14T19:00:29.202] Recovered JobId=69 Assoc=0
[2024-05-14T19:00:29.202] Recovered JobId=70 Assoc=0
[2024-05-14T19:00:29.202] Recovered JobId=71 Assoc=0
[2024-05-14T19:00:29.202] Recovered JobId=72 Assoc=0
[2024-05-14T19:00:29.202] Recovered JobId=73 Assoc=0
[2024-05-14T19:00:29.202] Recovered JobId=74 Assoc=0
[2024-05-14T19:00:29.202] Recovered JobId=75 Assoc=0
[2024-05-14T19:00:29.202] Recovered information about 36 jobs
[2024-05-14T19:00:29.202] cons_res: select_p_node_init
[2024-05-14T19:00:29.202] cons_res: preparing for 2 partitions
[2024-05-14T19:00:29.204] Recovered state of 0 reservations
[2024-05-14T19:00:29.204] _preserve_plugins: backup_controller not specified
[2024-05-14T19:00:29.204] cons_res: select_p_reconfigure
[2024-05-14T19:00:29.204] cons_res: select_p_node_init
[2024-05-14T19:00:29.204] cons_res: preparing for 2 partitions
[2024-05-14T19:00:29.204] Running as primary controller
[2024-05-14T19:00:29.205] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T19:00:29.205] No parameter for mcs plugin, default values set
[2024-05-14T19:00:29.205] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T19:01:29.505] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T19:01:49.840] Invalid node state transition requested for node compute00 from=UNKNOWN* to=RESUME
[2024-05-14T19:01:49.848] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-05-14T19:02:52.893] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:02:52.980] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:02:53.054] Node compute01 now responding
[2024-05-14T19:02:53.162] Batch JobId=53 missing from batch node compute00 (not found BatchStartTime after startup), Requeuing job
[2024-05-14T19:02:53.162] _job_complete: JobId=53 WTERMSIG 126
[2024-05-14T19:02:53.162] _job_complete: JobId=53 cancelled by node failure
[2024-05-14T19:02:53.232] _job_complete: requeue JobId=53 due to node failure
[2024-05-14T19:02:53.232] _job_complete: JobId=53 done
[2024-05-14T19:02:53.232] Node compute00 now responding
[2024-05-14T19:02:53.232] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1715713251 last response=1715713229
[2024-05-14T19:02:53.410] Requeuing JobId=53
[2024-05-14T19:02:53.412] sched: Allocate JobId=40 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.414] sched: Allocate JobId=41 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.499] _job_complete: JobId=41 WEXITSTATUS 0
[2024-05-14T19:02:53.510] _job_complete: JobId=41 done
[2024-05-14T19:02:53.510] _job_complete: JobId=40 WEXITSTATUS 0
[2024-05-14T19:02:53.516] _job_complete: JobId=40 done
[2024-05-14T19:02:53.595] sched: Allocate JobId=42 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.597] sched: Allocate JobId=43 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.668] _job_complete: JobId=43 WEXITSTATUS 0
[2024-05-14T19:02:53.673] _job_complete: JobId=43 done
[2024-05-14T19:02:53.674] _job_complete: JobId=42 WEXITSTATUS 0
[2024-05-14T19:02:53.679] _job_complete: JobId=42 done
[2024-05-14T19:02:53.700] sched: Allocate JobId=44 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.704] sched: Allocate JobId=45 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.758] _job_complete: JobId=44 WEXITSTATUS 0
[2024-05-14T19:02:53.770] _job_complete: JobId=44 done
[2024-05-14T19:02:53.772] _job_complete: JobId=45 WEXITSTATUS 0
[2024-05-14T19:02:53.778] _job_complete: JobId=45 done
[2024-05-14T19:02:53.797] sched: Allocate JobId=46 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.798] sched: Allocate JobId=47 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.848] _job_complete: JobId=47 WEXITSTATUS 0
[2024-05-14T19:02:53.854] _job_complete: JobId=47 done
[2024-05-14T19:02:53.855] _job_complete: JobId=46 WEXITSTATUS 0
[2024-05-14T19:02:53.861] _job_complete: JobId=46 done
[2024-05-14T19:02:53.937] sched: Allocate JobId=48 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:53.939] sched: Allocate JobId=49 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.009] _job_complete: JobId=49 WEXITSTATUS 0
[2024-05-14T19:02:54.014] _job_complete: JobId=49 done
[2024-05-14T19:02:54.015] _job_complete: JobId=48 WEXITSTATUS 0
[2024-05-14T19:02:54.021] _job_complete: JobId=48 done
[2024-05-14T19:02:54.102] sched: Allocate JobId=50 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.104] sched: Allocate JobId=51 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.171] _job_complete: JobId=51 WEXITSTATUS 0
[2024-05-14T19:02:54.178] _job_complete: JobId=51 done
[2024-05-14T19:02:54.180] _job_complete: JobId=50 WEXITSTATUS 0
[2024-05-14T19:02:54.185] _job_complete: JobId=50 done
[2024-05-14T19:02:54.265] sched: Allocate JobId=52 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.268] sched: Allocate JobId=54 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.325] _job_complete: JobId=54 WEXITSTATUS 0
[2024-05-14T19:02:54.340] _job_complete: JobId=54 done
[2024-05-14T19:02:54.341] _job_complete: JobId=52 WEXITSTATUS 0
[2024-05-14T19:02:54.349] _job_complete: JobId=52 done
[2024-05-14T19:02:54.426] sched: Allocate JobId=55 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.428] sched: Allocate JobId=56 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.501] _job_complete: JobId=56 WEXITSTATUS 0
[2024-05-14T19:02:54.513] _job_complete: JobId=56 done
[2024-05-14T19:02:54.513] _job_complete: JobId=55 WEXITSTATUS 0
[2024-05-14T19:02:54.521] _job_complete: JobId=55 done
[2024-05-14T19:02:54.599] sched: Allocate JobId=57 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.602] sched: Allocate JobId=58 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.669] _job_complete: JobId=58 WEXITSTATUS 0
[2024-05-14T19:02:54.677] _job_complete: JobId=58 done
[2024-05-14T19:02:54.677] _job_complete: JobId=57 WEXITSTATUS 0
[2024-05-14T19:02:54.685] _job_complete: JobId=57 done
[2024-05-14T19:02:54.796] sched: Allocate JobId=66 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.799] sched: Allocate JobId=67 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.900] _job_complete: JobId=67 WEXITSTATUS 0
[2024-05-14T19:02:54.907] _job_complete: JobId=67 done
[2024-05-14T19:02:54.907] _job_complete: JobId=66 WEXITSTATUS 0
[2024-05-14T19:02:54.914] _job_complete: JobId=66 done
[2024-05-14T19:02:54.993] sched: Allocate JobId=68 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:54.995] sched: Allocate JobId=69 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:55.059] _job_complete: JobId=69 WEXITSTATUS 0
[2024-05-14T19:02:55.069] _job_complete: JobId=69 done
[2024-05-14T19:02:55.070] _job_complete: JobId=68 WEXITSTATUS 0
[2024-05-14T19:02:55.078] _job_complete: JobId=68 done
[2024-05-14T19:02:55.156] sched: Allocate JobId=70 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:55.159] sched: Allocate JobId=71 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:55.236] _job_complete: JobId=71 WEXITSTATUS 0
[2024-05-14T19:02:55.246] _job_complete: JobId=71 done
[2024-05-14T19:02:55.246] _job_complete: JobId=70 WEXITSTATUS 0
[2024-05-14T19:02:55.253] _job_complete: JobId=70 done
[2024-05-14T19:02:55.335] sched: Allocate JobId=72 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:55.338] sched: Allocate JobId=73 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:55.421] _job_complete: JobId=72 WEXITSTATUS 0
[2024-05-14T19:02:55.428] _job_complete: JobId=72 done
[2024-05-14T19:02:55.429] _job_complete: JobId=73 WEXITSTATUS 0
[2024-05-14T19:02:55.436] _job_complete: JobId=73 done
[2024-05-14T19:02:55.547] sched: Allocate JobId=74 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:55.550] sched: Allocate JobId=75 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:02:55.655] _job_complete: JobId=74 WEXITSTATUS 0
[2024-05-14T19:02:55.663] _job_complete: JobId=74 done
[2024-05-14T19:02:55.663] _job_complete: JobId=75 WEXITSTATUS 0
[2024-05-14T19:02:55.671] _job_complete: JobId=75 done
[2024-05-14T19:04:23.798] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=65 uid 0
[2024-05-14T19:04:26.997] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=64 uid 0
[2024-05-14T19:04:28.326] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=63 uid 0
[2024-05-14T19:04:29.449] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=62 uid 0
[2024-05-14T19:04:30.580] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=61 uid 0
[2024-05-14T19:04:40.357] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=60 uid 0
[2024-05-14T19:04:44.889] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=59 uid 0
[2024-05-14T19:04:47.224] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=53 uid 0
[2024-05-14T19:05:29.354] error: Nodes compute[00-01] not responding
[2024-05-14T19:05:29.564] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:05:29.565] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:06:21.719] killing old slurmctld[31258]
[2024-05-14T19:06:21.720] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T19:06:21.807] Saving all slurm state
[2024-05-14T19:06:21.893] layouts: all layouts are now unloaded.
[2024-05-14T19:06:21.898] error: chdir(/var/log): Permission denied
[2024-05-14T19:06:21.899] Job accounting information stored, but details not gathered
[2024-05-14T19:06:21.899] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T19:06:21.916] No memory enforcing mechanism configured.
[2024-05-14T19:06:21.916] layouts: no layout to initialize
[2024-05-14T19:06:21.930] layouts: loading entities/relations information
[2024-05-14T19:06:21.930] Recovered state of 2 nodes
[2024-05-14T19:06:21.930] Down nodes: compute00
[2024-05-14T19:06:21.931] Recovered JobId=40 Assoc=0
[2024-05-14T19:06:21.936] Recovered JobId=41 Assoc=0
[2024-05-14T19:06:21.939] Recovered JobId=42 Assoc=0
[2024-05-14T19:06:21.944] Recovered JobId=43 Assoc=0
[2024-05-14T19:06:21.949] Recovered JobId=44 Assoc=0
[2024-05-14T19:06:21.954] Recovered JobId=45 Assoc=0
[2024-05-14T19:06:21.958] Recovered JobId=46 Assoc=0
[2024-05-14T19:06:21.963] Recovered JobId=47 Assoc=0
[2024-05-14T19:06:21.968] Recovered JobId=48 Assoc=0
[2024-05-14T19:06:21.972] Recovered JobId=49 Assoc=0
[2024-05-14T19:06:21.977] Recovered JobId=50 Assoc=0
[2024-05-14T19:06:21.982] Recovered JobId=51 Assoc=0
[2024-05-14T19:06:21.987] Recovered JobId=52 Assoc=0
[2024-05-14T19:06:21.991] Recovered JobId=53 Assoc=0
[2024-05-14T19:06:21.996] Recovered JobId=54 Assoc=0
[2024-05-14T19:06:22.001] Recovered JobId=55 Assoc=0
[2024-05-14T19:06:22.006] Recovered JobId=56 Assoc=0
[2024-05-14T19:06:22.011] Recovered JobId=57 Assoc=0
[2024-05-14T19:06:22.016] Recovered JobId=58 Assoc=0
[2024-05-14T19:06:22.022] Recovered JobId=59 Assoc=0
[2024-05-14T19:06:22.027] Recovered JobId=60 Assoc=0
[2024-05-14T19:06:22.032] Recovered JobId=61 Assoc=0
[2024-05-14T19:06:22.037] Recovered JobId=62 Assoc=0
[2024-05-14T19:06:22.042] Recovered JobId=63 Assoc=0
[2024-05-14T19:06:22.047] Recovered JobId=64 Assoc=0
[2024-05-14T19:06:22.052] Recovered JobId=65 Assoc=0
[2024-05-14T19:06:22.058] Recovered JobId=66 Assoc=0
[2024-05-14T19:06:22.062] Recovered JobId=67 Assoc=0
[2024-05-14T19:06:22.082] Recovered JobId=68 Assoc=0
[2024-05-14T19:06:22.089] Recovered JobId=69 Assoc=0
[2024-05-14T19:06:22.095] Recovered JobId=70 Assoc=0
[2024-05-14T19:06:22.102] Recovered JobId=71 Assoc=0
[2024-05-14T19:06:22.109] Recovered JobId=72 Assoc=0
[2024-05-14T19:06:22.118] Recovered JobId=73 Assoc=0
[2024-05-14T19:06:22.125] Recovered JobId=74 Assoc=0
[2024-05-14T19:06:22.132] Recovered JobId=75 Assoc=0
[2024-05-14T19:06:22.138] Recovered information about 36 jobs
[2024-05-14T19:06:22.138] cons_res: select_p_node_init
[2024-05-14T19:06:22.139] cons_res: preparing for 2 partitions
[2024-05-14T19:06:22.140] Recovered state of 0 reservations
[2024-05-14T19:06:22.140] _preserve_plugins: backup_controller not specified
[2024-05-14T19:06:22.140] cons_res: select_p_reconfigure
[2024-05-14T19:06:22.140] cons_res: select_p_node_init
[2024-05-14T19:06:22.140] cons_res: preparing for 2 partitions
[2024-05-14T19:06:22.140] Running as primary controller
[2024-05-14T19:06:22.141] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T19:06:22.141] No parameter for mcs plugin, default values set
[2024-05-14T19:06:22.141] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T19:06:28.718] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T19:06:28.802] Saving all slurm state
[2024-05-14T19:06:28.850] layouts: all layouts are now unloaded.
[2024-05-14T19:06:39.088] error: chdir(/var/log): Permission denied
[2024-05-14T19:06:39.088] Job accounting information stored, but details not gathered
[2024-05-14T19:06:39.088] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T19:06:39.090] No memory enforcing mechanism configured.
[2024-05-14T19:06:39.090] layouts: no layout to initialize
[2024-05-14T19:06:39.120] layouts: loading entities/relations information
[2024-05-14T19:06:39.121] Recovered state of 2 nodes
[2024-05-14T19:06:39.121] Down nodes: compute00
[2024-05-14T19:06:39.121] Recovered JobId=40 Assoc=0
[2024-05-14T19:06:39.127] Recovered JobId=41 Assoc=0
[2024-05-14T19:06:39.132] Recovered JobId=42 Assoc=0
[2024-05-14T19:06:39.136] Recovered JobId=43 Assoc=0
[2024-05-14T19:06:39.141] Recovered JobId=44 Assoc=0
[2024-05-14T19:06:39.146] Recovered JobId=45 Assoc=0
[2024-05-14T19:06:39.151] Recovered JobId=46 Assoc=0
[2024-05-14T19:06:39.155] Recovered JobId=47 Assoc=0
[2024-05-14T19:06:39.160] Recovered JobId=48 Assoc=0
[2024-05-14T19:06:39.165] Recovered JobId=49 Assoc=0
[2024-05-14T19:06:39.170] Recovered JobId=50 Assoc=0
[2024-05-14T19:06:39.174] Recovered JobId=51 Assoc=0
[2024-05-14T19:06:39.179] Recovered JobId=52 Assoc=0
[2024-05-14T19:06:39.184] Recovered JobId=53 Assoc=0
[2024-05-14T19:06:39.189] Recovered JobId=54 Assoc=0
[2024-05-14T19:06:39.194] Recovered JobId=55 Assoc=0
[2024-05-14T19:06:39.198] Recovered JobId=56 Assoc=0
[2024-05-14T19:06:39.203] Recovered JobId=57 Assoc=0
[2024-05-14T19:06:39.208] Recovered JobId=58 Assoc=0
[2024-05-14T19:06:39.214] Recovered JobId=59 Assoc=0
[2024-05-14T19:06:39.218] Recovered JobId=60 Assoc=0
[2024-05-14T19:06:39.223] Recovered JobId=61 Assoc=0
[2024-05-14T19:06:39.228] Recovered JobId=62 Assoc=0
[2024-05-14T19:06:39.234] Recovered JobId=63 Assoc=0
[2024-05-14T19:06:39.240] Recovered JobId=64 Assoc=0
[2024-05-14T19:06:39.245] Recovered JobId=65 Assoc=0
[2024-05-14T19:06:39.251] Recovered JobId=66 Assoc=0
[2024-05-14T19:06:39.257] Recovered JobId=67 Assoc=0
[2024-05-14T19:06:39.263] Recovered JobId=68 Assoc=0
[2024-05-14T19:06:39.269] Recovered JobId=69 Assoc=0
[2024-05-14T19:06:39.274] Recovered JobId=70 Assoc=0
[2024-05-14T19:06:39.280] Recovered JobId=71 Assoc=0
[2024-05-14T19:06:39.286] Recovered JobId=72 Assoc=0
[2024-05-14T19:06:39.292] Recovered JobId=73 Assoc=0
[2024-05-14T19:06:39.298] Recovered JobId=74 Assoc=0
[2024-05-14T19:06:39.304] Recovered JobId=75 Assoc=0
[2024-05-14T19:06:39.310] Recovered information about 36 jobs
[2024-05-14T19:06:39.310] cons_res: select_p_node_init
[2024-05-14T19:06:39.310] cons_res: preparing for 2 partitions
[2024-05-14T19:06:39.311] Recovered state of 0 reservations
[2024-05-14T19:06:39.312] _preserve_plugins: backup_controller not specified
[2024-05-14T19:06:39.312] cons_res: select_p_reconfigure
[2024-05-14T19:06:39.312] cons_res: select_p_node_init
[2024-05-14T19:06:39.312] cons_res: preparing for 2 partitions
[2024-05-14T19:06:39.312] Running as primary controller
[2024-05-14T19:06:39.313] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T19:06:39.313] No parameter for mcs plugin, default values set
[2024-05-14T19:06:39.313] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T19:07:39.716] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T19:07:45.483] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:07:45.687] Node compute01 now responding
[2024-05-14T19:07:45.949] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:07:46.117] Node compute00 now responding
[2024-05-14T19:07:46.117] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1715713622 last response=1715713599
[2024-05-14T19:09:43.465] _slurm_rpc_submit_batch_job: JobId=76 InitPrio=200 usec=1735
[2024-05-14T19:11:39.680] error: Nodes compute01 not responding
[2024-05-14T19:15:29.602] error: Security violation, UPDATE_NODE RPC from uid=1001
[2024-05-14T19:15:29.602] _slurm_rpc_update_node for compute00: Invalid user id
[2024-05-14T19:16:07.522] update_node: node compute00 state set to IDLE
[2024-05-14T19:16:07.853] error: Nodes compute01 not responding, setting DOWN
[2024-05-14T19:16:13.273] update_node: node compute01 state set to IDLE
[2024-05-14T19:16:32.352] _slurm_rpc_submit_batch_job: JobId=77 InitPrio=200 usec=1504
[2024-05-14T19:16:39.691] error: Nodes compute[00-01] not responding
[2024-05-14T19:16:46.631] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=76 uid 1001
[2024-05-14T19:16:49.717] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=77 uid 1001
[2024-05-14T19:17:10.847] _slurm_rpc_submit_batch_job: JobId=78 InitPrio=200 usec=1291
[2024-05-14T19:21:39.720] error: Nodes compute[00-01] not responding
[2024-05-14T19:22:54.058] error: Nodes compute[00-01] not responding, setting DOWN
[2024-05-14T19:26:23.269] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:26:23.426] Node compute01 now responding
[2024-05-14T19:26:23.426] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1715714717 last response=1715714173
[2024-05-14T19:26:23.435] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:26:23.664] Node compute00 now responding
[2024-05-14T19:26:23.664] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1715714711 last response=1715714167
[2024-05-14T19:26:39.630] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:26:39.631] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:27:41.043] update_node: node compute00 state set to IDLE
[2024-05-14T19:27:41.972] Node compute00 now responding
[2024-05-14T19:27:44.008] update_node: node compute01 state set to IDLE
[2024-05-14T19:27:44.136] Node compute01 now responding
[2024-05-14T19:27:46.263] sched: Allocate JobId=78 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T19:28:35.840] _slurm_rpc_submit_batch_job: JobId=79 InitPrio=10037 usec=300
[2024-05-14T19:28:36.816] preempted JobId=78 has been killed to reclaim resources for JobId=79
[2024-05-14T19:28:36.999] sched: Allocate JobId=79 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T19:28:37.063] _job_complete: JobId=79 WEXITSTATUS 0
[2024-05-14T19:28:37.068] _job_complete: JobId=79 done
[2024-05-14T19:29:08.503] _slurm_rpc_submit_batch_job: JobId=80 InitPrio=200 usec=552
[2024-05-14T19:29:09.414] backfill: Started JobId=80 in longjobs on compute[00-01]
[2024-05-14T19:29:15.042] _slurm_rpc_submit_batch_job: JobId=81 InitPrio=10037 usec=279
[2024-05-14T19:29:16.149] preempted JobId=80 has been killed to reclaim resources for JobId=81
[2024-05-14T19:29:16.323] sched: Allocate JobId=81 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T19:29:16.384] _job_complete: JobId=81 WEXITSTATUS 0
[2024-05-14T19:29:16.403] _job_complete: JobId=81 done
[2024-05-14T19:31:39.636] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:31:39.637] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:36:40.161] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:36:40.161] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:40:13.413] killing old slurmctld[32228]
[2024-05-14T19:40:13.414] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T19:40:13.458] Saving all slurm state
[2024-05-14T19:40:13.552] layouts: all layouts are now unloaded.
[2024-05-14T19:40:13.558] error: chdir(/var/log): Permission denied
[2024-05-14T19:40:13.558] Job accounting information stored, but details not gathered
[2024-05-14T19:40:13.558] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T19:40:13.562] No memory enforcing mechanism configured.
[2024-05-14T19:40:13.562] layouts: no layout to initialize
[2024-05-14T19:40:13.567] layouts: loading entities/relations information
[2024-05-14T19:40:13.567] Recovered state of 2 nodes
[2024-05-14T19:40:13.567] Recovered information about 0 jobs
[2024-05-14T19:40:13.567] cons_res: select_p_node_init
[2024-05-14T19:40:13.567] cons_res: preparing for 2 partitions
[2024-05-14T19:40:13.568] Recovered state of 0 reservations
[2024-05-14T19:40:13.568] _preserve_plugins: backup_controller not specified
[2024-05-14T19:40:13.568] cons_res: select_p_reconfigure
[2024-05-14T19:40:13.568] cons_res: select_p_node_init
[2024-05-14T19:40:13.568] cons_res: preparing for 2 partitions
[2024-05-14T19:40:13.568] Running as primary controller
[2024-05-14T19:40:13.568] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T19:40:13.568] No parameter for mcs plugin, default values set
[2024-05-14T19:40:13.568] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T19:40:25.651] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T19:40:25.673] Saving all slurm state
[2024-05-14T19:40:25.716] layouts: all layouts are now unloaded.
[2024-05-14T19:40:39.442] error: chdir(/var/log): Permission denied
[2024-05-14T19:40:39.442] Job accounting information stored, but details not gathered
[2024-05-14T19:40:39.442] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T19:40:39.445] No memory enforcing mechanism configured.
[2024-05-14T19:40:39.445] layouts: no layout to initialize
[2024-05-14T19:40:39.499] layouts: loading entities/relations information
[2024-05-14T19:40:39.499] Recovered state of 2 nodes
[2024-05-14T19:40:39.499] Recovered information about 0 jobs
[2024-05-14T19:40:39.500] cons_res: select_p_node_init
[2024-05-14T19:40:39.500] cons_res: preparing for 2 partitions
[2024-05-14T19:40:39.500] Recovered state of 0 reservations
[2024-05-14T19:40:39.500] _preserve_plugins: backup_controller not specified
[2024-05-14T19:40:39.500] cons_res: select_p_reconfigure
[2024-05-14T19:40:39.500] cons_res: select_p_node_init
[2024-05-14T19:40:39.500] cons_res: preparing for 2 partitions
[2024-05-14T19:40:39.500] Running as primary controller
[2024-05-14T19:40:39.501] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T19:40:39.501] No parameter for mcs plugin, default values set
[2024-05-14T19:40:39.501] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T19:41:39.116] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T19:42:40.979] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:42:41.121] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:42:41.135] Node compute00 now responding
[2024-05-14T19:42:41.311] Node compute01 now responding
[2024-05-14T19:43:02.654] Invalid node state transition requested for node compute00 from=IDLE to=RESUME
[2024-05-14T19:43:02.654] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-05-14T19:43:58.016] _slurm_rpc_submit_batch_job: JobId=82 InitPrio=200 usec=1826
[2024-05-14T19:43:58.706] sched: Allocate JobId=82 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T19:44:36.459] _slurm_rpc_submit_batch_job: JobId=83 InitPrio=10037 usec=281
[2024-05-14T19:45:00.619] _slurm_rpc_submit_batch_job: JobId=84 InitPrio=10037 usec=289
[2024-05-14T19:45:01.146] _slurm_rpc_submit_batch_job: JobId=85 InitPrio=10037 usec=318
[2024-05-14T19:45:01.516] _slurm_rpc_submit_batch_job: JobId=86 InitPrio=10037 usec=282
[2024-05-14T19:45:01.770] _slurm_rpc_submit_batch_job: JobId=87 InitPrio=10037 usec=325
[2024-05-14T19:45:02.044] _slurm_rpc_submit_batch_job: JobId=88 InitPrio=10037 usec=296
[2024-05-14T19:45:02.251] _slurm_rpc_submit_batch_job: JobId=89 InitPrio=10037 usec=309
[2024-05-14T19:45:02.443] _slurm_rpc_submit_batch_job: JobId=90 InitPrio=10037 usec=300
[2024-05-14T19:45:02.634] _slurm_rpc_submit_batch_job: JobId=91 InitPrio=10037 usec=447
[2024-05-14T19:45:02.843] _slurm_rpc_submit_batch_job: JobId=92 InitPrio=10037 usec=380
[2024-05-14T19:45:03.035] _slurm_rpc_submit_batch_job: JobId=93 InitPrio=10037 usec=305
[2024-05-14T19:45:03.627] _slurm_rpc_submit_batch_job: JobId=94 InitPrio=10037 usec=317
[2024-05-14T19:45:39.744] error: Nodes compute[00-01] not responding
[2024-05-14T19:45:39.938] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:45:40.144] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:47:24.619] _slurm_rpc_submit_batch_job: JobId=95 InitPrio=200 usec=334
[2024-05-14T19:47:46.137] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=95 uid 1001
[2024-05-14T19:47:55.579] _slurm_rpc_submit_batch_job: JobId=96 InitPrio=200 usec=283
[2024-05-14T19:48:03.848] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=82 uid 1001
[2024-05-14T19:48:04.042] sched: Allocate JobId=83 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.045] sched: Allocate JobId=84 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.047] sched: Allocate JobId=85 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.049] sched: Allocate JobId=86 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.056] sched: Allocate JobId=87 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.063] sched: Allocate JobId=88 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.071] sched: Allocate JobId=89 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.078] sched: Allocate JobId=90 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.085] sched: Allocate JobId=91 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.092] sched: Allocate JobId=92 NodeList=compute00 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.099] sched: Allocate JobId=93 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.106] sched: Allocate JobId=94 NodeList=compute01 #CPUs=1 Partition=shortjobs
[2024-05-14T19:48:04.253] _job_complete: JobId=84 WEXITSTATUS 0
[2024-05-14T19:48:04.273] _job_complete: JobId=84 done
[2024-05-14T19:48:04.273] _job_complete: JobId=83 WEXITSTATUS 0
[2024-05-14T19:48:04.288] _job_complete: JobId=83 done
[2024-05-14T19:48:04.289] _job_complete: JobId=86 WEXITSTATUS 0
[2024-05-14T19:48:04.299] _job_complete: JobId=86 done
[2024-05-14T19:48:04.299] _job_complete: JobId=88 WEXITSTATUS 0
[2024-05-14T19:48:04.318] _job_complete: JobId=88 done
[2024-05-14T19:48:04.318] _job_complete: JobId=85 WEXITSTATUS 0
[2024-05-14T19:48:04.332] _job_complete: JobId=85 done
[2024-05-14T19:48:04.332] _job_complete: JobId=87 WEXITSTATUS 0
[2024-05-14T19:48:04.351] _job_complete: JobId=87 done
[2024-05-14T19:48:04.351] _job_complete: JobId=89 WEXITSTATUS 0
[2024-05-14T19:48:04.368] _job_complete: JobId=89 done
[2024-05-14T19:48:04.368] _job_complete: JobId=91 WEXITSTATUS 0
[2024-05-14T19:48:04.378] _job_complete: JobId=91 done
[2024-05-14T19:48:04.380] _job_complete: JobId=92 WEXITSTATUS 0
[2024-05-14T19:48:04.392] _job_complete: JobId=92 done
[2024-05-14T19:48:06.341] _job_complete: JobId=90 WEXITSTATUS 0
[2024-05-14T19:48:06.351] _job_complete: JobId=90 done
[2024-05-14T19:48:06.375] _job_complete: JobId=93 WEXITSTATUS 0
[2024-05-14T19:48:06.383] _job_complete: JobId=93 done
[2024-05-14T19:48:06.406] _job_complete: JobId=94 WEXITSTATUS 0
[2024-05-14T19:48:06.427] _job_complete: JobId=94 done
[2024-05-14T19:48:06.504] sched: Allocate JobId=96 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T19:48:18.539] _slurm_rpc_submit_batch_job: JobId=97 InitPrio=200 usec=348
[2024-05-14T19:48:19.440] sched: Allocate JobId=97 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T19:48:46.570] _slurm_rpc_submit_batch_job: JobId=98 InitPrio=200 usec=279
[2024-05-14T19:48:46.728] sched: Allocate JobId=98 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T19:48:47.259] _slurm_rpc_submit_batch_job: JobId=99 InitPrio=200 usec=328
[2024-05-14T19:48:47.851] _slurm_rpc_submit_batch_job: JobId=100 InitPrio=200 usec=314
[2024-05-14T19:48:49.929] sched: Allocate JobId=99 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T19:50:39.305] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:50:39.549] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T19:55:40.197] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T19:55:40.291] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:00:40.091] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:00:40.347] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:04:02.982] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=97 uid 1001
[2024-05-14T20:04:03.179] sched: Allocate JobId=100 NodeList=compute[00-01] #CPUs=4 Partition=longjobs
[2024-05-14T20:04:07.365] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=98 uid 1001
[2024-05-14T20:04:10.566] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=99 uid 1001
[2024-05-14T20:05:22.182] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=100 uid 1001
[2024-05-14T20:05:28.069] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=96 uid 1001
[2024-05-14T20:05:39.483] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:05:39.497] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:10:39.552] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:10:39.554] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:13:34.526] killing old slurmctld[2284]
[2024-05-14T20:13:34.526] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T20:13:34.586] Saving all slurm state
[2024-05-14T20:13:34.667] layouts: all layouts are now unloaded.
[2024-05-14T20:13:34.676] error: chdir(/var/log): Permission denied
[2024-05-14T20:13:34.676] Job accounting information stored, but details not gathered
[2024-05-14T20:13:34.677] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:13:34.684] No memory enforcing mechanism configured.
[2024-05-14T20:13:34.684] layouts: no layout to initialize
[2024-05-14T20:13:34.689] layouts: loading entities/relations information
[2024-05-14T20:13:34.690] Recovered state of 2 nodes
[2024-05-14T20:13:34.690] Recovered information about 0 jobs
[2024-05-14T20:13:34.690] cons_res: select_p_node_init
[2024-05-14T20:13:34.690] cons_res: preparing for 2 partitions
[2024-05-14T20:13:34.690] Recovered state of 0 reservations
[2024-05-14T20:13:34.690] _preserve_plugins: backup_controller not specified
[2024-05-14T20:13:34.690] cons_res: select_p_reconfigure
[2024-05-14T20:13:34.690] cons_res: select_p_node_init
[2024-05-14T20:13:34.690] cons_res: preparing for 2 partitions
[2024-05-14T20:13:34.690] Running as primary controller
[2024-05-14T20:13:34.692] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T20:13:34.692] No parameter for mcs plugin, default values set
[2024-05-14T20:13:34.692] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T20:13:41.293] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T20:13:41.373] Saving all slurm state
[2024-05-14T20:13:41.425] layouts: all layouts are now unloaded.
[2024-05-14T20:13:48.820] error: chdir(/var/log): Permission denied
[2024-05-14T20:13:48.821] Job accounting information stored, but details not gathered
[2024-05-14T20:13:48.821] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:13:48.823] No memory enforcing mechanism configured.
[2024-05-14T20:13:48.823] layouts: no layout to initialize
[2024-05-14T20:13:48.832] layouts: loading entities/relations information
[2024-05-14T20:13:48.832] Recovered state of 2 nodes
[2024-05-14T20:13:48.833] Recovered information about 0 jobs
[2024-05-14T20:13:48.833] cons_res: select_p_node_init
[2024-05-14T20:13:48.833] cons_res: preparing for 2 partitions
[2024-05-14T20:13:48.833] Recovered state of 0 reservations
[2024-05-14T20:13:48.833] _preserve_plugins: backup_controller not specified
[2024-05-14T20:13:48.833] cons_res: select_p_reconfigure
[2024-05-14T20:13:48.833] cons_res: select_p_node_init
[2024-05-14T20:13:48.833] cons_res: preparing for 2 partitions
[2024-05-14T20:13:48.833] Running as primary controller
[2024-05-14T20:13:48.833] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T20:13:48.833] No parameter for mcs plugin, default values set
[2024-05-14T20:13:48.833] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T20:14:48.380] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T20:15:41.672] killing old slurmctld[5055]
[2024-05-14T20:15:41.673] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T20:15:41.691] Saving all slurm state
[2024-05-14T20:15:41.846] layouts: all layouts are now unloaded.
[2024-05-14T20:15:41.853] error: chdir(/var/log): Permission denied
[2024-05-14T20:15:41.853] Job accounting information stored, but details not gathered
[2024-05-14T20:15:41.853] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:15:41.875] No memory enforcing mechanism configured.
[2024-05-14T20:15:41.875] layouts: no layout to initialize
[2024-05-14T20:15:41.890] layouts: loading entities/relations information
[2024-05-14T20:15:41.890] Recovered state of 2 nodes
[2024-05-14T20:15:41.890] Recovered information about 0 jobs
[2024-05-14T20:15:41.890] cons_res: select_p_node_init
[2024-05-14T20:15:41.890] cons_res: preparing for 2 partitions
[2024-05-14T20:15:41.890] Recovered state of 0 reservations
[2024-05-14T20:15:41.890] _preserve_plugins: backup_controller not specified
[2024-05-14T20:15:41.891] cons_res: select_p_reconfigure
[2024-05-14T20:15:41.891] cons_res: select_p_node_init
[2024-05-14T20:15:41.891] cons_res: preparing for 2 partitions
[2024-05-14T20:15:41.891] Running as primary controller
[2024-05-14T20:15:41.891] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T20:15:41.891] No parameter for mcs plugin, default values set
[2024-05-14T20:15:41.891] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T20:15:44.990] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T20:15:45.074] Saving all slurm state
[2024-05-14T20:15:45.122] layouts: all layouts are now unloaded.
[2024-05-14T20:15:52.027] error: chdir(/var/log): Permission denied
[2024-05-14T20:15:52.027] Job accounting information stored, but details not gathered
[2024-05-14T20:15:52.028] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:15:52.030] No memory enforcing mechanism configured.
[2024-05-14T20:15:52.030] layouts: no layout to initialize
[2024-05-14T20:15:52.040] layouts: loading entities/relations information
[2024-05-14T20:15:52.040] Recovered state of 2 nodes
[2024-05-14T20:15:52.040] Recovered information about 0 jobs
[2024-05-14T20:15:52.040] cons_res: select_p_node_init
[2024-05-14T20:15:52.040] cons_res: preparing for 2 partitions
[2024-05-14T20:15:52.040] Recovered state of 0 reservations
[2024-05-14T20:15:52.040] _preserve_plugins: backup_controller not specified
[2024-05-14T20:15:52.040] cons_res: select_p_reconfigure
[2024-05-14T20:15:52.040] cons_res: select_p_node_init
[2024-05-14T20:15:52.040] cons_res: preparing for 2 partitions
[2024-05-14T20:15:52.040] Running as primary controller
[2024-05-14T20:15:52.040] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T20:15:52.040] No parameter for mcs plugin, default values set
[2024-05-14T20:15:52.040] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T20:16:52.291] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T20:17:57.856] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:17:57.875] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:17:58.057] Node compute01 now responding
[2024-05-14T20:17:58.725] Node compute00 now responding
[2024-05-14T20:18:33.119] _get_job_parts: invalid partition specified: longjobs
[2024-05-14T20:18:33.119] _slurm_rpc_submit_batch_job: Invalid partition name specified
[2024-05-14T20:20:52.105] error: Nodes compute[00-01] not responding
[2024-05-14T20:20:52.317] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:20:52.319] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:22:05.712] _slurm_rpc_submit_batch_job: JobId=101 InitPrio=10100 usec=4554
[2024-05-14T20:22:05.725] backfill: Started JobId=101 in fair on compute[00-01]
[2024-05-14T20:22:16.067] _slurm_rpc_submit_batch_job: JobId=102 InitPrio=137 usec=298
[2024-05-14T20:22:41.380] _slurm_rpc_submit_batch_job: JobId=103 InitPrio=137 usec=278
[2024-05-14T20:24:00.292] _slurm_rpc_submit_batch_job: JobId=104 InitPrio=10100 usec=282
[2024-05-14T20:24:00.346] sched: Allocate JobId=104 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-14T20:25:22.724] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=101 uid 1001
[2024-05-14T20:25:24.946] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=104 uid 1001
[2024-05-14T20:25:25.111] sched: Allocate JobId=102 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T20:25:25.113] sched: Allocate JobId=103 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T20:25:42.131] _slurm_rpc_submit_batch_job: JobId=105 InitPrio=10100 usec=288
[2024-05-14T20:25:52.534] _job_complete: JobId=103 WEXITSTATUS 0
[2024-05-14T20:25:52.545] _job_complete: JobId=103 done
[2024-05-14T20:25:52.659] _job_complete: JobId=102 WEXITSTATUS 0
[2024-05-14T20:25:52.666] _job_complete: JobId=102 done
[2024-05-14T20:25:52.723] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:25:52.764] sched: Allocate JobId=105 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-14T20:25:52.764] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:26:56.101] _slurm_rpc_submit_batch_job: JobId=106 InitPrio=137 usec=302
[2024-05-14T20:26:57.045] _slurm_rpc_submit_batch_job: JobId=107 InitPrio=137 usec=279
[2024-05-14T20:26:57.650] _slurm_rpc_submit_batch_job: JobId=108 InitPrio=137 usec=279
[2024-05-14T20:27:09.171] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=105 uid 1001
[2024-05-14T20:27:09.464] sched: Allocate JobId=106 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T20:27:09.467] sched: Allocate JobId=107 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T20:27:09.469] sched: Allocate JobId=108 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T20:27:16.608] _slurm_rpc_submit_batch_job: JobId=109 InitPrio=137 usec=300
[2024-05-14T20:27:17.187] _slurm_rpc_submit_batch_job: JobId=110 InitPrio=137 usec=299
[2024-05-14T20:27:17.585] sched: Allocate JobId=109 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T20:27:17.589] sched: Allocate JobId=110 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T20:27:17.589] _slurm_rpc_submit_batch_job: JobId=111 InitPrio=137 usec=345
[2024-05-14T20:27:20.721] sched: Allocate JobId=111 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T20:27:35.807] _slurm_rpc_submit_batch_job: JobId=112 InitPrio=10100 usec=335
[2024-05-14T20:28:09.079] _slurm_rpc_submit_batch_job: JobId=113 InitPrio=137 usec=300
[2024-05-14T20:28:09.715] _slurm_rpc_submit_batch_job: JobId=114 InitPrio=137 usec=286
[2024-05-14T20:28:10.261] _slurm_rpc_submit_batch_job: JobId=115 InitPrio=137 usec=306
[2024-05-14T20:28:10.619] _slurm_rpc_submit_batch_job: JobId=116 InitPrio=137 usec=342
[2024-05-14T20:30:52.852] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:30:52.853] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:35:53.190] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:35:53.341] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:40:53.362] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T20:40:53.406] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T20:41:34.587] killing old slurmctld[5327]
[2024-05-14T20:41:34.587] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T20:41:34.694] Saving all slurm state
[2024-05-14T20:41:34.737] layouts: all layouts are now unloaded.
[2024-05-14T20:41:34.740] error: chdir(/var/log): Permission denied
[2024-05-14T20:41:34.740] Job accounting information stored, but details not gathered
[2024-05-14T20:41:34.740] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:41:34.740] error: Couldn't find the specified plugin name for GANG looking at all files
[2024-05-14T20:41:34.744] error: cannot find preempt plugin for GANG
[2024-05-14T20:41:34.744] error: cannot create preempt context for GANG
[2024-05-14T20:41:34.744] fatal: failed to initialize preempt plugin
[2024-05-14T20:41:44.819] error: chdir(/var/log): Permission denied
[2024-05-14T20:41:44.819] Job accounting information stored, but details not gathered
[2024-05-14T20:41:44.819] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:41:44.820] error: Couldn't find the specified plugin name for GANG looking at all files
[2024-05-14T20:41:44.820] error: cannot find preempt plugin for GANG
[2024-05-14T20:41:44.820] error: cannot create preempt context for GANG
[2024-05-14T20:41:44.820] fatal: failed to initialize preempt plugin
[2024-05-14T20:42:31.782] error: chdir(/var/log): Permission denied
[2024-05-14T20:42:31.782] Job accounting information stored, but details not gathered
[2024-05-14T20:42:31.782] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:42:31.783] error: Couldn't find the specified plugin name for GANG looking at all files
[2024-05-14T20:42:31.783] error: cannot find preempt plugin for GANG
[2024-05-14T20:42:31.783] error: cannot create preempt context for GANG
[2024-05-14T20:42:31.783] fatal: failed to initialize preempt plugin
[2024-05-14T20:43:15.664] error: chdir(/var/log): Permission denied
[2024-05-14T20:43:15.665] Job accounting information stored, but details not gathered
[2024-05-14T20:43:15.665] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:43:15.665] error: Couldn't find the specified plugin name for GANG looking at all files
[2024-05-14T20:43:15.665] error: cannot find preempt plugin for GANG
[2024-05-14T20:43:15.665] error: cannot create preempt context for GANG
[2024-05-14T20:43:15.665] fatal: failed to initialize preempt plugin
[2024-05-14T20:43:23.426] error: chdir(/var/log): Permission denied
[2024-05-14T20:43:23.426] Job accounting information stored, but details not gathered
[2024-05-14T20:43:23.426] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:43:23.426] error: Couldn't find the specified plugin name for GANG looking at all files
[2024-05-14T20:43:23.426] error: cannot find preempt plugin for GANG
[2024-05-14T20:43:23.426] error: cannot create preempt context for GANG
[2024-05-14T20:43:23.426] fatal: failed to initialize preempt plugin
[2024-05-14T20:44:09.534] error: chdir(/var/log): Permission denied
[2024-05-14T20:44:09.535] Job accounting information stored, but details not gathered
[2024-05-14T20:44:09.535] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:44:09.536] error: Couldn't find the specified plugin name for GANG looking at all files
[2024-05-14T20:44:09.538] error: cannot find preempt plugin for GANG
[2024-05-14T20:44:09.539] error: cannot create preempt context for GANG
[2024-05-14T20:44:09.539] fatal: failed to initialize preempt plugin
[2024-05-14T20:44:19.015] error: chdir(/var/log): Permission denied
[2024-05-14T20:44:19.015] Job accounting information stored, but details not gathered
[2024-05-14T20:44:19.015] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:44:19.015] error: Couldn't find the specified plugin name for GANG looking at all files
[2024-05-14T20:44:19.016] error: cannot find preempt plugin for GANG
[2024-05-14T20:44:19.016] error: cannot create preempt context for GANG
[2024-05-14T20:44:19.016] fatal: failed to initialize preempt plugin
[2024-05-14T20:45:58.373] error: chdir(/var/log): Permission denied
[2024-05-14T20:45:58.382] Job accounting information stored, but details not gathered
[2024-05-14T20:45:58.385] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T20:45:58.398] error: Couldn't find the specified plugin name for GANG looking at all files
[2024-05-14T20:45:58.412] error: cannot find preempt plugin for GANG
[2024-05-14T20:45:58.412] error: cannot create preempt context for GANG
[2024-05-14T20:45:58.412] fatal: failed to initialize preempt plugin
[2024-05-14T21:00:03.611] error: chdir(/var/log): Permission denied
[2024-05-14T21:00:03.611] Job accounting information stored, but details not gathered
[2024-05-14T21:00:03.612] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T21:00:03.654] No memory enforcing mechanism configured.
[2024-05-14T21:00:03.655] layouts: no layout to initialize
[2024-05-14T21:00:03.727] layouts: loading entities/relations information
[2024-05-14T21:00:03.728] Recovered state of 2 nodes
[2024-05-14T21:00:03.730] Recovered JobId=106 Assoc=0
[2024-05-14T21:00:03.736] Recovered JobId=107 Assoc=0
[2024-05-14T21:00:03.739] Recovered JobId=108 Assoc=0
[2024-05-14T21:00:03.743] Recovered JobId=109 Assoc=0
[2024-05-14T21:00:03.746] Recovered JobId=110 Assoc=0
[2024-05-14T21:00:03.749] Recovered JobId=111 Assoc=0
[2024-05-14T21:00:03.752] Recovered JobId=112 Assoc=0
[2024-05-14T21:00:03.753] Recovered JobId=113 Assoc=0
[2024-05-14T21:00:03.753] Recovered JobId=114 Assoc=0
[2024-05-14T21:00:03.753] Recovered JobId=115 Assoc=0
[2024-05-14T21:00:03.753] Recovered JobId=116 Assoc=0
[2024-05-14T21:00:03.753] Recovered information about 11 jobs
[2024-05-14T21:00:03.753] cons_res: select_p_node_init
[2024-05-14T21:00:03.753] cons_res: preparing for 2 partitions
[2024-05-14T21:00:03.754] error: select/cons_res: job overflow: could not find idle resources for JobId=110
[2024-05-14T21:00:03.754] error: select/cons_res: job overflow: could not find idle resources for JobId=111
[2024-05-14T21:00:03.756] Recovered state of 0 reservations
[2024-05-14T21:00:03.757] gang: suspending JobId=110: Job is not running
[2024-05-14T21:00:03.757] gang: suspending JobId=111: Job is not running
[2024-05-14T21:00:03.757] _preserve_plugins: backup_controller not specified
[2024-05-14T21:00:03.757] cons_res: select_p_reconfigure
[2024-05-14T21:00:03.757] cons_res: select_p_node_init
[2024-05-14T21:00:03.758] cons_res: preparing for 2 partitions
[2024-05-14T21:00:03.758] error: select/cons_res: job overflow: could not find idle resources for JobId=110
[2024-05-14T21:00:03.758] error: select/cons_res: job overflow: could not find idle resources for JobId=111
[2024-05-14T21:00:03.758] Running as primary controller
[2024-05-14T21:00:03.764] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T21:00:03.764] No parameter for mcs plugin, default values set
[2024-05-14T21:00:03.764] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T21:00:08.023] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T21:00:08.128] Saving all slurm state
[2024-05-14T21:00:08.181] layouts: all layouts are now unloaded.
[2024-05-14T21:00:13.497] error: chdir(/var/log): Permission denied
[2024-05-14T21:00:13.498] Job accounting information stored, but details not gathered
[2024-05-14T21:00:13.498] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T21:00:13.500] No memory enforcing mechanism configured.
[2024-05-14T21:00:13.500] layouts: no layout to initialize
[2024-05-14T21:00:13.590] layouts: loading entities/relations information
[2024-05-14T21:00:13.590] Recovered state of 2 nodes
[2024-05-14T21:00:13.591] Recovered JobId=106 Assoc=0
[2024-05-14T21:00:13.616] Recovered JobId=107 Assoc=0
[2024-05-14T21:00:13.620] Recovered JobId=108 Assoc=0
[2024-05-14T21:00:13.625] Recovered JobId=109 Assoc=0
[2024-05-14T21:00:13.629] Recovered JobId=110 Assoc=0
[2024-05-14T21:00:13.633] Recovered JobId=111 Assoc=0
[2024-05-14T21:00:13.637] Recovered JobId=112 Assoc=0
[2024-05-14T21:00:13.637] Recovered JobId=113 Assoc=0
[2024-05-14T21:00:13.637] Recovered JobId=114 Assoc=0
[2024-05-14T21:00:13.637] Recovered JobId=115 Assoc=0
[2024-05-14T21:00:13.637] Recovered JobId=116 Assoc=0
[2024-05-14T21:00:13.637] Recovered information about 11 jobs
[2024-05-14T21:00:13.637] cons_res: select_p_node_init
[2024-05-14T21:00:13.638] cons_res: preparing for 2 partitions
[2024-05-14T21:00:13.638] error: select/cons_res: job overflow: could not find idle resources for JobId=110
[2024-05-14T21:00:13.638] error: select/cons_res: job overflow: could not find idle resources for JobId=111
[2024-05-14T21:00:13.639] Recovered state of 0 reservations
[2024-05-14T21:00:13.639] gang: suspending JobId=110: Job is not running
[2024-05-14T21:00:13.639] gang: suspending JobId=111: Job is not running
[2024-05-14T21:00:13.639] _preserve_plugins: backup_controller not specified
[2024-05-14T21:00:13.639] cons_res: select_p_reconfigure
[2024-05-14T21:00:13.639] cons_res: select_p_node_init
[2024-05-14T21:00:13.639] cons_res: preparing for 2 partitions
[2024-05-14T21:00:13.639] error: select/cons_res: job overflow: could not find idle resources for JobId=110
[2024-05-14T21:00:13.639] error: select/cons_res: job overflow: could not find idle resources for JobId=111
[2024-05-14T21:00:13.639] Running as primary controller
[2024-05-14T21:00:13.640] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T21:00:13.640] No parameter for mcs plugin, default values set
[2024-05-14T21:00:13.640] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T21:01:13.486] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T21:01:40.563] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=113 uid 0
[2024-05-14T21:01:44.499] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=112 uid 0
[2024-05-14T21:01:46.994] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=111 uid 0
[2024-05-14T21:01:50.229] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=114 uid 0
[2024-05-14T21:01:59.345] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=110 uid 0
[2024-05-14T21:02:01.761] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=109 uid 0
[2024-05-14T21:02:03.828] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=108 uid 0
[2024-05-14T21:02:05.247] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=107 uid 0
[2024-05-14T21:02:13.065] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=106 uid 0
[2024-05-14T21:02:13.742] gang: suspending JobId=106: Job/step already completing or completed
[2024-05-14T21:02:13.742] gang: suspending JobId=107: Job/step already completing or completed
[2024-05-14T21:02:15.909] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=111 uid 0
[2024-05-14T21:02:19.119] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=109 uid 0
[2024-05-14T21:02:43.743] gang: suspending JobId=110: Job/step already completing or completed
[2024-05-14T21:02:43.744] gang: suspending JobId=111: Job/step already completing or completed
[2024-05-14T21:02:48.178] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=111 uid 0
[2024-05-14T21:03:13.750] gang: suspending JobId=106: Job/step already completing or completed
[2024-05-14T21:03:13.750] gang: suspending JobId=107: Job/step already completing or completed
[2024-05-14T21:03:28.643] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=111 uid 0
[2024-05-14T21:03:30.627] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=111 uid 0
[2024-05-14T21:03:43.751] gang: suspending JobId=110: Job/step already completing or completed
[2024-05-14T21:03:43.751] gang: suspending JobId=111: Job/step already completing or completed
[2024-05-14T21:04:13.752] gang: suspending JobId=106: Job/step already completing or completed
[2024-05-14T21:04:13.752] gang: suspending JobId=107: Job/step already completing or completed
[2024-05-14T21:04:43.768] gang: suspending JobId=110: Job/step already completing or completed
[2024-05-14T21:04:43.768] gang: suspending JobId=111: Job/step already completing or completed
[2024-05-14T21:05:01.268] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=115 uid 0
[2024-05-14T21:05:01.268] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=116 uid 0
[2024-05-14T21:05:13.333] error: Nodes compute[00-01] not responding
[2024-05-14T21:05:13.769] gang: suspending JobId=106: Job/step already completing or completed
[2024-05-14T21:05:13.769] gang: suspending JobId=107: Job/step already completing or completed
[2024-05-14T21:05:43.769] gang: suspending JobId=110: Job/step already completing or completed
[2024-05-14T21:05:43.769] gang: suspending JobId=111: Job/step already completing or completed
[2024-05-14T21:06:13.799] gang: suspending JobId=106: Job/step already completing or completed
[2024-05-14T21:06:13.799] gang: suspending JobId=107: Job/step already completing or completed
[2024-05-14T21:06:43.800] gang: suspending JobId=110: Job/step already completing or completed
[2024-05-14T21:06:43.800] gang: suspending JobId=111: Job/step already completing or completed
[2024-05-14T21:06:57.190] cleanup_completing: JobId=106 completion process took 270 seconds
[2024-05-14T21:06:57.191] cleanup_completing: JobId=107 completion process took 292 seconds
[2024-05-14T21:06:57.191] cleanup_completing: JobId=110 completion process took 314 seconds
[2024-05-14T21:06:57.191] cleanup_completing: JobId=111 completion process took 314 seconds
[2024-05-14T21:06:57.191] cleanup_completing: JobId=108 completion process took 294 seconds
[2024-05-14T21:06:57.191] cleanup_completing: JobId=109 completion process took 296 seconds
[2024-05-14T21:06:57.191] error: Nodes compute[00-01] not responding, setting DOWN
[2024-05-14T21:11:50.274] update_node: node compute00 state set to IDLE
[2024-05-14T21:11:57.071] update_node: node compute01 state set to IDLE
[2024-05-14T21:13:35.748] _slurm_rpc_submit_batch_job: JobId=117 InitPrio=10037 usec=13694
[2024-05-14T21:15:13.298] error: Nodes compute[00-01] not responding
[2024-05-14T21:18:41.305] error: Nodes compute[00-01] not responding, setting DOWN
[2024-05-14T21:20:15.493] killing old slurmctld[2386]
[2024-05-14T21:20:15.493] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T21:20:15.530] Saving all slurm state
[2024-05-14T21:20:15.603] layouts: all layouts are now unloaded.
[2024-05-14T21:20:15.608] error: chdir(/var/log): Permission denied
[2024-05-14T21:20:15.608] Job accounting information stored, but details not gathered
[2024-05-14T21:20:15.608] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T21:20:15.629] No memory enforcing mechanism configured.
[2024-05-14T21:20:15.629] layouts: no layout to initialize
[2024-05-14T21:20:15.642] layouts: loading entities/relations information
[2024-05-14T21:20:15.642] Recovered state of 2 nodes
[2024-05-14T21:20:15.642] Down nodes: compute[00-01]
[2024-05-14T21:20:15.642] Recovered JobId=117 Assoc=0
[2024-05-14T21:20:15.642] Recovered information about 1 jobs
[2024-05-14T21:20:15.642] cons_res: select_p_node_init
[2024-05-14T21:20:15.642] cons_res: preparing for 2 partitions
[2024-05-14T21:20:15.642] Recovered state of 0 reservations
[2024-05-14T21:20:15.642] _preserve_plugins: backup_controller not specified
[2024-05-14T21:20:15.642] cons_res: select_p_reconfigure
[2024-05-14T21:20:15.642] cons_res: select_p_node_init
[2024-05-14T21:20:15.642] cons_res: preparing for 2 partitions
[2024-05-14T21:20:15.642] Running as primary controller
[2024-05-14T21:20:15.643] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T21:20:15.643] No parameter for mcs plugin, default values set
[2024-05-14T21:20:15.643] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T21:20:47.831] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T21:20:47.903] Saving all slurm state
[2024-05-14T21:20:48.010] layouts: all layouts are now unloaded.
[2024-05-14T21:21:04.252] error: chdir(/var/log): Permission denied
[2024-05-14T21:21:04.252] Job accounting information stored, but details not gathered
[2024-05-14T21:21:04.252] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T21:21:04.254] No memory enforcing mechanism configured.
[2024-05-14T21:21:04.254] layouts: no layout to initialize
[2024-05-14T21:21:04.267] layouts: loading entities/relations information
[2024-05-14T21:21:04.267] Recovered state of 2 nodes
[2024-05-14T21:21:04.267] Down nodes: compute[00-01]
[2024-05-14T21:21:04.268] Recovered JobId=117 Assoc=0
[2024-05-14T21:21:04.268] Recovered information about 1 jobs
[2024-05-14T21:21:04.268] cons_res: select_p_node_init
[2024-05-14T21:21:04.268] cons_res: preparing for 2 partitions
[2024-05-14T21:21:04.268] Recovered state of 0 reservations
[2024-05-14T21:21:04.268] _preserve_plugins: backup_controller not specified
[2024-05-14T21:21:04.268] cons_res: select_p_reconfigure
[2024-05-14T21:21:04.268] cons_res: select_p_node_init
[2024-05-14T21:21:04.268] cons_res: preparing for 2 partitions
[2024-05-14T21:21:04.268] Running as primary controller
[2024-05-14T21:21:04.268] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T21:21:04.268] No parameter for mcs plugin, default values set
[2024-05-14T21:21:04.268] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T21:21:42.870] killing old slurmctld[3958]
[2024-05-14T21:21:42.871] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T21:21:42.929] Saving all slurm state
[2024-05-14T21:21:43.011] layouts: all layouts are now unloaded.
[2024-05-14T21:21:43.020] error: chdir(/var/log): Permission denied
[2024-05-14T21:21:43.020] Job accounting information stored, but details not gathered
[2024-05-14T21:21:43.020] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T21:21:43.026] No memory enforcing mechanism configured.
[2024-05-14T21:21:43.026] layouts: no layout to initialize
[2024-05-14T21:21:43.030] layouts: loading entities/relations information
[2024-05-14T21:21:43.031] Recovered state of 2 nodes
[2024-05-14T21:21:43.031] Down nodes: compute[00-01]
[2024-05-14T21:21:43.031] Recovered JobId=117 Assoc=0
[2024-05-14T21:21:43.031] Recovered information about 1 jobs
[2024-05-14T21:21:43.031] cons_res: select_p_node_init
[2024-05-14T21:21:43.031] cons_res: preparing for 2 partitions
[2024-05-14T21:21:43.031] Recovered state of 0 reservations
[2024-05-14T21:21:43.031] _preserve_plugins: backup_controller not specified
[2024-05-14T21:21:43.031] cons_res: select_p_reconfigure
[2024-05-14T21:21:43.031] cons_res: select_p_node_init
[2024-05-14T21:21:43.031] cons_res: preparing for 2 partitions
[2024-05-14T21:21:43.031] Running as primary controller
[2024-05-14T21:21:43.032] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T21:21:43.032] No parameter for mcs plugin, default values set
[2024-05-14T21:21:43.032] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T21:21:45.941] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T21:21:45.957] Saving all slurm state
[2024-05-14T21:21:45.990] layouts: all layouts are now unloaded.
[2024-05-14T21:21:50.484] error: chdir(/var/log): Permission denied
[2024-05-14T21:21:50.485] Job accounting information stored, but details not gathered
[2024-05-14T21:21:50.485] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T21:21:50.494] No memory enforcing mechanism configured.
[2024-05-14T21:21:50.494] layouts: no layout to initialize
[2024-05-14T21:21:50.504] layouts: loading entities/relations information
[2024-05-14T21:21:50.504] Recovered state of 2 nodes
[2024-05-14T21:21:50.504] Down nodes: compute[00-01]
[2024-05-14T21:21:50.504] Recovered JobId=117 Assoc=0
[2024-05-14T21:21:50.505] Recovered information about 1 jobs
[2024-05-14T21:21:50.505] cons_res: select_p_node_init
[2024-05-14T21:21:50.505] cons_res: preparing for 2 partitions
[2024-05-14T21:21:50.505] Recovered state of 0 reservations
[2024-05-14T21:21:50.505] _preserve_plugins: backup_controller not specified
[2024-05-14T21:21:50.505] cons_res: select_p_reconfigure
[2024-05-14T21:21:50.505] cons_res: select_p_node_init
[2024-05-14T21:21:50.505] cons_res: preparing for 2 partitions
[2024-05-14T21:21:50.505] Running as primary controller
[2024-05-14T21:21:50.505] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T21:21:50.506] No parameter for mcs plugin, default values set
[2024-05-14T21:21:50.506] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T21:22:25.311] update_node: node compute00 state set to IDLE
[2024-05-14T21:22:26.006] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T21:22:53.117] update_node: node compute01 state set to IDLE
[2024-05-14T21:23:37.040] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T21:23:37.070] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T21:23:37.219] Node compute01 now responding
[2024-05-14T21:23:37.505] sched: Allocate JobId=117 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T21:23:37.587] Node compute00 now responding
[2024-05-14T21:24:09.627] Invalid node state transition requested for node compute01 from=ALLOCATED to=RESUME
[2024-05-14T21:24:09.627] _slurm_rpc_update_node for compute01: Invalid node state specified
[2024-05-14T21:24:50.958] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=117 uid 0
[2024-05-14T21:25:49.321] _slurm_rpc_submit_batch_job: JobId=118 InitPrio=110 usec=1472
[2024-05-14T21:25:50.088] sched: Allocate JobId=118 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-14T21:25:57.182] _slurm_rpc_submit_batch_job: JobId=119 InitPrio=110 usec=341
[2024-05-14T21:25:57.293] sched: Allocate JobId=119 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-14T21:26:50.104] error: Nodes compute[00-01] not responding
[2024-05-14T21:26:50.295] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T21:26:50.486] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T21:26:53.422] _slurm_rpc_submit_batch_job: JobId=120 InitPrio=10037 usec=285
[2024-05-14T21:30:37.933] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=120 uid 1001
[2024-05-14T21:30:41.627] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=119 uid 1001
[2024-05-14T21:30:44.842] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=118 uid 1001
[2024-05-14T21:30:59.206] _slurm_rpc_submit_batch_job: JobId=121 InitPrio=10037 usec=1896
[2024-05-14T21:30:59.877] sched: Allocate JobId=121 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T21:31:09.708] _slurm_rpc_submit_batch_job: JobId=122 InitPrio=10037 usec=292
[2024-05-14T21:31:10.416] sched: Allocate JobId=122 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T21:31:10.458] _slurm_rpc_submit_batch_job: JobId=123 InitPrio=10037 usec=234
[2024-05-14T21:31:11.068] _slurm_rpc_submit_batch_job: JobId=124 InitPrio=10037 usec=290
[2024-05-14T21:31:11.580] _slurm_rpc_submit_batch_job: JobId=125 InitPrio=10037 usec=280
[2024-05-14T21:31:11.964] _slurm_rpc_submit_batch_job: JobId=126 InitPrio=10037 usec=283
[2024-05-14T21:31:13.472] sched: Allocate JobId=123 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T21:31:13.483] sched: Allocate JobId=124 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T21:31:32.876] _slurm_rpc_submit_batch_job: JobId=127 InitPrio=110 usec=300
[2024-05-14T21:31:50.446] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T21:31:50.462] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T21:33:37.492] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=121 uid 1001
[2024-05-14T21:33:37.660] sched: Allocate JobId=125 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T21:33:51.694] _slurm_rpc_submit_batch_job: JobId=128 InitPrio=10037 usec=333
[2024-05-14T21:34:04.664] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=122 uid 1001
[2024-05-14T21:34:04.786] sched: Allocate JobId=126 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T21:34:15.783] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=124 uid 1001
[2024-05-14T21:34:15.917] sched: Allocate JobId=128 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T21:36:51.051] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T21:36:51.071] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T21:41:51.289] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T21:41:51.297] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T21:46:51.377] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T21:46:51.392] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T21:47:17.806] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=127 uid 1001
[2024-05-14T21:47:17.839] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=128 uid 1001
[2024-05-14T21:47:17.839] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=125 uid 1001
[2024-05-14T21:47:17.839] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=126 uid 1001
[2024-05-14T21:47:17.853] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=123 uid 1001
[2024-05-14T21:51:50.868] error: Nodes compute[00-01] not responding
[2024-05-14T21:54:36.724] error: Nodes compute[00-01] not responding, setting DOWN
[2024-05-14T22:04:47.384] killing old slurmctld[4031]
[2024-05-14T22:04:47.385] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T22:04:47.421] Saving all slurm state
[2024-05-14T22:04:47.487] layouts: all layouts are now unloaded.
[2024-05-14T22:04:47.494] error: chdir(/var/log): Permission denied
[2024-05-14T22:04:47.494] Job accounting information stored, but details not gathered
[2024-05-14T22:04:47.494] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T22:04:47.500] No memory enforcing mechanism configured.
[2024-05-14T22:04:47.500] layouts: no layout to initialize
[2024-05-14T22:04:47.504] layouts: loading entities/relations information
[2024-05-14T22:04:47.504] Recovered state of 2 nodes
[2024-05-14T22:04:47.504] Down nodes: compute[00-01]
[2024-05-14T22:04:47.505] Recovered information about 0 jobs
[2024-05-14T22:04:47.505] cons_res: select_p_node_init
[2024-05-14T22:04:47.505] cons_res: preparing for 2 partitions
[2024-05-14T22:04:47.505] Recovered state of 0 reservations
[2024-05-14T22:04:47.506] _preserve_plugins: backup_controller not specified
[2024-05-14T22:04:47.506] cons_res: select_p_reconfigure
[2024-05-14T22:04:47.506] cons_res: select_p_node_init
[2024-05-14T22:04:47.506] cons_res: preparing for 2 partitions
[2024-05-14T22:04:47.506] Running as primary controller
[2024-05-14T22:04:47.506] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T22:04:47.506] No parameter for mcs plugin, default values set
[2024-05-14T22:04:47.506] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T22:04:54.914] Terminate signal (SIGINT or SIGTERM) received
[2024-05-14T22:04:54.926] Saving all slurm state
[2024-05-14T22:04:55.071] layouts: all layouts are now unloaded.
[2024-05-14T22:05:02.148] error: chdir(/var/log): Permission denied
[2024-05-14T22:05:02.148] Job accounting information stored, but details not gathered
[2024-05-14T22:05:02.148] slurmctld version 18.08.8 started on cluster linux
[2024-05-14T22:05:02.150] No memory enforcing mechanism configured.
[2024-05-14T22:05:02.150] layouts: no layout to initialize
[2024-05-14T22:05:02.159] layouts: loading entities/relations information
[2024-05-14T22:05:02.159] Recovered state of 2 nodes
[2024-05-14T22:05:02.159] Down nodes: compute[00-01]
[2024-05-14T22:05:02.159] Recovered information about 0 jobs
[2024-05-14T22:05:02.159] cons_res: select_p_node_init
[2024-05-14T22:05:02.159] cons_res: preparing for 2 partitions
[2024-05-14T22:05:02.159] Recovered state of 0 reservations
[2024-05-14T22:05:02.159] _preserve_plugins: backup_controller not specified
[2024-05-14T22:05:02.159] cons_res: select_p_reconfigure
[2024-05-14T22:05:02.159] cons_res: select_p_node_init
[2024-05-14T22:05:02.159] cons_res: preparing for 2 partitions
[2024-05-14T22:05:02.159] Running as primary controller
[2024-05-14T22:05:02.159] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-14T22:05:02.160] No parameter for mcs plugin, default values set
[2024-05-14T22:05:02.160] mcs: MCSParameters = (null). ondemand set.
[2024-05-14T22:06:02.623] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-14T22:06:29.438] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T22:06:29.670] Node compute01 now responding
[2024-05-14T22:06:29.670] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1715724322 last response=1715724302
[2024-05-14T22:06:30.202] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T22:06:30.459] Node compute00 now responding
[2024-05-14T22:06:30.459] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1715724325 last response=1715724302
[2024-05-14T22:06:54.469] update_node: node compute01 state set to IDLE
[2024-05-14T22:06:55.110] Node compute01 now responding
[2024-05-14T22:06:58.193] update_node: node compute00 state set to IDLE
[2024-05-14T22:06:58.305] Node compute00 now responding
[2024-05-14T22:07:29.371] _slurm_rpc_submit_batch_job: JobId=129 InitPrio=10000 usec=1579
[2024-05-14T22:07:29.830] backfill: Started JobId=129 in urgent on compute00
[2024-05-14T22:07:39.565] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=129 uid 1001
[2024-05-14T22:07:50.132] _slurm_rpc_submit_batch_job: JobId=130 InitPrio=10 usec=887
[2024-05-14T22:07:51.054] sched: Allocate JobId=130 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-14T22:08:03.617] _slurm_rpc_submit_batch_job: JobId=131 InitPrio=10000 usec=319
[2024-05-14T22:09:20.572] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=130 uid 1001
[2024-05-14T22:09:20.775] sched: Allocate JobId=131 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T22:09:35.006] _slurm_rpc_submit_batch_job: JobId=132 InitPrio=10000 usec=228
[2024-05-14T22:09:35.889] sched: Allocate JobId=132 NodeList=compute00 #CPUs=1 Partition=urgent
[2024-05-14T22:09:36.511] _slurm_rpc_submit_batch_job: JobId=133 InitPrio=10000 usec=286
[2024-05-14T22:09:37.440] _slurm_rpc_submit_batch_job: JobId=134 InitPrio=10000 usec=281
[2024-05-14T22:09:37.999] _slurm_rpc_submit_batch_job: JobId=135 InitPrio=10000 usec=279
[2024-05-14T22:09:38.483] _slurm_rpc_submit_batch_job: JobId=136 InitPrio=10000 usec=345
[2024-05-14T22:09:38.998] sched: Allocate JobId=133 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T22:09:39.000] sched: Allocate JobId=134 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T22:09:54.911] _slurm_rpc_submit_batch_job: JobId=137 InitPrio=10 usec=289
[2024-05-14T22:10:02.634] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T22:10:02.653] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T22:10:13.262] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=134 uid 1001
[2024-05-14T22:10:15.072] sched: Allocate JobId=135 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T22:10:17.028] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=133 uid 1001
[2024-05-14T22:10:17.139] sched: Allocate JobId=136 NodeList=compute01 #CPUs=1 Partition=urgent
[2024-05-14T22:10:18.941] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=132 uid 1001
[2024-05-14T22:12:42.036] _slurm_rpc_submit_batch_job: JobId=138 InitPrio=10 usec=314
[2024-05-14T22:13:38.628] _slurm_rpc_submit_batch_job: JobId=139 InitPrio=10000 usec=3667
[2024-05-14T22:13:43.202] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=137 uid 1001
[2024-05-14T22:13:56.088] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=136 uid 1001
[2024-05-14T22:13:58.346] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=135 uid 1001
[2024-05-14T22:13:59.971] error: select/cons_res: node compute01 memory is under-allocated (0-500) for JobId=135
[2024-05-14T22:13:59.971] error: select/cons_res: node compute01 memory is under-allocated (0-500) for JobId=135
[2024-05-14T22:14:00.219] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=131 uid 1001
[2024-05-14T22:14:00.537] sched: Allocate JobId=139 NodeList=compute[00-01] #CPUs=4 Partition=urgent
[2024-05-14T22:14:24.539] _slurm_rpc_submit_batch_job: JobId=140 InitPrio=10000 usec=373
[2024-05-14T22:15:02.230] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T22:15:02.439] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T22:20:02.215] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-14T22:20:02.434] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-14T22:23:25.604] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=139 uid 1001
[2024-05-14T22:23:25.800] sched: Allocate JobId=140 NodeList=compute[00-01] #CPUs=4 Partition=urgent
[2024-05-14T22:23:28.358] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=138 uid 1001
[2024-05-14T22:23:33.444] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=140 uid 1001
[2024-05-16T17:08:58.957] error: chdir(/var/log): Permission denied
[2024-05-16T17:08:59.016] Job accounting information stored, but details not gathered
[2024-05-16T17:08:59.019] slurmctld version 18.08.8 started on cluster linux
[2024-05-16T17:08:59.226] No memory enforcing mechanism configured.
[2024-05-16T17:08:59.226] layouts: no layout to initialize
[2024-05-16T17:08:59.260] layouts: loading entities/relations information
[2024-05-16T17:08:59.263] Recovered state of 2 nodes
[2024-05-16T17:08:59.266] Recovered JobId=138 Assoc=0
[2024-05-16T17:08:59.335] Recovered JobId=139 Assoc=0
[2024-05-16T17:08:59.344] Recovered JobId=140 Assoc=0
[2024-05-16T17:08:59.352] Recovered information about 3 jobs
[2024-05-16T17:08:59.352] cons_res: select_p_node_init
[2024-05-16T17:08:59.352] cons_res: preparing for 2 partitions
[2024-05-16T17:08:59.356] Recovered state of 0 reservations
[2024-05-16T17:08:59.357] _preserve_plugins: backup_controller not specified
[2024-05-16T17:08:59.358] cons_res: select_p_reconfigure
[2024-05-16T17:08:59.358] cons_res: select_p_node_init
[2024-05-16T17:08:59.358] cons_res: preparing for 2 partitions
[2024-05-16T17:08:59.358] Running as primary controller
[2024-05-16T17:08:59.361] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-16T17:08:59.362] No parameter for mcs plugin, default values set
[2024-05-16T17:08:59.362] mcs: MCSParameters = (null). ondemand set.
[2024-05-16T17:09:59.695] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-16T17:13:59.513] error: Nodes compute[00-01] not responding
[2024-05-16T17:15:43.238] error: Nodes compute[00-01] not responding, setting DOWN
[2024-05-16T17:16:18.117] killing old slurmctld[1322]
[2024-05-16T17:16:18.117] Terminate signal (SIGINT or SIGTERM) received
[2024-05-16T17:16:18.491] Saving all slurm state
[2024-05-16T17:16:18.583] layouts: all layouts are now unloaded.
[2024-05-16T17:16:18.593] error: chdir(/var/log): Permission denied
[2024-05-16T17:16:18.593] Job accounting information stored, but details not gathered
[2024-05-16T17:16:18.593] slurmctld version 18.08.8 started on cluster linux
[2024-05-16T17:16:18.598] No memory enforcing mechanism configured.
[2024-05-16T17:16:18.599] layouts: no layout to initialize
[2024-05-16T17:16:18.604] layouts: loading entities/relations information
[2024-05-16T17:16:18.604] Recovered state of 2 nodes
[2024-05-16T17:16:18.604] Down nodes: compute[00-01]
[2024-05-16T17:16:18.604] Recovered information about 0 jobs
[2024-05-16T17:16:18.604] cons_res: select_p_node_init
[2024-05-16T17:16:18.604] cons_res: preparing for 2 partitions
[2024-05-16T17:16:18.604] Recovered state of 0 reservations
[2024-05-16T17:16:18.604] _preserve_plugins: backup_controller not specified
[2024-05-16T17:16:18.605] cons_res: select_p_reconfigure
[2024-05-16T17:16:18.605] cons_res: select_p_node_init
[2024-05-16T17:16:18.605] cons_res: preparing for 2 partitions
[2024-05-16T17:16:18.605] Running as primary controller
[2024-05-16T17:16:18.605] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-16T17:16:18.605] No parameter for mcs plugin, default values set
[2024-05-16T17:16:18.605] mcs: MCSParameters = (null). ondemand set.
[2024-05-16T17:16:34.762] Terminate signal (SIGINT or SIGTERM) received
[2024-05-16T17:16:34.845] Saving all slurm state
[2024-05-16T17:16:34.949] layouts: all layouts are now unloaded.
[2024-05-16T17:16:42.149] error: chdir(/var/log): Permission denied
[2024-05-16T17:16:42.149] Job accounting information stored, but details not gathered
[2024-05-16T17:16:42.149] slurmctld version 18.08.8 started on cluster linux
[2024-05-16T17:16:42.152] No memory enforcing mechanism configured.
[2024-05-16T17:16:42.152] layouts: no layout to initialize
[2024-05-16T17:16:42.211] layouts: loading entities/relations information
[2024-05-16T17:16:42.211] Recovered state of 2 nodes
[2024-05-16T17:16:42.211] Down nodes: compute[00-01]
[2024-05-16T17:16:42.211] Recovered information about 0 jobs
[2024-05-16T17:16:42.211] cons_res: select_p_node_init
[2024-05-16T17:16:42.211] cons_res: preparing for 2 partitions
[2024-05-16T17:16:42.212] Recovered state of 0 reservations
[2024-05-16T17:16:42.212] _preserve_plugins: backup_controller not specified
[2024-05-16T17:16:42.212] cons_res: select_p_reconfigure
[2024-05-16T17:16:42.212] cons_res: select_p_node_init
[2024-05-16T17:16:42.212] cons_res: preparing for 2 partitions
[2024-05-16T17:16:42.212] Running as primary controller
[2024-05-16T17:16:42.212] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-16T17:16:42.212] No parameter for mcs plugin, default values set
[2024-05-16T17:16:42.212] mcs: MCSParameters = (null). ondemand set.
[2024-05-16T17:17:42.735] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-16T17:20:33.693] killing old slurmctld[2137]
[2024-05-16T17:20:33.694] Terminate signal (SIGINT or SIGTERM) received
[2024-05-16T17:20:33.775] Saving all slurm state
[2024-05-16T17:20:33.821] layouts: all layouts are now unloaded.
[2024-05-16T17:20:33.823] error: chdir(/var/log): Permission denied
[2024-05-16T17:20:33.823] Job accounting information stored, but details not gathered
[2024-05-16T17:20:33.823] slurmctld version 18.08.8 started on cluster linux
[2024-05-16T17:20:33.826] No memory enforcing mechanism configured.
[2024-05-16T17:20:33.827] layouts: no layout to initialize
[2024-05-16T17:20:33.832] layouts: loading entities/relations information
[2024-05-16T17:20:33.832] Recovered state of 2 nodes
[2024-05-16T17:20:33.832] Down nodes: compute[00-01]
[2024-05-16T17:20:33.832] Recovered information about 0 jobs
[2024-05-16T17:20:33.832] cons_res: select_p_node_init
[2024-05-16T17:20:33.832] cons_res: preparing for 2 partitions
[2024-05-16T17:20:33.832] Recovered state of 0 reservations
[2024-05-16T17:20:33.832] _preserve_plugins: backup_controller not specified
[2024-05-16T17:20:33.832] cons_res: select_p_reconfigure
[2024-05-16T17:20:33.832] cons_res: select_p_node_init
[2024-05-16T17:20:33.832] cons_res: preparing for 2 partitions
[2024-05-16T17:20:33.832] Running as primary controller
[2024-05-16T17:20:33.832] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-16T17:20:33.832] No parameter for mcs plugin, default values set
[2024-05-16T17:20:33.832] mcs: MCSParameters = (null). ondemand set.
[2024-05-16T17:20:35.929] Terminate signal (SIGINT or SIGTERM) received
[2024-05-16T17:20:35.942] Saving all slurm state
[2024-05-16T17:20:36.011] layouts: all layouts are now unloaded.
[2024-05-16T17:20:40.468] error: chdir(/var/log): Permission denied
[2024-05-16T17:20:40.469] Job accounting information stored, but details not gathered
[2024-05-16T17:20:40.469] slurmctld version 18.08.8 started on cluster linux
[2024-05-16T17:20:40.494] No memory enforcing mechanism configured.
[2024-05-16T17:20:40.494] layouts: no layout to initialize
[2024-05-16T17:20:40.512] layouts: loading entities/relations information
[2024-05-16T17:20:40.512] Recovered state of 2 nodes
[2024-05-16T17:20:40.512] Down nodes: compute[00-01]
[2024-05-16T17:20:40.512] Recovered information about 0 jobs
[2024-05-16T17:20:40.512] cons_res: select_p_node_init
[2024-05-16T17:20:40.512] cons_res: preparing for 2 partitions
[2024-05-16T17:20:40.515] Recovered state of 0 reservations
[2024-05-16T17:20:40.515] _preserve_plugins: backup_controller not specified
[2024-05-16T17:20:40.515] cons_res: select_p_reconfigure
[2024-05-16T17:20:40.515] cons_res: select_p_node_init
[2024-05-16T17:20:40.515] cons_res: preparing for 2 partitions
[2024-05-16T17:20:40.515] Running as primary controller
[2024-05-16T17:20:40.517] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-16T17:20:40.517] No parameter for mcs plugin, default values set
[2024-05-16T17:20:40.517] mcs: MCSParameters = (null). ondemand set.
[2024-05-16T17:21:40.342] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-16T17:22:11.595] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T17:22:11.732] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T17:22:11.793] Node compute01 now responding
[2024-05-16T17:22:11.793] node compute01 returned to service
[2024-05-16T17:22:11.945] Node compute00 now responding
[2024-05-16T17:22:11.945] node compute00 returned to service
[2024-05-16T17:22:47.234] Invalid node state transition requested for node compute00 from=IDLE to=RESUME
[2024-05-16T17:22:47.234] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-05-16T17:25:40.398] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T17:25:40.401] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T17:25:55.806] Invalid node state transition requested for node compute00 from=IDLE to=RESUME
[2024-05-16T17:25:55.806] _slurm_rpc_update_node for compute00: Invalid node state specified
[2024-05-16T17:30:40.405] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T17:30:40.407] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T17:35:40.652] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T17:35:40.665] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T17:38:03.394] error: chdir(/var/log): Permission denied
[2024-05-16T17:38:03.421] Job accounting information stored, but details not gathered
[2024-05-16T17:38:03.423] slurmctld version 18.08.8 started on cluster linux
[2024-05-16T17:38:03.842] No memory enforcing mechanism configured.
[2024-05-16T17:38:03.842] layouts: no layout to initialize
[2024-05-16T17:38:03.946] layouts: loading entities/relations information
[2024-05-16T17:38:03.946] Recovered state of 2 nodes
[2024-05-16T17:38:03.947] Recovered information about 0 jobs
[2024-05-16T17:38:03.947] cons_res: select_p_node_init
[2024-05-16T17:38:03.947] cons_res: preparing for 2 partitions
[2024-05-16T17:38:03.949] Recovered state of 0 reservations
[2024-05-16T17:38:03.950] _preserve_plugins: backup_controller not specified
[2024-05-16T17:38:03.950] cons_res: select_p_reconfigure
[2024-05-16T17:38:03.950] cons_res: select_p_node_init
[2024-05-16T17:38:03.950] cons_res: preparing for 2 partitions
[2024-05-16T17:38:03.950] Running as primary controller
[2024-05-16T17:38:04.149] error: You are not running a supported accounting_storage plugin
(accounting_storage/filetxt).
Fairshare can only be calculated with either 'accounting_storage/slurmdbd' or 'accounting_storage/mysql' enabled.  If you want multifactor priority without fairshare ignore this message.
[2024-05-16T17:38:04.149] No parameter for mcs plugin, default values set
[2024-05-16T17:38:04.149] mcs: MCSParameters = (null). ondemand set.
[2024-05-16T17:39:04.355] SchedulerParameters=default_queue_depth=100,max_rpc_cnt=0,max_sched_time=2,partition_job_depth=0,sched_max_job_start=0,sched_min_interval=2
[2024-05-16T17:43:04.874] error: Nodes compute[00-01] not responding
[2024-05-16T17:44:48.333] error: Nodes compute[00-01] not responding, setting DOWN
[2024-05-16T17:47:18.792] sched: _slurm_rpc_allocate_resources JobId=141 NodeList=(null) usec=694
[2024-05-16T17:47:43.269] _job_complete: JobId=141 WTERMSIG 126
[2024-05-16T17:47:43.269] _job_complete: JobId=141 cancelled by interactive user
[2024-05-16T17:47:43.295] _job_complete: JobId=141 done
[2024-05-16T17:47:43.296] _slurm_rpc_complete_job_allocation: JobId=141 error Job/step already completing or completed
[2024-05-16T17:49:43.415] sched: _slurm_rpc_allocate_resources JobId=142 NodeList=(null) usec=223
[2024-05-16T17:49:52.939] _job_complete: JobId=142 WTERMSIG 126
[2024-05-16T17:49:52.940] _job_complete: JobId=142 cancelled by interactive user
[2024-05-16T17:49:52.961] _job_complete: JobId=142 done
[2024-05-16T17:49:52.962] _slurm_rpc_complete_job_allocation: JobId=142 error Job/step already completing or completed
[2024-05-16T17:50:52.535] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T17:50:52.688] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T17:50:52.724] Node compute01 now responding
[2024-05-16T17:50:52.724] validate_node_specs: Node compute01 unexpectedly rebooted boot_time=1715881684 last response=1715881083
[2024-05-16T17:50:52.871] Node compute00 now responding
[2024-05-16T17:50:52.871] validate_node_specs: Node compute00 unexpectedly rebooted boot_time=1715881686 last response=1715881083
[2024-05-16T17:51:22.105] update_node: node compute00 state set to IDLE
[2024-05-16T17:51:23.039] Node compute00 now responding
[2024-05-16T17:51:28.199] update_node: node compute01 state set to IDLE
[2024-05-16T17:51:29.173] Node compute01 now responding
[2024-05-16T17:52:08.181] sched: _slurm_rpc_allocate_resources JobId=143 NodeList=compute[00-01] usec=8081
[2024-05-16T17:53:04.891] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T17:53:04.898] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T17:54:36.623] _job_complete: JobId=143 WEXITSTATUS 0
[2024-05-16T17:54:36.631] _job_complete: JobId=143 done
[2024-05-16T17:58:04.804] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T17:58:04.806] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:03:05.184] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:03:05.186] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:05:13.838] _slurm_rpc_submit_batch_job: JobId=144 InitPrio=60 usec=690
[2024-05-16T18:05:13.995] sched: Allocate JobId=144 NodeList=compute00 #CPUs=2 Partition=fair
[2024-05-16T18:05:14.117] _job_complete: JobId=144 WEXITSTATUS 0
[2024-05-16T18:05:14.124] _job_complete: JobId=144 done
[2024-05-16T18:08:04.457] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:08:04.461] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:13:05.245] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:13:05.256] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:18:05.586] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:18:05.588] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:23:05.901] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:23:05.903] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:24:33.985] _slurm_rpc_submit_batch_job: JobId=145 InitPrio=60 usec=694
[2024-05-16T18:24:34.053] sched: Allocate JobId=145 NodeList=compute00 #CPUs=2 Partition=fair
[2024-05-16T18:24:34.103] _job_complete: JobId=145 WEXITSTATUS 2
[2024-05-16T18:24:34.110] _job_complete: JobId=145 done
[2024-05-16T18:25:12.269] _slurm_rpc_submit_batch_job: JobId=146 InitPrio=85 usec=1342
[2024-05-16T18:25:12.778] sched: Allocate JobId=146 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:25:12.842] _job_complete: JobId=146 WEXITSTATUS 2
[2024-05-16T18:25:12.852] _job_complete: JobId=146 done
[2024-05-16T18:26:32.307] _slurm_rpc_submit_batch_job: JobId=147 InitPrio=85 usec=1225
[2024-05-16T18:26:32.769] sched: Allocate JobId=147 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:26:32.823] _job_complete: JobId=147 WEXITSTATUS 2
[2024-05-16T18:26:32.829] _job_complete: JobId=147 done
[2024-05-16T18:27:16.337] _slurm_rpc_submit_batch_job: JobId=148 InitPrio=85 usec=1267
[2024-05-16T18:27:16.494] sched: Allocate JobId=148 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:28:05.525] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:28:05.561] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:28:08.692] _slurm_rpc_submit_batch_job: JobId=149 InitPrio=85 usec=310
[2024-05-16T18:28:09.483] sched: Allocate JobId=149 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:28:09.518] _job_complete: JobId=149 WEXITSTATUS 2
[2024-05-16T18:28:09.524] _job_complete: JobId=149 done
[2024-05-16T18:28:55.167] _slurm_rpc_submit_batch_job: JobId=150 InitPrio=85 usec=241
[2024-05-16T18:28:55.195] sched: Allocate JobId=150 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:28:55.239] _job_complete: JobId=150 WEXITSTATUS 2
[2024-05-16T18:28:55.246] _job_complete: JobId=150 done
[2024-05-16T18:29:48.656] _slurm_rpc_submit_batch_job: JobId=151 InitPrio=85 usec=241
[2024-05-16T18:29:48.832] sched: Allocate JobId=151 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:30:56.242] _slurm_rpc_submit_batch_job: JobId=152 InitPrio=10075 usec=291
[2024-05-16T18:31:32.460] _slurm_rpc_submit_batch_job: JobId=153 InitPrio=10075 usec=287
[2024-05-16T18:31:45.406] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=153 uid 1002
[2024-05-16T18:31:50.278] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=152 uid 1002
[2024-05-16T18:32:13.578] _slurm_rpc_submit_batch_job: JobId=154 InitPrio=85 usec=308
[2024-05-16T18:32:14.210] sched: Allocate JobId=154 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:32:31.048] _slurm_rpc_submit_batch_job: JobId=155 InitPrio=85 usec=336
[2024-05-16T18:32:31.982] _slurm_rpc_submit_batch_job: JobId=156 InitPrio=85 usec=291
[2024-05-16T18:32:32.318] sched: Allocate JobId=155 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:32:32.321] sched: Allocate JobId=156 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:33:06.114] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:33:06.352] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:33:32.017] _slurm_rpc_submit_batch_job: JobId=157 InitPrio=85 usec=299
[2024-05-16T18:33:32.505] sched: Allocate JobId=157 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:33:32.576] _slurm_rpc_submit_batch_job: JobId=158 InitPrio=85 usec=301
[2024-05-16T18:33:33.143] _slurm_rpc_submit_batch_job: JobId=159 InitPrio=85 usec=306
[2024-05-16T18:33:33.963] _slurm_rpc_submit_batch_job: JobId=160 InitPrio=85 usec=312
[2024-05-16T18:33:34.580] _slurm_rpc_submit_batch_job: JobId=161 InitPrio=85 usec=3032
[2024-05-16T18:33:34.650] backfill: Started JobId=158 in fair on compute[00-01]
[2024-05-16T18:33:34.657] backfill: Started JobId=159 in fair on compute[00-01]
[2024-05-16T18:34:10.930] _slurm_rpc_submit_batch_job: JobId=162 InitPrio=85 usec=325
[2024-05-16T18:34:11.338] _slurm_rpc_submit_batch_job: JobId=163 InitPrio=85 usec=340
[2024-05-16T18:34:11.749] _slurm_rpc_submit_batch_job: JobId=164 InitPrio=85 usec=336
[2024-05-16T18:35:36.709] _slurm_rpc_submit_batch_job: JobId=165 InitPrio=85 usec=335
[2024-05-16T18:36:03.330] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=165 uid 1002
[2024-05-16T18:36:31.688] _slurm_rpc_submit_batch_job: JobId=166 InitPrio=10075 usec=325
[2024-05-16T18:37:41.849] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=148 uid 1002
[2024-05-16T18:37:41.850] error: Security violation, REQUEST_KILL_JOB RPC for JobId=148 from uid 1002
[2024-05-16T18:37:41.850] _slurm_rpc_kill_job: job_str_signal() JobId=148 sig 9 returned Access/permission denied
[2024-05-16T18:37:45.285] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=149 uid 1002
[2024-05-16T18:37:45.286] job_str_signal(3): invalid JobId=149
[2024-05-16T18:37:45.286] _slurm_rpc_kill_job: job_str_signal() JobId=149 sig 9 returned Invalid job id specified
[2024-05-16T18:37:58.509] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=155 uid 1002
[2024-05-16T18:38:01.022] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=148 uid 1003
[2024-05-16T18:38:01.187] cleanup_completing: JobId=148 completion process took 86 seconds
[2024-05-16T18:38:01.320] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=156 uid 1002
[2024-05-16T18:38:05.992] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:38:06.194] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:38:11.262] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=159 uid 1003
[2024-05-16T18:38:11.484] cleanup_completing: JobId=159 completion process took 66 seconds
[2024-05-16T18:38:15.665] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=158 uid 1003
[2024-05-16T18:38:21.597] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=151 uid 1002
[2024-05-16T18:38:24.373] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=157 uid 1003
[2024-05-16T18:38:39.217] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=154 uid 1003
[2024-05-16T18:38:41.121] sched: Allocate JobId=166 NodeList=compute[00-01] #CPUs=2 Partition=urgent
[2024-05-16T18:38:41.122] sched: Allocate JobId=160 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:38:41.126] sched: Allocate JobId=161 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:38:41.129] sched: Allocate JobId=162 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:38:41.133] sched: Allocate JobId=163 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:38:42.698] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=154 uid 1003
[2024-05-16T18:40:28.605] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=164 uid 1002
[2024-05-16T18:40:28.605] error: Security violation, REQUEST_KILL_JOB RPC for JobId=164 from uid 1002
[2024-05-16T18:40:28.605] _slurm_rpc_kill_job: job_str_signal() JobId=164 sig 9 returned Access/permission denied
[2024-05-16T18:40:28.606] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=160 uid 1002
[2024-05-16T18:40:28.607] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=163 uid 1002
[2024-05-16T18:40:28.607] error: Security violation, REQUEST_KILL_JOB RPC for JobId=160 from uid 1002
[2024-05-16T18:40:28.607] _slurm_rpc_kill_job: job_str_signal() JobId=160 sig 9 returned Access/permission denied
[2024-05-16T18:40:28.607] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=161 uid 1002
[2024-05-16T18:40:28.607] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=162 uid 1002
[2024-05-16T18:40:28.607] error: Security violation, REQUEST_KILL_JOB RPC for JobId=161 from uid 1002
[2024-05-16T18:40:28.607] _slurm_rpc_kill_job: job_str_signal() JobId=161 sig 9 returned Access/permission denied
[2024-05-16T18:40:28.607] error: Security violation, REQUEST_KILL_JOB RPC for JobId=163 from uid 1002
[2024-05-16T18:40:28.607] _slurm_rpc_kill_job: job_str_signal() JobId=163 sig 9 returned Access/permission denied
[2024-05-16T18:40:28.608] error: Security violation, REQUEST_KILL_JOB RPC for JobId=162 from uid 1002
[2024-05-16T18:40:28.608] _slurm_rpc_kill_job: job_str_signal() JobId=162 sig 9 returned Access/permission denied
[2024-05-16T18:40:35.817] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=166 uid 1002
[2024-05-16T18:40:35.991] sched: Allocate JobId=164 NodeList=compute[00-01] #CPUs=2 Partition=fair
[2024-05-16T18:40:47.580] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=161 uid 1003
[2024-05-16T18:40:47.580] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=163 uid 1003
[2024-05-16T18:40:47.580] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=164 uid 1003
[2024-05-16T18:40:47.580] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=160 uid 1003
[2024-05-16T18:40:47.830] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=162 uid 1003
[2024-05-16T18:40:48.003] cleanup_completing: JobId=160 completion process took 73 seconds
[2024-05-16T18:41:24.268] _slurm_rpc_submit_batch_job: JobId=167 InitPrio=110 usec=1193
[2024-05-16T18:41:24.600] sched: Allocate JobId=167 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:41:25.059] _slurm_rpc_submit_batch_job: JobId=168 InitPrio=110 usec=328
[2024-05-16T18:41:26.393] _slurm_rpc_submit_batch_job: JobId=169 InitPrio=110 usec=398
[2024-05-16T18:41:27.109] _slurm_rpc_submit_batch_job: JobId=170 InitPrio=110 usec=342
[2024-05-16T18:41:27.647] sched: Allocate JobId=168 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:41:27.652] sched: Allocate JobId=169 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:41:27.664] sched: Allocate JobId=170 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:41:27.766] _slurm_rpc_submit_batch_job: JobId=171 InitPrio=110 usec=297
[2024-05-16T18:41:28.442] _slurm_rpc_submit_batch_job: JobId=172 InitPrio=110 usec=311
[2024-05-16T18:41:29.264] _slurm_rpc_submit_batch_job: JobId=173 InitPrio=110 usec=359
[2024-05-16T18:41:29.881] _slurm_rpc_submit_batch_job: JobId=174 InitPrio=110 usec=324
[2024-05-16T18:41:30.485] _slurm_rpc_submit_batch_job: JobId=175 InitPrio=110 usec=316
[2024-05-16T18:41:30.998] _slurm_rpc_submit_batch_job: JobId=176 InitPrio=110 usec=312
[2024-05-16T18:41:45.846] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=171 uid 1003
[2024-05-16T18:41:46.968] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=172 uid 1003
[2024-05-16T18:42:11.137] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=176 uid 1003
[2024-05-16T18:42:12.700] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=175 uid 1003
[2024-05-16T18:42:13.696] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=174 uid 1003
[2024-05-16T18:42:15.130] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=173 uid 1003
[2024-05-16T18:43:05.248] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:43:05.486] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:43:07.162] _slurm_rpc_submit_batch_job: JobId=177 InitPrio=110 usec=309
[2024-05-16T18:43:12.290] _slurm_rpc_submit_batch_job: JobId=178 InitPrio=110 usec=304
[2024-05-16T18:43:12.978] _slurm_rpc_submit_batch_job: JobId=179 InitPrio=110 usec=359
[2024-05-16T18:43:13.518] _slurm_rpc_submit_batch_job: JobId=180 InitPrio=110 usec=299
[2024-05-16T18:43:43.240] _slurm_rpc_submit_batch_job: JobId=181 InitPrio=10100 usec=298
[2024-05-16T18:43:44.326] _slurm_rpc_submit_batch_job: JobId=182 InitPrio=10100 usec=325
[2024-05-16T18:43:45.410] _slurm_rpc_submit_batch_job: JobId=183 InitPrio=10100 usec=342
[2024-05-16T18:43:47.291] _slurm_rpc_submit_batch_job: JobId=184 InitPrio=10100 usec=413
[2024-05-16T18:44:10.847] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=168 uid 1003
[2024-05-16T18:44:10.847] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=167 uid 1003
[2024-05-16T18:44:10.847] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=170 uid 1003
[2024-05-16T18:44:10.864] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=169 uid 1003
[2024-05-16T18:44:11.007] cleanup_completing: JobId=169 completion process took 66 seconds
[2024-05-16T18:44:11.010] sched: Allocate JobId=181 NodeList=compute[00-01] #CPUs=4 Partition=urgent
[2024-05-16T18:45:34.364] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=182 uid 1002
[2024-05-16T18:45:34.364] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=183 uid 1002
[2024-05-16T18:45:34.433] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=184 uid 1002
[2024-05-16T18:45:34.450] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=181 uid 1002
[2024-05-16T18:45:34.619] sched: Allocate JobId=177 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:45:34.623] sched: Allocate JobId=178 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:45:34.626] sched: Allocate JobId=179 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:45:34.630] sched: Allocate JobId=180 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:48:05.265] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:48:05.444] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:49:02.182] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=177 uid 1002
[2024-05-16T18:49:02.182] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=180 uid 1002
[2024-05-16T18:49:02.196] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=178 uid 1002
[2024-05-16T18:49:02.196] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=179 uid 1002
[2024-05-16T18:49:02.363] cleanup_completing: JobId=180 completion process took 87 seconds
[2024-05-16T18:49:23.709] _slurm_rpc_submit_batch_job: JobId=185 InitPrio=10100 usec=1356
[2024-05-16T18:49:23.942] _slurm_rpc_submit_batch_job: JobId=186 InitPrio=10100 usec=1630
[2024-05-16T18:49:24.114] _slurm_rpc_submit_batch_job: JobId=187 InitPrio=10100 usec=1523
[2024-05-16T18:49:24.222] sched: Allocate JobId=185 NodeList=compute[00-01] #CPUs=4 Partition=urgent
[2024-05-16T18:49:24.265] _slurm_rpc_submit_batch_job: JobId=188 InitPrio=10100 usec=666
[2024-05-16T18:49:24.431] _slurm_rpc_submit_batch_job: JobId=189 InitPrio=10100 usec=297
[2024-05-16T18:49:24.591] _slurm_rpc_submit_batch_job: JobId=190 InitPrio=10100 usec=318
[2024-05-16T18:49:24.748] _slurm_rpc_submit_batch_job: JobId=191 InitPrio=10100 usec=312
[2024-05-16T18:49:24.907] _slurm_rpc_submit_batch_job: JobId=192 InitPrio=10100 usec=327
[2024-05-16T18:49:25.965] _slurm_rpc_submit_batch_job: JobId=193 InitPrio=10100 usec=362
[2024-05-16T18:49:26.162] _slurm_rpc_submit_batch_job: JobId=194 InitPrio=10100 usec=325
[2024-05-16T18:49:26.968] _slurm_rpc_submit_batch_job: JobId=195 InitPrio=10100 usec=317
[2024-05-16T18:49:27.334] _slurm_rpc_submit_batch_job: JobId=196 InitPrio=10100 usec=317
[2024-05-16T18:49:27.666] _slurm_rpc_submit_batch_job: JobId=197 InitPrio=10100 usec=301
[2024-05-16T18:49:27.976] _slurm_rpc_submit_batch_job: JobId=198 InitPrio=10100 usec=329
[2024-05-16T18:49:28.269] _slurm_rpc_submit_batch_job: JobId=199 InitPrio=10100 usec=348
[2024-05-16T18:49:28.304] _slurm_rpc_submit_batch_job: JobId=200 InitPrio=110 usec=377
[2024-05-16T18:49:28.466] _slurm_rpc_submit_batch_job: JobId=201 InitPrio=10100 usec=371
[2024-05-16T18:49:28.683] _slurm_rpc_submit_batch_job: JobId=202 InitPrio=10100 usec=312
[2024-05-16T18:49:28.714] _slurm_rpc_submit_batch_job: JobId=203 InitPrio=110 usec=318
[2024-05-16T18:49:28.886] _slurm_rpc_submit_batch_job: JobId=204 InitPrio=10100 usec=317
[2024-05-16T18:49:28.913] _slurm_rpc_submit_batch_job: JobId=205 InitPrio=110 usec=358
[2024-05-16T18:49:29.075] _slurm_rpc_submit_batch_job: JobId=206 InitPrio=10100 usec=336
[2024-05-16T18:49:29.186] _slurm_rpc_submit_batch_job: JobId=207 InitPrio=110 usec=328
[2024-05-16T18:49:29.310] _slurm_rpc_submit_batch_job: JobId=208 InitPrio=10100 usec=332
[2024-05-16T18:49:29.432] _slurm_rpc_submit_batch_job: JobId=209 InitPrio=110 usec=408
[2024-05-16T18:49:29.527] _slurm_rpc_submit_batch_job: JobId=210 InitPrio=10100 usec=339
[2024-05-16T18:49:29.633] _slurm_rpc_submit_batch_job: JobId=211 InitPrio=110 usec=312
[2024-05-16T18:49:29.801] _slurm_rpc_submit_batch_job: JobId=212 InitPrio=110 usec=349
[2024-05-16T18:49:30.048] _slurm_rpc_submit_batch_job: JobId=213 InitPrio=110 usec=386
[2024-05-16T18:49:30.250] _slurm_rpc_submit_batch_job: JobId=214 InitPrio=110 usec=442
[2024-05-16T18:49:30.556] _slurm_rpc_submit_batch_job: JobId=215 InitPrio=110 usec=319
[2024-05-16T18:49:30.762] _slurm_rpc_submit_batch_job: JobId=216 InitPrio=110 usec=308
[2024-05-16T18:49:30.966] _slurm_rpc_submit_batch_job: JobId=217 InitPrio=110 usec=342
[2024-05-16T18:49:31.171] _slurm_rpc_submit_batch_job: JobId=218 InitPrio=110 usec=316
[2024-05-16T18:49:31.470] _slurm_rpc_submit_batch_job: JobId=219 InitPrio=10100 usec=328
[2024-05-16T18:49:31.478] _slurm_rpc_submit_batch_job: JobId=220 InitPrio=110 usec=352
[2024-05-16T18:49:31.693] _slurm_rpc_submit_batch_job: JobId=221 InitPrio=110 usec=404
[2024-05-16T18:49:31.887] _slurm_rpc_submit_batch_job: JobId=222 InitPrio=110 usec=378
[2024-05-16T18:49:31.947] _slurm_rpc_submit_batch_job: JobId=223 InitPrio=10100 usec=345
[2024-05-16T18:49:32.194] _slurm_rpc_submit_batch_job: JobId=224 InitPrio=110 usec=315
[2024-05-16T18:49:32.352] _slurm_rpc_submit_batch_job: JobId=225 InitPrio=10100 usec=324
[2024-05-16T18:49:32.399] _slurm_rpc_submit_batch_job: JobId=226 InitPrio=110 usec=364
[2024-05-16T18:49:32.604] _slurm_rpc_submit_batch_job: JobId=227 InitPrio=110 usec=342
[2024-05-16T18:49:32.645] _slurm_rpc_submit_batch_job: JobId=228 InitPrio=10100 usec=365
[2024-05-16T18:49:32.911] _slurm_rpc_submit_batch_job: JobId=229 InitPrio=110 usec=337
[2024-05-16T18:49:32.956] _slurm_rpc_submit_batch_job: JobId=230 InitPrio=10100 usec=381
[2024-05-16T18:49:33.115] _slurm_rpc_submit_batch_job: JobId=231 InitPrio=110 usec=375
[2024-05-16T18:49:33.193] _slurm_rpc_submit_batch_job: JobId=232 InitPrio=10100 usec=379
[2024-05-16T18:49:33.326] _slurm_rpc_submit_batch_job: JobId=233 InitPrio=110 usec=331
[2024-05-16T18:49:33.426] _slurm_rpc_submit_batch_job: JobId=234 InitPrio=10100 usec=364
[2024-05-16T18:49:33.627] _slurm_rpc_submit_batch_job: JobId=235 InitPrio=110 usec=324
[2024-05-16T18:49:33.643] _slurm_rpc_submit_batch_job: JobId=236 InitPrio=10100 usec=383
[2024-05-16T18:49:33.832] _slurm_rpc_submit_batch_job: JobId=237 InitPrio=110 usec=348
[2024-05-16T18:49:33.842] _slurm_rpc_submit_batch_job: JobId=238 InitPrio=10100 usec=346
[2024-05-16T18:49:34.045] _slurm_rpc_submit_batch_job: JobId=239 InitPrio=110 usec=346
[2024-05-16T18:49:34.056] _slurm_rpc_submit_batch_job: JobId=240 InitPrio=10100 usec=371
[2024-05-16T18:49:34.270] _slurm_rpc_submit_batch_job: JobId=241 InitPrio=10100 usec=351
[2024-05-16T18:49:34.280] _slurm_rpc_submit_batch_job: JobId=242 InitPrio=110 usec=442
[2024-05-16T18:49:34.469] _slurm_rpc_submit_batch_job: JobId=243 InitPrio=10100 usec=386
[2024-05-16T18:49:34.553] _slurm_rpc_submit_batch_job: JobId=244 InitPrio=110 usec=342
[2024-05-16T18:49:34.664] _slurm_rpc_submit_batch_job: JobId=245 InitPrio=10100 usec=359
[2024-05-16T18:49:34.841] _slurm_rpc_submit_batch_job: JobId=246 InitPrio=10100 usec=406
[2024-05-16T18:49:34.864] _slurm_rpc_submit_batch_job: JobId=247 InitPrio=110 usec=346
[2024-05-16T18:49:35.018] _slurm_rpc_submit_batch_job: JobId=248 InitPrio=10100 usec=416
[2024-05-16T18:49:35.162] _slurm_rpc_submit_batch_job: JobId=249 InitPrio=110 usec=360
[2024-05-16T18:49:35.194] _slurm_rpc_submit_batch_job: JobId=250 InitPrio=10100 usec=388
[2024-05-16T18:49:35.369] _slurm_rpc_submit_batch_job: JobId=251 InitPrio=110 usec=318
[2024-05-16T18:49:35.392] _slurm_rpc_submit_batch_job: JobId=252 InitPrio=10100 usec=368
[2024-05-16T18:49:35.572] _slurm_rpc_submit_batch_job: JobId=253 InitPrio=110 usec=363
[2024-05-16T18:49:35.596] _slurm_rpc_submit_batch_job: JobId=254 InitPrio=10100 usec=389
[2024-05-16T18:49:35.749] _slurm_rpc_submit_batch_job: JobId=255 InitPrio=10100 usec=386
[2024-05-16T18:49:35.880] _slurm_rpc_submit_batch_job: JobId=256 InitPrio=110 usec=820
[2024-05-16T18:49:35.945] _slurm_rpc_submit_batch_job: JobId=257 InitPrio=10100 usec=392
[2024-05-16T18:49:36.089] _slurm_rpc_submit_batch_job: JobId=258 InitPrio=110 usec=360
[2024-05-16T18:49:36.121] _slurm_rpc_submit_batch_job: JobId=259 InitPrio=10100 usec=385
[2024-05-16T18:49:36.299] _slurm_rpc_submit_batch_job: JobId=260 InitPrio=10100 usec=384
[2024-05-16T18:49:36.343] _slurm_rpc_submit_batch_job: JobId=261 InitPrio=110 usec=429
[2024-05-16T18:49:36.475] _slurm_rpc_submit_batch_job: JobId=262 InitPrio=10100 usec=394
[2024-05-16T18:49:36.534] _slurm_rpc_submit_batch_job: JobId=263 InitPrio=110 usec=339
[2024-05-16T18:49:36.670] _slurm_rpc_submit_batch_job: JobId=264 InitPrio=10100 usec=442
[2024-05-16T18:49:36.805] _slurm_rpc_submit_batch_job: JobId=265 InitPrio=110 usec=346
[2024-05-16T18:49:36.847] _slurm_rpc_submit_batch_job: JobId=266 InitPrio=10100 usec=379
[2024-05-16T18:49:37.107] _slurm_rpc_submit_batch_job: JobId=267 InitPrio=110 usec=387
[2024-05-16T18:49:37.288] _slurm_rpc_submit_batch_job: JobId=268 InitPrio=110 usec=392
[2024-05-16T18:49:37.620] _slurm_rpc_submit_batch_job: JobId=269 InitPrio=110 usec=357
[2024-05-16T18:49:37.797] _slurm_rpc_submit_batch_job: JobId=270 InitPrio=110 usec=363
[2024-05-16T18:49:38.120] _slurm_rpc_submit_batch_job: JobId=271 InitPrio=110 usec=358
[2024-05-16T18:49:38.438] _slurm_rpc_submit_batch_job: JobId=272 InitPrio=110 usec=342
[2024-05-16T18:49:38.745] _slurm_rpc_submit_batch_job: JobId=273 InitPrio=110 usec=387
[2024-05-16T18:49:38.949] _slurm_rpc_submit_batch_job: JobId=274 InitPrio=110 usec=344
[2024-05-16T18:49:39.193] _slurm_rpc_submit_batch_job: JobId=275 InitPrio=110 usec=350
[2024-05-16T18:49:39.385] _slurm_rpc_submit_batch_job: JobId=276 InitPrio=10100 usec=414
[2024-05-16T18:49:39.567] _slurm_rpc_submit_batch_job: JobId=277 InitPrio=110 usec=339
[2024-05-16T18:49:39.875] _slurm_rpc_submit_batch_job: JobId=278 InitPrio=110 usec=345
[2024-05-16T18:49:39.904] _slurm_rpc_submit_batch_job: JobId=279 InitPrio=10100 usec=434
[2024-05-16T18:49:40.182] _slurm_rpc_submit_batch_job: JobId=280 InitPrio=110 usec=327
[2024-05-16T18:49:40.387] _slurm_rpc_submit_batch_job: JobId=281 InitPrio=110 usec=379
[2024-05-16T18:49:40.421] _slurm_rpc_submit_batch_job: JobId=282 InitPrio=10100 usec=465
[2024-05-16T18:49:40.692] _slurm_rpc_submit_batch_job: JobId=283 InitPrio=110 usec=461
[2024-05-16T18:49:40.863] _slurm_rpc_submit_batch_job: JobId=284 InitPrio=10100 usec=417
[2024-05-16T18:49:40.998] _slurm_rpc_submit_batch_job: JobId=285 InitPrio=110 usec=367
[2024-05-16T18:49:41.192] _slurm_rpc_submit_batch_job: JobId=286 InitPrio=10100 usec=432
[2024-05-16T18:49:41.305] _slurm_rpc_submit_batch_job: JobId=287 InitPrio=110 usec=323
[2024-05-16T18:49:41.484] _slurm_rpc_submit_batch_job: JobId=288 InitPrio=10100 usec=382
[2024-05-16T18:49:41.538] _slurm_rpc_submit_batch_job: JobId=289 InitPrio=110 usec=451
[2024-05-16T18:49:41.681] _slurm_rpc_submit_batch_job: JobId=290 InitPrio=10100 usec=451
[2024-05-16T18:49:41.920] _slurm_rpc_submit_batch_job: JobId=291 InitPrio=110 usec=393
[2024-05-16T18:49:41.989] _slurm_rpc_submit_batch_job: JobId=292 InitPrio=10100 usec=462
[2024-05-16T18:49:42.206] _slurm_rpc_submit_batch_job: JobId=293 InitPrio=10100 usec=448
[2024-05-16T18:49:42.422] _slurm_rpc_submit_batch_job: JobId=294 InitPrio=10100 usec=407
[2024-05-16T18:49:42.534] _slurm_rpc_submit_batch_job: JobId=295 InitPrio=110 usec=342
[2024-05-16T18:49:42.596] _slurm_rpc_submit_batch_job: JobId=296 InitPrio=10100 usec=408
[2024-05-16T18:49:42.774] _slurm_rpc_submit_batch_job: JobId=297 InitPrio=10100 usec=462
[2024-05-16T18:49:42.843] _slurm_rpc_submit_batch_job: JobId=298 InitPrio=110 usec=377
[2024-05-16T18:49:42.972] _slurm_rpc_submit_batch_job: JobId=299 InitPrio=10100 usec=391
[2024-05-16T18:49:43.147] _slurm_rpc_submit_batch_job: JobId=300 InitPrio=110 usec=371
[2024-05-16T18:49:43.170] _slurm_rpc_submit_batch_job: JobId=301 InitPrio=10100 usec=424
[2024-05-16T18:49:43.328] _slurm_rpc_submit_batch_job: JobId=302 InitPrio=10100 usec=427
[2024-05-16T18:49:43.457] _slurm_rpc_submit_batch_job: JobId=303 InitPrio=110 usec=324
[2024-05-16T18:49:43.544] _slurm_rpc_submit_batch_job: JobId=304 InitPrio=10100 usec=422
[2024-05-16T18:49:43.725] _slurm_rpc_submit_batch_job: JobId=305 InitPrio=10100 usec=451
[2024-05-16T18:49:43.762] _slurm_rpc_submit_batch_job: JobId=306 InitPrio=110 usec=371
[2024-05-16T18:49:43.973] _slurm_rpc_submit_batch_job: JobId=307 InitPrio=10100 usec=399
[2024-05-16T18:49:44.068] _slurm_rpc_submit_batch_job: JobId=308 InitPrio=110 usec=325
[2024-05-16T18:49:44.375] _slurm_rpc_submit_batch_job: JobId=309 InitPrio=110 usec=331
[2024-05-16T18:49:44.628] _slurm_rpc_submit_batch_job: JobId=310 InitPrio=110 usec=381
[2024-05-16T18:49:44.989] _slurm_rpc_submit_batch_job: JobId=311 InitPrio=110 usec=352
[2024-05-16T18:49:45.195] _slurm_rpc_submit_batch_job: JobId=312 InitPrio=110 usec=348
[2024-05-16T18:49:46.140] _slurm_rpc_submit_batch_job: JobId=313 InitPrio=10100 usec=430
[2024-05-16T18:49:46.219] _slurm_rpc_submit_batch_job: JobId=314 InitPrio=110 usec=351
[2024-05-16T18:49:46.383] _slurm_rpc_submit_batch_job: JobId=315 InitPrio=110 usec=384
[2024-05-16T18:49:46.629] _slurm_rpc_submit_batch_job: JobId=316 InitPrio=110 usec=327
[2024-05-16T18:49:46.835] _slurm_rpc_submit_batch_job: JobId=317 InitPrio=110 usec=363
[2024-05-16T18:49:46.866] _slurm_rpc_submit_batch_job: JobId=318 InitPrio=10100 usec=448
[2024-05-16T18:49:46.978] _slurm_rpc_submit_batch_job: JobId=319 InitPrio=110 usec=327
[2024-05-16T18:49:47.101] _slurm_rpc_submit_batch_job: JobId=320 InitPrio=10100 usec=463
[2024-05-16T18:49:47.146] _slurm_rpc_submit_batch_job: JobId=321 InitPrio=110 usec=333
[2024-05-16T18:49:47.297] _slurm_rpc_submit_batch_job: JobId=322 InitPrio=10100 usec=457
[2024-05-16T18:49:47.346] _slurm_rpc_submit_batch_job: JobId=323 InitPrio=110 usec=375
[2024-05-16T18:49:47.492] _slurm_rpc_submit_batch_job: JobId=324 InitPrio=10100 usec=444
[2024-05-16T18:49:47.596] _slurm_rpc_submit_batch_job: JobId=325 InitPrio=110 usec=316
[2024-05-16T18:49:47.647] _slurm_rpc_submit_batch_job: JobId=326 InitPrio=10100 usec=452
[2024-05-16T18:49:47.725] _slurm_rpc_submit_batch_job: JobId=327 InitPrio=110 usec=385
[2024-05-16T18:49:47.827] _slurm_rpc_submit_batch_job: JobId=328 InitPrio=10100 usec=455
[2024-05-16T18:49:47.863] _slurm_rpc_submit_batch_job: JobId=329 InitPrio=110 usec=341
[2024-05-16T18:49:48.003] _slurm_rpc_submit_batch_job: JobId=330 InitPrio=10100 usec=452
[2024-05-16T18:49:48.065] _slurm_rpc_submit_batch_job: JobId=331 InitPrio=110 usec=513
[2024-05-16T18:49:48.200] _slurm_rpc_submit_batch_job: JobId=332 InitPrio=10100 usec=439
[2024-05-16T18:49:48.319] _slurm_rpc_submit_batch_job: JobId=333 InitPrio=110 usec=392
[2024-05-16T18:49:48.376] _slurm_rpc_submit_batch_job: JobId=334 InitPrio=10100 usec=458
[2024-05-16T18:49:48.452] _slurm_rpc_submit_batch_job: JobId=335 InitPrio=110 usec=332
[2024-05-16T18:49:48.618] _slurm_rpc_submit_batch_job: JobId=336 InitPrio=110 usec=371
[2024-05-16T18:49:48.866] _slurm_rpc_submit_batch_job: JobId=337 InitPrio=110 usec=380
[2024-05-16T18:49:49.157] _slurm_rpc_submit_batch_job: JobId=338 InitPrio=110 usec=351
[2024-05-16T18:49:49.441] _slurm_rpc_submit_batch_job: JobId=339 InitPrio=110 usec=382
[2024-05-16T18:49:49.598] _slurm_rpc_submit_batch_job: JobId=340 InitPrio=110 usec=486
[2024-05-16T18:49:49.801] _slurm_rpc_submit_batch_job: JobId=341 InitPrio=110 usec=336
[2024-05-16T18:49:50.050] _slurm_rpc_submit_batch_job: JobId=342 InitPrio=110 usec=342
[2024-05-16T18:49:50.176] _slurm_rpc_submit_batch_job: JobId=343 InitPrio=110 usec=385
[2024-05-16T18:49:50.328] _slurm_rpc_submit_batch_job: JobId=344 InitPrio=110 usec=280
[2024-05-16T18:49:50.623] _slurm_rpc_submit_batch_job: JobId=345 InitPrio=110 usec=365
[2024-05-16T18:49:50.764] _slurm_rpc_submit_batch_job: JobId=346 InitPrio=110 usec=405
[2024-05-16T18:49:50.902] _slurm_rpc_submit_batch_job: JobId=347 InitPrio=110 usec=333
[2024-05-16T18:49:51.041] _slurm_rpc_submit_batch_job: JobId=348 InitPrio=110 usec=392
[2024-05-16T18:49:51.319] _slurm_rpc_submit_batch_job: JobId=349 InitPrio=110 usec=358
[2024-05-16T18:49:51.461] _slurm_rpc_submit_batch_job: JobId=350 InitPrio=110 usec=389
[2024-05-16T18:49:51.594] _slurm_rpc_submit_batch_job: JobId=351 InitPrio=110 usec=352
[2024-05-16T18:49:51.949] _slurm_rpc_submit_batch_job: JobId=352 InitPrio=110 usec=343
[2024-05-16T18:49:52.008] _slurm_rpc_submit_batch_job: JobId=353 InitPrio=110 usec=362
[2024-05-16T18:49:52.071] _slurm_rpc_submit_batch_job: JobId=354 InitPrio=10100 usec=489
[2024-05-16T18:49:52.147] _slurm_rpc_submit_batch_job: JobId=355 InitPrio=110 usec=347
[2024-05-16T18:49:52.445] _slurm_rpc_submit_batch_job: JobId=356 InitPrio=110 usec=330
[2024-05-16T18:49:52.585] _slurm_rpc_submit_batch_job: JobId=357 InitPrio=110 usec=359
[2024-05-16T18:49:52.720] _slurm_rpc_submit_batch_job: JobId=358 InitPrio=110 usec=401
[2024-05-16T18:49:52.816] _slurm_rpc_submit_batch_job: JobId=359 InitPrio=10100 usec=470
[2024-05-16T18:49:52.991] _slurm_rpc_submit_batch_job: JobId=360 InitPrio=110 usec=360
[2024-05-16T18:49:53.150] _slurm_rpc_submit_batch_job: JobId=361 InitPrio=110 usec=382
[2024-05-16T18:49:53.306] _slurm_rpc_submit_batch_job: JobId=362 InitPrio=110 usec=364
[2024-05-16T18:49:53.355] _slurm_rpc_submit_batch_job: JobId=363 InitPrio=10100 usec=471
[2024-05-16T18:49:53.875] _slurm_rpc_submit_batch_job: JobId=364 InitPrio=10100 usec=502
[2024-05-16T18:49:53.997] _slurm_rpc_submit_batch_job: JobId=365 InitPrio=110 usec=444
[2024-05-16T18:49:54.305] _slurm_rpc_submit_batch_job: JobId=366 InitPrio=110 usec=347
[2024-05-16T18:49:54.393] _slurm_rpc_submit_batch_job: JobId=367 InitPrio=10100 usec=510
[2024-05-16T18:49:54.413] _slurm_rpc_submit_batch_job: JobId=368 InitPrio=110 usec=324
[2024-05-16T18:49:54.571] _slurm_rpc_submit_batch_job: JobId=369 InitPrio=110 usec=418
[2024-05-16T18:49:54.816] _slurm_rpc_submit_batch_job: JobId=370 InitPrio=110 usec=444
[2024-05-16T18:49:54.894] _slurm_rpc_submit_batch_job: JobId=371 InitPrio=10100 usec=484
[2024-05-16T18:49:55.040] _slurm_rpc_submit_batch_job: JobId=372 InitPrio=110 usec=321
[2024-05-16T18:49:55.198] _slurm_rpc_submit_batch_job: JobId=373 InitPrio=110 usec=362
[2024-05-16T18:49:55.281] _slurm_rpc_submit_batch_job: JobId=374 InitPrio=10100 usec=526
[2024-05-16T18:49:55.354] _slurm_rpc_submit_batch_job: JobId=375 InitPrio=110 usec=349
[2024-05-16T18:49:55.739] _slurm_rpc_submit_batch_job: JobId=376 InitPrio=110 usec=385
[2024-05-16T18:49:55.806] _slurm_rpc_submit_batch_job: JobId=377 InitPrio=110 usec=374
[2024-05-16T18:49:55.969] _slurm_rpc_submit_batch_job: JobId=378 InitPrio=10100 usec=491
[2024-05-16T18:49:56.045] _slurm_rpc_submit_batch_job: JobId=379 InitPrio=110 usec=344
[2024-05-16T18:49:56.143] _slurm_rpc_submit_batch_job: JobId=380 InitPrio=110 usec=357
[2024-05-16T18:49:56.281] _slurm_rpc_submit_batch_job: JobId=381 InitPrio=10100 usec=496
[2024-05-16T18:49:56.298] _slurm_rpc_submit_batch_job: JobId=382 InitPrio=110 usec=336
[2024-05-16T18:49:56.454] _slurm_rpc_submit_batch_job: JobId=383 InitPrio=110 usec=362
[2024-05-16T18:49:56.516] _slurm_rpc_submit_batch_job: JobId=384 InitPrio=10100 usec=512
[2024-05-16T18:49:56.769] _slurm_rpc_submit_batch_job: JobId=385 InitPrio=10100 usec=470
[2024-05-16T18:49:56.867] _slurm_rpc_submit_batch_job: JobId=386 InitPrio=110 usec=390
[2024-05-16T18:49:56.949] _slurm_rpc_submit_batch_job: JobId=387 InitPrio=110 usec=338
[2024-05-16T18:49:56.968] _slurm_rpc_submit_batch_job: JobId=388 InitPrio=10100 usec=595
[2024-05-16T18:49:57.124] _slurm_rpc_submit_batch_job: JobId=389 InitPrio=10100 usec=524
[2024-05-16T18:49:57.125] _slurm_rpc_submit_batch_job: JobId=390 InitPrio=110 usec=321
[2024-05-16T18:49:57.305] _slurm_rpc_submit_batch_job: JobId=391 InitPrio=10100 usec=597
[2024-05-16T18:49:57.306] _slurm_rpc_submit_batch_job: JobId=392 InitPrio=110 usec=344
[2024-05-16T18:49:57.463] _slurm_rpc_submit_batch_job: JobId=393 InitPrio=10100 usec=515
[2024-05-16T18:49:57.624] _slurm_rpc_submit_batch_job: JobId=394 InitPrio=10100 usec=538
[2024-05-16T18:49:57.684] _slurm_rpc_submit_batch_job: JobId=395 InitPrio=110 usec=364
[2024-05-16T18:49:57.804] _slurm_rpc_submit_batch_job: JobId=396 InitPrio=10100 usec=550
[2024-05-16T18:49:57.888] _slurm_rpc_submit_batch_job: JobId=397 InitPrio=110 usec=412
[2024-05-16T18:49:57.969] _slurm_rpc_submit_batch_job: JobId=398 InitPrio=110 usec=376
[2024-05-16T18:49:57.982] _slurm_rpc_submit_batch_job: JobId=399 InitPrio=10100 usec=542
[2024-05-16T18:49:58.115] _slurm_rpc_submit_batch_job: JobId=400 InitPrio=110 usec=422
[2024-05-16T18:49:58.160] _slurm_rpc_submit_batch_job: JobId=401 InitPrio=10100 usec=565
[2024-05-16T18:49:58.298] _slurm_rpc_submit_batch_job: JobId=402 InitPrio=110 usec=414
[2024-05-16T18:49:58.339] _slurm_rpc_submit_batch_job: JobId=403 InitPrio=10100 usec=585
[2024-05-16T18:49:58.517] _slurm_rpc_submit_batch_job: JobId=404 InitPrio=10100 usec=571
[2024-05-16T18:49:58.695] _slurm_rpc_submit_batch_job: JobId=405 InitPrio=10100 usec=531
[2024-05-16T18:49:58.709] _slurm_rpc_submit_batch_job: JobId=406 InitPrio=110 usec=409
[2024-05-16T18:49:58.873] _slurm_rpc_submit_batch_job: JobId=407 InitPrio=10100 usec=516
[2024-05-16T18:49:58.913] _slurm_rpc_submit_batch_job: JobId=408 InitPrio=110 usec=359
[2024-05-16T18:49:58.976] _slurm_rpc_submit_batch_job: JobId=409 InitPrio=110 usec=346
[2024-05-16T18:49:59.050] _slurm_rpc_submit_batch_job: JobId=410 InitPrio=10100 usec=574
[2024-05-16T18:49:59.133] _slurm_rpc_submit_batch_job: JobId=411 InitPrio=110 usec=455
[2024-05-16T18:49:59.210] _slurm_rpc_submit_batch_job: JobId=412 InitPrio=10100 usec=524
[2024-05-16T18:49:59.322] _slurm_rpc_submit_batch_job: JobId=413 InitPrio=110 usec=354
[2024-05-16T18:49:59.387] _slurm_rpc_submit_batch_job: JobId=414 InitPrio=10100 usec=528
[2024-05-16T18:49:59.548] _slurm_rpc_submit_batch_job: JobId=415 InitPrio=110 usec=383
[2024-05-16T18:49:59.565] _slurm_rpc_submit_batch_job: JobId=416 InitPrio=10100 usec=525
[2024-05-16T18:49:59.707] _slurm_rpc_submit_batch_job: JobId=417 InitPrio=110 usec=356
[2024-05-16T18:49:59.725] _slurm_rpc_submit_batch_job: JobId=418 InitPrio=10100 usec=510
[2024-05-16T18:49:59.823] _slurm_rpc_submit_batch_job: JobId=419 InitPrio=110 usec=412
[2024-05-16T18:49:59.883] _slurm_rpc_submit_batch_job: JobId=420 InitPrio=10100 usec=509
[2024-05-16T18:50:00.063] _slurm_rpc_submit_batch_job: JobId=421 InitPrio=10100 usec=543
[2024-05-16T18:50:00.103] _slurm_rpc_submit_batch_job: JobId=422 InitPrio=110 usec=1675
[2024-05-16T18:50:00.221] _slurm_rpc_submit_batch_job: JobId=423 InitPrio=10100 usec=514
[2024-05-16T18:50:00.240] _slurm_rpc_submit_batch_job: JobId=424 InitPrio=110 usec=360
[2024-05-16T18:50:00.398] _slurm_rpc_submit_batch_job: JobId=425 InitPrio=110 usec=358
[2024-05-16T18:50:00.416] _slurm_rpc_submit_batch_job: JobId=426 InitPrio=10100 usec=522
[2024-05-16T18:50:00.549] _slurm_rpc_submit_batch_job: JobId=427 InitPrio=110 usec=367
[2024-05-16T18:50:00.573] _slurm_rpc_submit_batch_job: JobId=428 InitPrio=10100 usec=566
[2024-05-16T18:50:00.830] _slurm_rpc_submit_batch_job: JobId=429 InitPrio=110 usec=369
[2024-05-16T18:50:00.968] _slurm_rpc_submit_batch_job: JobId=430 InitPrio=110 usec=394
[2024-05-16T18:50:01.263] _slurm_rpc_submit_batch_job: JobId=431 InitPrio=110 usec=378
[2024-05-16T18:50:01.406] _slurm_rpc_submit_batch_job: JobId=432 InitPrio=110 usec=371
[2024-05-16T18:50:01.538] _slurm_rpc_submit_batch_job: JobId=433 InitPrio=110 usec=386
[2024-05-16T18:50:01.837] _slurm_rpc_submit_batch_job: JobId=434 InitPrio=110 usec=359
[2024-05-16T18:50:02.086] _slurm_rpc_submit_batch_job: JobId=435 InitPrio=110 usec=627
[2024-05-16T18:50:02.116] _slurm_rpc_submit_batch_job: JobId=436 InitPrio=10100 usec=517
[2024-05-16T18:50:02.138] _slurm_rpc_submit_batch_job: JobId=437 InitPrio=110 usec=388
[2024-05-16T18:50:02.442] _slurm_rpc_submit_batch_job: JobId=438 InitPrio=110 usec=412
[2024-05-16T18:50:02.599] _slurm_rpc_submit_batch_job: JobId=439 InitPrio=110 usec=360
[2024-05-16T18:50:02.801] _slurm_rpc_submit_batch_job: JobId=440 InitPrio=110 usec=366
[2024-05-16T18:50:03.072] _slurm_rpc_submit_batch_job: JobId=441 InitPrio=110 usec=377
[2024-05-16T18:50:03.209] _slurm_rpc_submit_batch_job: JobId=442 InitPrio=110 usec=355
[2024-05-16T18:50:03.416] _slurm_rpc_submit_batch_job: JobId=443 InitPrio=110 usec=804
[2024-05-16T18:50:03.642] _slurm_rpc_submit_batch_job: JobId=444 InitPrio=10100 usec=540
[2024-05-16T18:50:03.684] _slurm_rpc_submit_batch_job: JobId=445 InitPrio=110 usec=437
[2024-05-16T18:50:03.975] _slurm_rpc_submit_batch_job: JobId=446 InitPrio=110 usec=399
[2024-05-16T18:50:04.195] _slurm_rpc_submit_batch_job: JobId=447 InitPrio=110 usec=361
[2024-05-16T18:50:04.335] _slurm_rpc_submit_batch_job: JobId=448 InitPrio=110 usec=352
[2024-05-16T18:50:04.644] _slurm_rpc_submit_batch_job: JobId=449 InitPrio=110 usec=356
[2024-05-16T18:50:04.822] _slurm_rpc_submit_batch_job: JobId=450 InitPrio=110 usec=358
[2024-05-16T18:50:04.916] _slurm_rpc_submit_batch_job: JobId=451 InitPrio=10100 usec=588
[2024-05-16T18:50:04.981] _slurm_rpc_submit_batch_job: JobId=452 InitPrio=110 usec=370
[2024-05-16T18:50:05.359] _slurm_rpc_submit_batch_job: JobId=453 InitPrio=110 usec=350
[2024-05-16T18:50:05.473] _slurm_rpc_submit_batch_job: JobId=454 InitPrio=110 usec=381
[2024-05-16T18:50:05.629] _slurm_rpc_submit_batch_job: JobId=455 InitPrio=110 usec=368
[2024-05-16T18:50:05.662] _slurm_rpc_submit_batch_job: JobId=456 InitPrio=10100 usec=550
[2024-05-16T18:50:05.882] _slurm_rpc_submit_batch_job: JobId=457 InitPrio=110 usec=345
[2024-05-16T18:50:06.105] _slurm_rpc_submit_batch_job: JobId=458 InitPrio=110 usec=358
[2024-05-16T18:50:06.216] _slurm_rpc_submit_batch_job: JobId=459 InitPrio=10100 usec=550
[2024-05-16T18:50:06.244] _slurm_rpc_submit_batch_job: JobId=460 InitPrio=110 usec=362
[2024-05-16T18:50:06.521] _slurm_rpc_submit_batch_job: JobId=461 InitPrio=110 usec=463
[2024-05-16T18:50:06.602] _slurm_rpc_submit_batch_job: JobId=462 InitPrio=10100 usec=594
[2024-05-16T18:50:06.654] _slurm_rpc_submit_batch_job: JobId=463 InitPrio=110 usec=387
[2024-05-16T18:50:06.953] _slurm_rpc_submit_batch_job: JobId=464 InitPrio=110 usec=357
[2024-05-16T18:50:07.106] _slurm_rpc_submit_batch_job: JobId=465 InitPrio=110 usec=344
[2024-05-16T18:50:07.121] _slurm_rpc_submit_batch_job: JobId=466 InitPrio=10100 usec=627
[2024-05-16T18:50:07.380] _slurm_rpc_submit_batch_job: JobId=467 InitPrio=110 usec=376
[2024-05-16T18:50:07.517] _slurm_rpc_submit_batch_job: JobId=468 InitPrio=110 usec=345
[2024-05-16T18:50:07.588] _slurm_rpc_submit_batch_job: JobId=469 InitPrio=10100 usec=656
[2024-05-16T18:50:07.812] _slurm_rpc_submit_batch_job: JobId=470 InitPrio=110 usec=372
[2024-05-16T18:50:07.988] _slurm_rpc_submit_batch_job: JobId=471 InitPrio=10100 usec=585
[2024-05-16T18:50:08.018] _slurm_rpc_submit_batch_job: JobId=472 InitPrio=110 usec=568
[2024-05-16T18:50:08.148] _slurm_rpc_submit_batch_job: JobId=473 InitPrio=110 usec=1115
[2024-05-16T18:50:08.337] _slurm_rpc_submit_batch_job: JobId=474 InitPrio=10100 usec=616
[2024-05-16T18:50:08.464] _slurm_rpc_submit_batch_job: JobId=475 InitPrio=110 usec=473
[2024-05-16T18:50:08.630] _slurm_rpc_submit_batch_job: JobId=476 InitPrio=10100 usec=612
[2024-05-16T18:50:08.640] _slurm_rpc_submit_batch_job: JobId=477 InitPrio=110 usec=1549
[2024-05-16T18:50:08.845] _slurm_rpc_submit_batch_job: JobId=478 InitPrio=110 usec=372
[2024-05-16T18:50:08.861] _slurm_rpc_submit_batch_job: JobId=479 InitPrio=10100 usec=587
[2024-05-16T18:50:09.096] _slurm_rpc_submit_batch_job: JobId=480 InitPrio=10100 usec=577
[2024-05-16T18:50:09.139] _slurm_rpc_submit_batch_job: JobId=481 InitPrio=110 usec=380
[2024-05-16T18:50:09.277] _slurm_rpc_submit_batch_job: JobId=482 InitPrio=10100 usec=629
[2024-05-16T18:50:09.471] _slurm_rpc_submit_batch_job: JobId=483 InitPrio=10100 usec=572
[2024-05-16T18:50:09.646] _slurm_rpc_submit_batch_job: JobId=484 InitPrio=10100 usec=629
[2024-05-16T18:50:09.780] _slurm_rpc_submit_batch_job: JobId=485 InitPrio=110 usec=360
[2024-05-16T18:50:09.822] _slurm_rpc_submit_batch_job: JobId=486 InitPrio=110 usec=363
[2024-05-16T18:50:09.862] _slurm_rpc_submit_batch_job: JobId=487 InitPrio=10100 usec=2948
[2024-05-16T18:50:10.005] _slurm_rpc_submit_batch_job: JobId=488 InitPrio=110 usec=391
[2024-05-16T18:50:10.172] _slurm_rpc_submit_batch_job: JobId=489 InitPrio=110 usec=387
[2024-05-16T18:50:10.252] _slurm_rpc_submit_batch_job: JobId=490 InitPrio=10100 usec=649
[2024-05-16T18:50:10.503] _slurm_rpc_submit_batch_job: JobId=491 InitPrio=110 usec=368
[2024-05-16T18:50:10.560] _slurm_rpc_submit_batch_job: JobId=492 InitPrio=10100 usec=642
[2024-05-16T18:50:10.676] _slurm_rpc_submit_batch_job: JobId=493 InitPrio=110 usec=393
[2024-05-16T18:50:10.776] _slurm_rpc_submit_batch_job: JobId=494 InitPrio=10100 usec=766
[2024-05-16T18:50:10.845] _slurm_rpc_submit_batch_job: JobId=495 InitPrio=110 usec=368
[2024-05-16T18:50:10.973] _slurm_rpc_submit_batch_job: JobId=496 InitPrio=10100 usec=604
[2024-05-16T18:50:11.022] _slurm_rpc_submit_batch_job: JobId=497 InitPrio=110 usec=402
[2024-05-16T18:50:11.189] _slurm_rpc_submit_batch_job: JobId=498 InitPrio=10100 usec=730
[2024-05-16T18:50:11.195] _slurm_rpc_submit_batch_job: JobId=499 InitPrio=110 usec=379
[2024-05-16T18:50:11.388] _slurm_rpc_submit_batch_job: JobId=500 InitPrio=10100 usec=619
[2024-05-16T18:50:11.537] _slurm_rpc_submit_batch_job: JobId=501 InitPrio=110 usec=363
[2024-05-16T18:50:11.602] _slurm_rpc_submit_batch_job: JobId=502 InitPrio=10100 usec=622
[2024-05-16T18:50:11.779] _slurm_rpc_submit_batch_job: JobId=503 InitPrio=10100 usec=615
[2024-05-16T18:50:11.809] _slurm_rpc_submit_batch_job: JobId=504 InitPrio=110 usec=345
[2024-05-16T18:50:11.915] _slurm_rpc_submit_batch_job: JobId=505 InitPrio=110 usec=372
[2024-05-16T18:50:11.974] _slurm_rpc_submit_batch_job: JobId=506 InitPrio=10100 usec=639
[2024-05-16T18:50:12.119] _slurm_rpc_submit_batch_job: JobId=507 InitPrio=110 usec=416
[2024-05-16T18:50:12.151] _slurm_rpc_submit_batch_job: JobId=508 InitPrio=10100 usec=602
[2024-05-16T18:50:12.333] _slurm_rpc_submit_batch_job: JobId=509 InitPrio=110 usec=380
[2024-05-16T18:50:12.345] _slurm_rpc_submit_batch_job: JobId=510 InitPrio=10100 usec=658
[2024-05-16T18:50:12.523] _slurm_rpc_submit_batch_job: JobId=511 InitPrio=10100 usec=689
[2024-05-16T18:50:12.533] _slurm_rpc_submit_batch_job: JobId=512 InitPrio=110 usec=385
[2024-05-16T18:50:12.719] _slurm_rpc_submit_batch_job: JobId=513 InitPrio=10100 usec=607
[2024-05-16T18:50:12.836] _slurm_rpc_submit_batch_job: JobId=514 InitPrio=110 usec=392
[2024-05-16T18:50:12.897] _slurm_rpc_submit_batch_job: JobId=515 InitPrio=10100 usec=715
[2024-05-16T18:50:13.039] _slurm_rpc_submit_batch_job: JobId=516 InitPrio=110 usec=363
[2024-05-16T18:50:13.095] _slurm_rpc_submit_batch_job: JobId=517 InitPrio=10100 usec=2381
[2024-05-16T18:50:13.289] _slurm_rpc_submit_batch_job: JobId=518 InitPrio=10100 usec=626
[2024-05-16T18:50:13.337] _slurm_rpc_submit_batch_job: JobId=519 InitPrio=110 usec=354
[2024-05-16T18:50:13.465] _slurm_rpc_submit_batch_job: JobId=520 InitPrio=10100 usec=646
[2024-05-16T18:50:13.504] _slurm_rpc_submit_batch_job: JobId=521 InitPrio=110 usec=382
[2024-05-16T18:50:13.686] _slurm_rpc_submit_batch_job: JobId=522 InitPrio=110 usec=378
[2024-05-16T18:50:13.971] _slurm_rpc_submit_batch_job: JobId=523 InitPrio=110 usec=365
[2024-05-16T18:50:14.241] _slurm_rpc_submit_batch_job: JobId=524 InitPrio=110 usec=371
[2024-05-16T18:50:14.389] _slurm_rpc_submit_batch_job: JobId=525 InitPrio=10100 usec=619
[2024-05-16T18:50:14.427] _slurm_rpc_submit_batch_job: JobId=526 InitPrio=110 usec=390
[2024-05-16T18:50:14.605] _slurm_rpc_submit_batch_job: JobId=527 InitPrio=10100 usec=624
[2024-05-16T18:50:14.622] _slurm_rpc_submit_batch_job: JobId=528 InitPrio=110 usec=402
[2024-05-16T18:50:14.792] _slurm_rpc_submit_batch_job: JobId=529 InitPrio=110 usec=344
[2024-05-16T18:50:14.801] _slurm_rpc_submit_batch_job: JobId=530 InitPrio=10100 usec=637
[2024-05-16T18:50:14.991] _slurm_rpc_submit_batch_job: JobId=531 InitPrio=110 usec=484
[2024-05-16T18:50:15.016] _slurm_rpc_submit_batch_job: JobId=532 InitPrio=10100 usec=673
[2024-05-16T18:50:15.193] _slurm_rpc_submit_batch_job: JobId=533 InitPrio=10100 usec=645
[2024-05-16T18:50:15.201] _slurm_rpc_submit_batch_job: JobId=534 InitPrio=110 usec=473
[2024-05-16T18:50:15.369] _slurm_rpc_submit_batch_job: JobId=535 InitPrio=10100 usec=645
[2024-05-16T18:50:15.419] _slurm_rpc_submit_batch_job: JobId=536 InitPrio=110 usec=393
[2024-05-16T18:50:15.546] _slurm_rpc_submit_batch_job: JobId=537 InitPrio=10100 usec=766
[2024-05-16T18:50:15.721] _slurm_rpc_submit_batch_job: JobId=538 InitPrio=10100 usec=671
[2024-05-16T18:50:15.802] _slurm_rpc_submit_batch_job: JobId=539 InitPrio=110 usec=369
[2024-05-16T18:50:15.889] _slurm_rpc_submit_batch_job: JobId=540 InitPrio=110 usec=499
[2024-05-16T18:50:15.899] _slurm_rpc_submit_batch_job: JobId=541 InitPrio=10100 usec=661
[2024-05-16T18:50:16.073] _slurm_rpc_submit_batch_job: JobId=542 InitPrio=10100 usec=687
[2024-05-16T18:50:16.113] _slurm_rpc_submit_batch_job: JobId=543 InitPrio=110 usec=405
[2024-05-16T18:50:16.232] _slurm_rpc_submit_batch_job: JobId=544 InitPrio=110 usec=374
[2024-05-16T18:50:16.410] _slurm_rpc_submit_batch_job: JobId=545 InitPrio=110 usec=356
[2024-05-16T18:50:16.619] _slurm_rpc_submit_batch_job: JobId=546 InitPrio=110 usec=359
[2024-05-16T18:50:16.877] _slurm_rpc_submit_batch_job: JobId=547 InitPrio=110 usec=364
[2024-05-16T18:50:17.116] _slurm_rpc_submit_batch_job: JobId=548 InitPrio=10100 usec=676
[2024-05-16T18:50:17.232] _slurm_rpc_submit_batch_job: JobId=549 InitPrio=110 usec=381
[2024-05-16T18:50:17.246] _slurm_rpc_submit_batch_job: JobId=550 InitPrio=110 usec=381
[2024-05-16T18:50:17.320] _slurm_rpc_submit_batch_job: JobId=551 InitPrio=10100 usec=671
[2024-05-16T18:50:17.442] _slurm_rpc_submit_batch_job: JobId=552 InitPrio=110 usec=378
[2024-05-16T18:50:17.511] _slurm_rpc_submit_batch_job: JobId=553 InitPrio=10100 usec=664
[2024-05-16T18:50:17.712] _slurm_rpc_submit_batch_job: JobId=554 InitPrio=10100 usec=715
[2024-05-16T18:50:17.847] _slurm_rpc_submit_batch_job: JobId=555 InitPrio=110 usec=355
[2024-05-16T18:50:17.886] _slurm_rpc_submit_batch_job: JobId=556 InitPrio=10100 usec=642
[2024-05-16T18:50:18.052] _slurm_rpc_submit_batch_job: JobId=557 InitPrio=110 usec=494
[2024-05-16T18:50:18.062] _slurm_rpc_submit_batch_job: JobId=558 InitPrio=10100 usec=711
[2024-05-16T18:50:18.165] _slurm_rpc_submit_batch_job: JobId=559 InitPrio=110 usec=390
[2024-05-16T18:50:18.223] _slurm_rpc_submit_batch_job: JobId=560 InitPrio=10100 usec=794
[2024-05-16T18:50:18.345] _slurm_rpc_submit_batch_job: JobId=561 InitPrio=110 usec=399
[2024-05-16T18:50:18.505] _slurm_rpc_submit_batch_job: JobId=562 InitPrio=110 usec=516
[2024-05-16T18:50:18.695] _slurm_rpc_submit_batch_job: JobId=563 InitPrio=110 usec=463
[2024-05-16T18:50:19.075] _slurm_rpc_submit_batch_job: JobId=564 InitPrio=110 usec=370
[2024-05-16T18:50:19.208] _slurm_rpc_submit_batch_job: JobId=565 InitPrio=10100 usec=860
[2024-05-16T18:50:19.210] _slurm_rpc_submit_batch_job: JobId=566 InitPrio=110 usec=410
[2024-05-16T18:50:19.385] _slurm_rpc_submit_batch_job: JobId=567 InitPrio=110 usec=440
[2024-05-16T18:50:19.423] _slurm_rpc_submit_batch_job: JobId=568 InitPrio=10100 usec=638
[2024-05-16T18:50:19.581] _slurm_rpc_submit_batch_job: JobId=569 InitPrio=110 usec=483
[2024-05-16T18:50:19.587] _slurm_rpc_submit_batch_job: JobId=570 InitPrio=10100 usec=700
[2024-05-16T18:50:19.780] _slurm_rpc_submit_batch_job: JobId=571 InitPrio=10100 usec=671
[2024-05-16T18:50:19.798] _slurm_rpc_submit_batch_job: JobId=572 InitPrio=110 usec=349
[2024-05-16T18:50:20.097] _slurm_rpc_submit_batch_job: JobId=573 InitPrio=110 usec=355
[2024-05-16T18:50:20.275] _slurm_rpc_submit_batch_job: JobId=574 InitPrio=110 usec=402
[2024-05-16T18:50:20.437] _slurm_rpc_submit_batch_job: JobId=575 InitPrio=110 usec=396
[2024-05-16T18:50:20.612] _slurm_rpc_submit_batch_job: JobId=576 InitPrio=110 usec=349
[2024-05-16T18:50:21.123] _slurm_rpc_submit_batch_job: JobId=577 InitPrio=110 usec=479
[2024-05-16T18:50:21.143] _slurm_rpc_submit_batch_job: JobId=578 InitPrio=110 usec=365
[2024-05-16T18:50:21.327] _slurm_rpc_submit_batch_job: JobId=579 InitPrio=110 usec=402
[2024-05-16T18:50:21.534] _slurm_rpc_submit_batch_job: JobId=580 InitPrio=110 usec=430
[2024-05-16T18:50:21.692] _slurm_rpc_submit_batch_job: JobId=581 InitPrio=110 usec=473
[2024-05-16T18:50:21.888] _slurm_rpc_submit_batch_job: JobId=582 InitPrio=110 usec=395
[2024-05-16T18:50:22.240] _slurm_rpc_submit_batch_job: JobId=583 InitPrio=110 usec=410
[2024-05-16T18:50:22.247] _slurm_rpc_submit_batch_job: JobId=584 InitPrio=10100 usec=715
[2024-05-16T18:50:22.455] _slurm_rpc_submit_batch_job: JobId=585 InitPrio=110 usec=384
[2024-05-16T18:50:22.501] _slurm_rpc_submit_batch_job: JobId=586 InitPrio=10100 usec=739
[2024-05-16T18:50:22.594] _slurm_rpc_submit_batch_job: JobId=587 InitPrio=110 usec=423
[2024-05-16T18:50:22.754] _slurm_rpc_submit_batch_job: JobId=588 InitPrio=110 usec=390
[2024-05-16T18:50:22.887] _slurm_rpc_submit_batch_job: JobId=589 InitPrio=10100 usec=715
[2024-05-16T18:50:22.966] _slurm_rpc_submit_batch_job: JobId=590 InitPrio=110 usec=404
[2024-05-16T18:50:23.163] _slurm_rpc_submit_batch_job: JobId=591 InitPrio=10100 usec=729
[2024-05-16T18:50:23.270] _slurm_rpc_submit_batch_job: JobId=592 InitPrio=110 usec=402
[2024-05-16T18:50:23.416] _slurm_rpc_submit_batch_job: JobId=593 InitPrio=10100 usec=695
[2024-05-16T18:50:23.448] _slurm_rpc_submit_batch_job: JobId=594 InitPrio=110 usec=417
[2024-05-16T18:50:23.623] _slurm_rpc_submit_batch_job: JobId=595 InitPrio=10100 usec=708
[2024-05-16T18:50:23.683] _slurm_rpc_submit_batch_job: JobId=596 InitPrio=110 usec=427
[2024-05-16T18:50:23.785] _slurm_rpc_submit_batch_job: JobId=597 InitPrio=110 usec=1053
[2024-05-16T18:50:23.829] _slurm_rpc_submit_batch_job: JobId=598 InitPrio=10100 usec=724
[2024-05-16T18:50:24.034] _slurm_rpc_submit_batch_job: JobId=599 InitPrio=10100 usec=677
[2024-05-16T18:50:24.125] _slurm_rpc_submit_batch_job: JobId=600 InitPrio=110 usec=400
[2024-05-16T18:50:24.280] _slurm_rpc_submit_batch_job: JobId=601 InitPrio=10100 usec=724
[2024-05-16T18:50:24.298] _slurm_rpc_submit_batch_job: JobId=602 InitPrio=110 usec=440
[2024-05-16T18:50:24.454] _slurm_rpc_submit_batch_job: JobId=603 InitPrio=110 usec=433
[2024-05-16T18:50:24.641] _slurm_rpc_submit_batch_job: JobId=604 InitPrio=110 usec=375
[2024-05-16T18:50:24.972] _slurm_rpc_submit_batch_job: JobId=605 InitPrio=110 usec=415
[2024-05-16T18:50:25.217] _slurm_rpc_submit_batch_job: JobId=606 InitPrio=110 usec=399
[2024-05-16T18:50:25.423] _slurm_rpc_submit_batch_job: JobId=607 InitPrio=110 usec=370
[2024-05-16T18:50:25.521] _slurm_rpc_submit_batch_job: JobId=608 InitPrio=110 usec=386
[2024-05-16T18:50:25.730] _slurm_rpc_submit_batch_job: JobId=609 InitPrio=110 usec=344
[2024-05-16T18:50:25.813] _slurm_rpc_submit_batch_job: JobId=610 InitPrio=10100 usec=765
[2024-05-16T18:50:25.877] _slurm_rpc_submit_batch_job: JobId=611 InitPrio=110 usec=373
[2024-05-16T18:50:26.123] _slurm_rpc_submit_batch_job: JobId=612 InitPrio=10100 usec=736
[2024-05-16T18:50:26.230] _slurm_rpc_submit_batch_job: JobId=613 InitPrio=110 usec=358
[2024-05-16T18:50:26.395] _slurm_rpc_submit_batch_job: JobId=614 InitPrio=10100 usec=690
[2024-05-16T18:50:26.447] _slurm_rpc_submit_batch_job: JobId=615 InitPrio=110 usec=375
[2024-05-16T18:50:26.611] _slurm_rpc_submit_batch_job: JobId=616 InitPrio=10100 usec=676
[2024-05-16T18:50:26.647] _slurm_rpc_submit_batch_job: JobId=617 InitPrio=110 usec=361
[2024-05-16T18:50:26.777] _slurm_rpc_submit_batch_job: JobId=618 InitPrio=110 usec=363
[2024-05-16T18:50:26.828] _slurm_rpc_submit_batch_job: JobId=619 InitPrio=10100 usec=683
[2024-05-16T18:50:26.959] _slurm_rpc_submit_batch_job: JobId=620 InitPrio=110 usec=392
[2024-05-16T18:50:27.007] _slurm_rpc_submit_batch_job: JobId=621 InitPrio=10100 usec=711
[2024-05-16T18:50:27.183] _slurm_rpc_submit_batch_job: JobId=622 InitPrio=10100 usec=748
[2024-05-16T18:50:27.372] _slurm_rpc_submit_batch_job: JobId=623 InitPrio=110 usec=610
[2024-05-16T18:50:27.380] _slurm_rpc_submit_batch_job: JobId=624 InitPrio=10100 usec=746
[2024-05-16T18:50:27.458] _slurm_rpc_submit_batch_job: JobId=625 InitPrio=110 usec=370
[2024-05-16T18:50:27.536] _slurm_rpc_submit_batch_job: JobId=626 InitPrio=10100 usec=722
[2024-05-16T18:50:27.629] _slurm_rpc_submit_batch_job: JobId=627 InitPrio=110 usec=357
[2024-05-16T18:50:27.729] _slurm_rpc_submit_batch_job: JobId=628 InitPrio=10100 usec=758
[2024-05-16T18:50:27.786] _slurm_rpc_submit_batch_job: JobId=629 InitPrio=110 usec=349
[2024-05-16T18:50:27.905] _slurm_rpc_submit_batch_job: JobId=630 InitPrio=10100 usec=715
[2024-05-16T18:50:27.984] _slurm_rpc_submit_batch_job: JobId=631 InitPrio=110 usec=364
[2024-05-16T18:50:28.084] _slurm_rpc_submit_batch_job: JobId=632 InitPrio=10100 usec=698
[2024-05-16T18:50:28.261] _slurm_rpc_submit_batch_job: JobId=633 InitPrio=110 usec=513
[2024-05-16T18:50:28.459] _slurm_rpc_submit_batch_job: JobId=634 InitPrio=110 usec=797
[2024-05-16T18:50:28.618] _slurm_rpc_submit_batch_job: JobId=635 InitPrio=110 usec=405
[2024-05-16T18:50:28.994] _slurm_rpc_submit_batch_job: JobId=636 InitPrio=110 usec=433
[2024-05-16T18:50:29.211] _slurm_rpc_submit_batch_job: JobId=637 InitPrio=110 usec=385
[2024-05-16T18:50:29.618] _slurm_rpc_submit_batch_job: JobId=638 InitPrio=110 usec=440
[2024-05-16T18:50:32.077] _slurm_rpc_submit_batch_job: JobId=639 InitPrio=110 usec=395
[2024-05-16T18:50:32.796] _slurm_rpc_submit_batch_job: JobId=640 InitPrio=110 usec=378
[2024-05-16T18:50:34.226] _slurm_rpc_submit_batch_job: JobId=641 InitPrio=110 usec=590
[2024-05-16T18:50:34.531] _slurm_rpc_submit_batch_job: JobId=642 InitPrio=110 usec=413
[2024-05-16T18:50:34.659] _slurm_rpc_submit_batch_job: JobId=643 InitPrio=110 usec=373
[2024-05-16T18:50:34.944] _slurm_rpc_submit_batch_job: JobId=644 InitPrio=110 usec=372
[2024-05-16T18:50:35.129] _slurm_rpc_submit_batch_job: JobId=645 InitPrio=110 usec=390
[2024-05-16T18:50:35.701] _slurm_rpc_submit_batch_job: JobId=646 InitPrio=110 usec=655
[2024-05-16T18:50:35.863] _slurm_rpc_submit_batch_job: JobId=647 InitPrio=110 usec=393
[2024-05-16T18:50:36.070] _slurm_rpc_submit_batch_job: JobId=648 InitPrio=110 usec=520
[2024-05-16T18:50:36.152] _slurm_rpc_submit_batch_job: JobId=649 InitPrio=110 usec=395
[2024-05-16T18:50:36.398] _slurm_rpc_submit_batch_job: JobId=650 InitPrio=110 usec=421
[2024-05-16T18:50:38.423] _slurm_rpc_submit_batch_job: JobId=651 InitPrio=110 usec=457
[2024-05-16T18:50:38.626] _slurm_rpc_submit_batch_job: JobId=652 InitPrio=110 usec=423
[2024-05-16T18:50:38.755] _slurm_rpc_submit_batch_job: JobId=653 InitPrio=110 usec=430
[2024-05-16T18:50:39.445] _slurm_rpc_submit_batch_job: JobId=654 InitPrio=110 usec=400
[2024-05-16T18:50:39.752] _slurm_rpc_submit_batch_job: JobId=655 InitPrio=110 usec=388
[2024-05-16T18:50:39.933] _slurm_rpc_submit_batch_job: JobId=656 InitPrio=110 usec=483
[2024-05-16T18:50:40.161] _slurm_rpc_submit_batch_job: JobId=657 InitPrio=110 usec=439
[2024-05-16T18:50:40.427] _slurm_rpc_submit_batch_job: JobId=658 InitPrio=110 usec=455
[2024-05-16T18:50:40.648] _slurm_rpc_submit_batch_job: JobId=659 InitPrio=110 usec=371
[2024-05-16T18:50:40.878] _slurm_rpc_submit_batch_job: JobId=660 InitPrio=110 usec=537
[2024-05-16T18:50:41.084] _slurm_rpc_submit_batch_job: JobId=661 InitPrio=110 usec=592
[2024-05-16T18:50:41.390] _slurm_rpc_submit_batch_job: JobId=662 InitPrio=110 usec=421
[2024-05-16T18:50:41.574] _slurm_rpc_submit_batch_job: JobId=663 InitPrio=110 usec=427
[2024-05-16T18:50:41.904] _slurm_rpc_submit_batch_job: JobId=664 InitPrio=110 usec=501
[2024-05-16T18:50:41.996] _slurm_rpc_submit_batch_job: JobId=665 InitPrio=110 usec=425
[2024-05-16T18:50:43.237] _slurm_rpc_submit_batch_job: JobId=666 InitPrio=110 usec=417
[2024-05-16T18:50:43.549] _slurm_rpc_submit_batch_job: JobId=667 InitPrio=110 usec=431
[2024-05-16T18:50:44.053] _slurm_rpc_submit_batch_job: JobId=668 InitPrio=110 usec=429
[2024-05-16T18:50:45.137] _slurm_rpc_submit_batch_job: JobId=669 InitPrio=110 usec=401
[2024-05-16T18:50:45.896] _slurm_rpc_submit_batch_job: JobId=670 InitPrio=110 usec=388
[2024-05-16T18:50:46.100] _slurm_rpc_submit_batch_job: JobId=671 InitPrio=110 usec=392
[2024-05-16T18:50:46.417] _slurm_rpc_submit_batch_job: JobId=672 InitPrio=110 usec=411
[2024-05-16T18:50:46.622] _slurm_rpc_submit_batch_job: JobId=673 InitPrio=110 usec=396
[2024-05-16T18:50:46.919] _slurm_rpc_submit_batch_job: JobId=674 InitPrio=110 usec=386
[2024-05-16T18:50:47.113] _slurm_rpc_submit_batch_job: JobId=675 InitPrio=110 usec=662
[2024-05-16T18:50:47.430] _slurm_rpc_submit_batch_job: JobId=676 InitPrio=110 usec=376
[2024-05-16T18:50:47.588] _slurm_rpc_submit_batch_job: JobId=677 InitPrio=110 usec=378
[2024-05-16T18:50:47.942] _slurm_rpc_submit_batch_job: JobId=678 InitPrio=110 usec=409
[2024-05-16T18:50:48.149] _slurm_rpc_submit_batch_job: JobId=679 InitPrio=110 usec=386
[2024-05-16T18:50:48.316] _slurm_rpc_submit_batch_job: JobId=680 InitPrio=110 usec=408
[2024-05-16T18:50:48.662] _slurm_rpc_submit_batch_job: JobId=681 InitPrio=110 usec=378
[2024-05-16T18:50:48.968] _slurm_rpc_submit_batch_job: JobId=682 InitPrio=110 usec=654
[2024-05-16T18:50:49.070] _slurm_rpc_submit_batch_job: JobId=683 InitPrio=110 usec=392
[2024-05-16T18:50:49.287] _slurm_rpc_submit_batch_job: JobId=684 InitPrio=110 usec=392
[2024-05-16T18:50:49.580] _slurm_rpc_submit_batch_job: JobId=685 InitPrio=110 usec=378
[2024-05-16T18:50:49.757] _slurm_rpc_submit_batch_job: JobId=686 InitPrio=110 usec=411
[2024-05-16T18:50:50.093] _slurm_rpc_submit_batch_job: JobId=687 InitPrio=110 usec=387
[2024-05-16T18:50:50.297] _slurm_rpc_submit_batch_job: JobId=688 InitPrio=110 usec=382
[2024-05-16T18:50:50.810] _slurm_rpc_submit_batch_job: JobId=689 InitPrio=110 usec=377
[2024-05-16T18:50:51.012] _slurm_rpc_submit_batch_job: JobId=690 InitPrio=110 usec=500
[2024-05-16T18:50:51.220] _slurm_rpc_submit_batch_job: JobId=691 InitPrio=110 usec=451
[2024-05-16T18:50:51.527] _slurm_rpc_submit_batch_job: JobId=692 InitPrio=110 usec=441
[2024-05-16T18:50:51.745] _slurm_rpc_submit_batch_job: JobId=693 InitPrio=110 usec=421
[2024-05-16T18:50:51.937] _slurm_rpc_submit_batch_job: JobId=694 InitPrio=110 usec=374
[2024-05-16T18:50:52.235] _slurm_rpc_submit_batch_job: JobId=695 InitPrio=110 usec=465
[2024-05-16T18:50:52.495] _slurm_rpc_submit_batch_job: JobId=696 InitPrio=110 usec=432
[2024-05-16T18:50:52.856] _slurm_rpc_submit_batch_job: JobId=697 InitPrio=110 usec=619
[2024-05-16T18:50:53.065] _slurm_rpc_submit_batch_job: JobId=698 InitPrio=110 usec=404
[2024-05-16T18:50:53.367] _slurm_rpc_submit_batch_job: JobId=699 InitPrio=110 usec=394
[2024-05-16T18:50:53.675] _slurm_rpc_submit_batch_job: JobId=700 InitPrio=110 usec=594
[2024-05-16T18:51:44.271] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=209 uid 1003
[2024-05-16T18:51:44.282] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=213 uid 1003
[2024-05-16T18:51:44.282] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=207 uid 1003
[2024-05-16T18:51:44.283] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=203 uid 1003
[2024-05-16T18:51:44.283] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=211 uid 1003
[2024-05-16T18:51:44.283] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=205 uid 1003
[2024-05-16T18:51:44.283] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=200 uid 1003
[2024-05-16T18:51:44.283] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=214 uid 1003
[2024-05-16T18:51:44.283] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=215 uid 1003
[2024-05-16T18:51:44.283] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=212 uid 1003
[2024-05-16T18:51:44.326] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=216 uid 1003
[2024-05-16T18:51:44.326] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=217 uid 1003
[2024-05-16T18:51:44.335] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=218 uid 1003
[2024-05-16T18:51:44.358] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=220 uid 1003
[2024-05-16T18:51:44.367] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=222 uid 1003
[2024-05-16T18:51:44.374] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=221 uid 1003
[2024-05-16T18:51:44.374] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=224 uid 1003
[2024-05-16T18:51:44.385] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=226 uid 1003
[2024-05-16T18:51:44.391] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=227 uid 1003
[2024-05-16T18:51:44.401] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=229 uid 1003
[2024-05-16T18:51:44.408] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=231 uid 1003
[2024-05-16T18:51:44.422] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=233 uid 1003
[2024-05-16T18:51:44.435] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=235 uid 1003
[2024-05-16T18:51:44.463] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=239 uid 1003
[2024-05-16T18:51:44.473] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=237 uid 1003
[2024-05-16T18:51:44.489] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=244 uid 1003
[2024-05-16T18:51:44.489] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=242 uid 1003
[2024-05-16T18:51:44.490] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=247 uid 1003
[2024-05-16T18:51:44.499] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=249 uid 1003
[2024-05-16T18:51:44.506] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=251 uid 1003
[2024-05-16T18:51:44.520] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=253 uid 1003
[2024-05-16T18:51:44.528] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=256 uid 1003
[2024-05-16T18:51:44.536] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=258 uid 1003
[2024-05-16T18:51:44.543] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=261 uid 1003
[2024-05-16T18:51:44.551] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=263 uid 1003
[2024-05-16T18:51:44.559] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=265 uid 1003
[2024-05-16T18:51:44.575] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=268 uid 1003
[2024-05-16T18:51:44.575] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=267 uid 1003
[2024-05-16T18:51:44.582] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=269 uid 1003
[2024-05-16T18:51:44.589] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=270 uid 1003
[2024-05-16T18:51:44.598] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=271 uid 1003
[2024-05-16T18:51:44.605] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=272 uid 1003
[2024-05-16T18:51:44.612] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=273 uid 1003
[2024-05-16T18:51:44.621] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=274 uid 1003
[2024-05-16T18:51:44.629] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=275 uid 1003
[2024-05-16T18:51:44.636] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=277 uid 1003
[2024-05-16T18:51:44.646] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=278 uid 1003
[2024-05-16T18:51:44.653] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=280 uid 1003
[2024-05-16T18:51:44.664] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=281 uid 1003
[2024-05-16T18:51:44.671] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=283 uid 1003
[2024-05-16T18:51:44.681] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=285 uid 1003
[2024-05-16T18:51:44.691] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=287 uid 1003
[2024-05-16T18:51:44.702] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=289 uid 1003
[2024-05-16T18:51:44.710] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=291 uid 1003
[2024-05-16T18:51:44.719] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=295 uid 1003
[2024-05-16T18:51:44.726] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=298 uid 1003
[2024-05-16T18:51:44.734] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=300 uid 1003
[2024-05-16T18:51:44.741] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=303 uid 1003
[2024-05-16T18:51:44.748] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=306 uid 1003
[2024-05-16T18:51:44.756] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=308 uid 1003
[2024-05-16T18:51:44.772] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=309 uid 1003
[2024-05-16T18:51:44.772] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=310 uid 1003
[2024-05-16T18:51:44.779] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=311 uid 1003
[2024-05-16T18:51:44.797] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=312 uid 1003
[2024-05-16T18:51:44.797] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=314 uid 1003
[2024-05-16T18:51:44.804] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=315 uid 1003
[2024-05-16T18:51:44.812] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=316 uid 1003
[2024-05-16T18:51:44.819] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=317 uid 1003
[2024-05-16T18:51:44.826] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=319 uid 1003
[2024-05-16T18:51:44.837] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=321 uid 1003
[2024-05-16T18:51:44.847] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=323 uid 1003
[2024-05-16T18:51:44.854] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=325 uid 1003
[2024-05-16T18:51:44.862] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=327 uid 1003
[2024-05-16T18:51:44.872] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=329 uid 1003
[2024-05-16T18:51:44.879] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=331 uid 1003
[2024-05-16T18:51:44.889] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=333 uid 1003
[2024-05-16T18:51:44.896] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=335 uid 1003
[2024-05-16T18:51:44.907] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=336 uid 1003
[2024-05-16T18:51:44.914] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=337 uid 1003
[2024-05-16T18:51:44.924] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=338 uid 1003
[2024-05-16T18:51:44.932] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=339 uid 1003
[2024-05-16T18:51:44.942] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=340 uid 1003
[2024-05-16T18:51:44.963] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=341 uid 1003
[2024-05-16T18:51:44.971] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=342 uid 1003
[2024-05-16T18:51:44.979] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=343 uid 1003
[2024-05-16T18:51:44.988] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=344 uid 1003
[2024-05-16T18:51:44.997] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=345 uid 1003
[2024-05-16T18:51:45.009] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=346 uid 1003
[2024-05-16T18:51:45.016] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=347 uid 1003
[2024-05-16T18:51:45.028] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=348 uid 1003
[2024-05-16T18:51:45.039] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=349 uid 1003
[2024-05-16T18:51:45.047] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=350 uid 1003
[2024-05-16T18:51:45.056] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=351 uid 1003
[2024-05-16T18:51:45.076] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=352 uid 1003
[2024-05-16T18:51:45.103] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=353 uid 1003
[2024-05-16T18:51:45.103] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=355 uid 1003
[2024-05-16T18:51:45.110] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=356 uid 1003
[2024-05-16T18:51:45.118] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=357 uid 1003
[2024-05-16T18:51:45.126] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=358 uid 1003
[2024-05-16T18:51:45.151] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=361 uid 1003
[2024-05-16T18:51:45.151] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=360 uid 1003
[2024-05-16T18:51:45.163] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=362 uid 1003
[2024-05-16T18:51:45.175] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=365 uid 1003
[2024-05-16T18:51:45.209] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=368 uid 1003
[2024-05-16T18:51:45.209] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=366 uid 1003
[2024-05-16T18:51:45.209] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=369 uid 1003
[2024-05-16T18:51:45.221] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=370 uid 1003
[2024-05-16T18:51:45.229] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=372 uid 1003
[2024-05-16T18:51:45.240] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=373 uid 1003
[2024-05-16T18:51:45.251] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=375 uid 1003
[2024-05-16T18:51:45.260] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=376 uid 1003
[2024-05-16T18:51:45.267] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=377 uid 1003
[2024-05-16T18:51:45.284] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=379 uid 1003
[2024-05-16T18:51:45.297] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=380 uid 1003
[2024-05-16T18:51:45.307] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=382 uid 1003
[2024-05-16T18:51:45.314] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=383 uid 1003
[2024-05-16T18:51:45.331] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=386 uid 1003
[2024-05-16T18:51:45.340] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=387 uid 1003
[2024-05-16T18:51:45.347] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=390 uid 1003
[2024-05-16T18:51:45.357] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=392 uid 1003
[2024-05-16T18:51:45.365] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=395 uid 1003
[2024-05-16T18:51:45.373] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=397 uid 1003
[2024-05-16T18:51:45.380] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=398 uid 1003
[2024-05-16T18:51:45.395] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=402 uid 1003
[2024-05-16T18:51:45.395] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=400 uid 1003
[2024-05-16T18:51:45.402] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=406 uid 1003
[2024-05-16T18:51:45.412] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=408 uid 1003
[2024-05-16T18:51:45.422] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=409 uid 1003
[2024-05-16T18:51:45.432] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=411 uid 1003
[2024-05-16T18:51:45.443] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=413 uid 1003
[2024-05-16T18:51:45.452] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=415 uid 1003
[2024-05-16T18:51:45.464] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=417 uid 1003
[2024-05-16T18:51:45.472] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=419 uid 1003
[2024-05-16T18:51:45.479] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=422 uid 1003
[2024-05-16T18:51:45.486] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=424 uid 1003
[2024-05-16T18:51:45.493] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=425 uid 1003
[2024-05-16T18:51:45.501] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=427 uid 1003
[2024-05-16T18:51:45.518] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=429 uid 1003
[2024-05-16T18:51:45.525] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=430 uid 1003
[2024-05-16T18:51:45.533] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=431 uid 1003
[2024-05-16T18:51:45.542] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=432 uid 1003
[2024-05-16T18:51:45.549] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=433 uid 1003
[2024-05-16T18:51:45.556] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=434 uid 1003
[2024-05-16T18:51:45.564] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=435 uid 1003
[2024-05-16T18:51:45.571] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=437 uid 1003
[2024-05-16T18:51:45.579] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=438 uid 1003
[2024-05-16T18:51:45.588] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=439 uid 1003
[2024-05-16T18:51:45.596] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=440 uid 1003
[2024-05-16T18:51:45.603] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=441 uid 1003
[2024-05-16T18:51:45.610] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=442 uid 1003
[2024-05-16T18:51:45.617] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=443 uid 1003
[2024-05-16T18:51:45.627] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=445 uid 1003
[2024-05-16T18:51:45.634] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=446 uid 1003
[2024-05-16T18:51:45.642] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=447 uid 1003
[2024-05-16T18:51:45.650] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=448 uid 1003
[2024-05-16T18:51:45.664] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=449 uid 1003
[2024-05-16T18:51:45.682] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=450 uid 1003
[2024-05-16T18:51:45.716] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=453 uid 1003
[2024-05-16T18:51:45.716] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=452 uid 1003
[2024-05-16T18:51:45.732] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=454 uid 1003
[2024-05-16T18:51:45.749] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=455 uid 1003
[2024-05-16T18:51:45.759] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=457 uid 1003
[2024-05-16T18:51:45.767] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=458 uid 1003
[2024-05-16T18:51:45.782] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=460 uid 1003
[2024-05-16T18:51:45.791] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=461 uid 1003
[2024-05-16T18:51:45.801] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=463 uid 1003
[2024-05-16T18:51:45.809] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=464 uid 1003
[2024-05-16T18:51:45.816] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=465 uid 1003
[2024-05-16T18:51:45.823] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=467 uid 1003
[2024-05-16T18:51:45.853] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=468 uid 1003
[2024-05-16T18:51:45.853] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=470 uid 1003
[2024-05-16T18:51:45.861] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=472 uid 1003
[2024-05-16T18:51:45.868] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=473 uid 1003
[2024-05-16T18:51:45.882] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=475 uid 1003
[2024-05-16T18:51:45.889] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=477 uid 1003
[2024-05-16T18:51:45.896] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=478 uid 1003
[2024-05-16T18:51:45.904] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=481 uid 1003
[2024-05-16T18:51:45.911] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=485 uid 1003
[2024-05-16T18:51:45.919] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=486 uid 1003
[2024-05-16T18:51:45.927] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=488 uid 1003
[2024-05-16T18:51:45.934] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=489 uid 1003
[2024-05-16T18:51:45.941] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=491 uid 1003
[2024-05-16T18:51:45.950] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=493 uid 1003
[2024-05-16T18:51:45.958] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=495 uid 1003
[2024-05-16T18:51:45.967] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=497 uid 1003
[2024-05-16T18:51:45.975] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=499 uid 1003
[2024-05-16T18:51:45.982] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=501 uid 1003
[2024-05-16T18:51:45.991] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=504 uid 1003
[2024-05-16T18:51:46.011] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=505 uid 1003
[2024-05-16T18:51:46.023] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=507 uid 1003
[2024-05-16T18:51:46.043] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=509 uid 1003
[2024-05-16T18:51:46.105] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=512 uid 1003
[2024-05-16T18:51:46.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=514 uid 1003
[2024-05-16T18:51:46.122] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=516 uid 1003
[2024-05-16T18:51:46.149] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=519 uid 1003
[2024-05-16T18:51:46.165] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=523 uid 1003
[2024-05-16T18:51:46.174] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=524 uid 1003
[2024-05-16T18:51:46.182] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=526 uid 1003
[2024-05-16T18:51:46.212] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=521 uid 1003
[2024-05-16T18:51:46.212] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=522 uid 1003
[2024-05-16T18:51:46.227] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=528 uid 1003
[2024-05-16T18:51:46.227] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=529 uid 1003
[2024-05-16T18:51:46.237] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=531 uid 1003
[2024-05-16T18:51:46.287] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=534 uid 1003
[2024-05-16T18:51:46.287] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=536 uid 1003
[2024-05-16T18:51:46.304] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=539 uid 1003
[2024-05-16T18:51:46.313] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=540 uid 1003
[2024-05-16T18:51:46.336] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=543 uid 1003
[2024-05-16T18:51:46.346] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=544 uid 1003
[2024-05-16T18:51:46.362] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=545 uid 1003
[2024-05-16T18:51:46.380] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=546 uid 1003
[2024-05-16T18:51:46.380] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=547 uid 1003
[2024-05-16T18:51:46.392] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=549 uid 1003
[2024-05-16T18:51:46.404] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=550 uid 1003
[2024-05-16T18:51:46.423] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=552 uid 1003
[2024-05-16T18:51:46.430] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=555 uid 1003
[2024-05-16T18:51:46.450] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=557 uid 1003
[2024-05-16T18:51:46.459] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=559 uid 1003
[2024-05-16T18:51:46.479] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=561 uid 1003
[2024-05-16T18:51:46.494] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=562 uid 1003
[2024-05-16T18:51:46.506] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=563 uid 1003
[2024-05-16T18:51:46.523] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=564 uid 1003
[2024-05-16T18:51:46.530] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=566 uid 1003
[2024-05-16T18:51:46.537] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=567 uid 1003
[2024-05-16T18:51:46.545] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=569 uid 1003
[2024-05-16T18:51:46.552] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=572 uid 1003
[2024-05-16T18:51:46.559] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=573 uid 1003
[2024-05-16T18:51:46.567] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=574 uid 1003
[2024-05-16T18:51:46.575] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=575 uid 1003
[2024-05-16T18:51:46.584] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=576 uid 1003
[2024-05-16T18:51:46.597] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=577 uid 1003
[2024-05-16T18:51:46.607] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=578 uid 1003
[2024-05-16T18:51:46.619] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=579 uid 1003
[2024-05-16T18:51:46.627] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=580 uid 1003
[2024-05-16T18:51:46.635] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=581 uid 1003
[2024-05-16T18:51:46.645] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=582 uid 1003
[2024-05-16T18:51:46.657] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=583 uid 1003
[2024-05-16T18:51:46.666] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=585 uid 1003
[2024-05-16T18:51:46.675] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=587 uid 1003
[2024-05-16T18:51:46.684] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=588 uid 1003
[2024-05-16T18:51:46.693] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=590 uid 1003
[2024-05-16T18:51:46.700] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=592 uid 1003
[2024-05-16T18:51:46.707] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=594 uid 1003
[2024-05-16T18:51:46.716] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=596 uid 1003
[2024-05-16T18:51:46.723] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=597 uid 1003
[2024-05-16T18:51:46.730] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=600 uid 1003
[2024-05-16T18:51:46.737] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=602 uid 1003
[2024-05-16T18:51:46.745] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=603 uid 1003
[2024-05-16T18:51:46.760] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=604 uid 1003
[2024-05-16T18:51:46.760] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=605 uid 1003
[2024-05-16T18:51:46.767] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=606 uid 1003
[2024-05-16T18:51:46.775] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=607 uid 1003
[2024-05-16T18:51:46.781] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=608 uid 1003
[2024-05-16T18:51:46.789] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=609 uid 1003
[2024-05-16T18:51:46.796] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=611 uid 1003
[2024-05-16T18:51:46.805] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=613 uid 1003
[2024-05-16T18:51:46.814] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=615 uid 1003
[2024-05-16T18:51:46.826] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=617 uid 1003
[2024-05-16T18:51:46.839] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=618 uid 1003
[2024-05-16T18:51:46.847] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=620 uid 1003
[2024-05-16T18:51:46.855] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=623 uid 1003
[2024-05-16T18:51:46.862] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=625 uid 1003
[2024-05-16T18:51:46.869] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=627 uid 1003
[2024-05-16T18:51:46.885] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=629 uid 1003
[2024-05-16T18:51:46.885] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=631 uid 1003
[2024-05-16T18:51:46.892] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=633 uid 1003
[2024-05-16T18:51:46.900] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=634 uid 1003
[2024-05-16T18:51:46.907] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=635 uid 1003
[2024-05-16T18:51:46.915] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=636 uid 1003
[2024-05-16T18:51:46.925] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=637 uid 1003
[2024-05-16T18:51:46.934] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=638 uid 1003
[2024-05-16T18:51:46.941] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=639 uid 1003
[2024-05-16T18:51:46.949] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=640 uid 1003
[2024-05-16T18:51:46.967] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=641 uid 1003
[2024-05-16T18:51:46.967] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=642 uid 1003
[2024-05-16T18:51:46.975] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=643 uid 1003
[2024-05-16T18:51:46.984] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=644 uid 1003
[2024-05-16T18:51:46.992] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=645 uid 1003
[2024-05-16T18:51:47.000] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=646 uid 1003
[2024-05-16T18:51:47.008] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=647 uid 1003
[2024-05-16T18:51:47.015] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=648 uid 1003
[2024-05-16T18:51:47.023] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=649 uid 1003
[2024-05-16T18:51:47.030] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=650 uid 1003
[2024-05-16T18:51:47.038] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=651 uid 1003
[2024-05-16T18:51:47.045] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=652 uid 1003
[2024-05-16T18:51:47.053] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=653 uid 1003
[2024-05-16T18:51:47.060] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=654 uid 1003
[2024-05-16T18:51:47.067] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=655 uid 1003
[2024-05-16T18:51:47.074] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=656 uid 1003
[2024-05-16T18:51:47.082] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=657 uid 1003
[2024-05-16T18:51:47.090] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=658 uid 1003
[2024-05-16T18:51:47.098] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=659 uid 1003
[2024-05-16T18:51:47.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=660 uid 1003
[2024-05-16T18:51:47.116] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=661 uid 1003
[2024-05-16T18:51:47.124] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=662 uid 1003
[2024-05-16T18:51:47.133] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=663 uid 1003
[2024-05-16T18:51:47.141] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=664 uid 1003
[2024-05-16T18:51:47.148] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=665 uid 1003
[2024-05-16T18:51:47.155] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=666 uid 1003
[2024-05-16T18:51:47.177] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=667 uid 1003
[2024-05-16T18:51:47.184] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=668 uid 1003
[2024-05-16T18:51:47.192] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=669 uid 1003
[2024-05-16T18:51:47.199] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=670 uid 1003
[2024-05-16T18:51:47.220] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=672 uid 1003
[2024-05-16T18:51:47.220] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=671 uid 1003
[2024-05-16T18:51:47.220] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=673 uid 1003
[2024-05-16T18:51:47.227] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=674 uid 1003
[2024-05-16T18:51:47.235] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=675 uid 1003
[2024-05-16T18:51:47.243] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=676 uid 1003
[2024-05-16T18:51:47.250] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=677 uid 1003
[2024-05-16T18:51:47.258] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=678 uid 1003
[2024-05-16T18:51:47.275] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=679 uid 1003
[2024-05-16T18:51:47.275] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=680 uid 1003
[2024-05-16T18:51:47.286] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=681 uid 1003
[2024-05-16T18:51:47.295] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=682 uid 1003
[2024-05-16T18:51:47.303] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=683 uid 1003
[2024-05-16T18:51:47.310] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=684 uid 1003
[2024-05-16T18:51:47.317] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=685 uid 1003
[2024-05-16T18:51:47.324] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=686 uid 1003
[2024-05-16T18:51:47.332] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=687 uid 1003
[2024-05-16T18:51:47.339] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=688 uid 1003
[2024-05-16T18:51:47.347] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=689 uid 1003
[2024-05-16T18:51:47.355] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=690 uid 1003
[2024-05-16T18:51:47.362] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=691 uid 1003
[2024-05-16T18:51:47.371] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=692 uid 1003
[2024-05-16T18:51:47.378] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=693 uid 1003
[2024-05-16T18:51:47.385] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=694 uid 1003
[2024-05-16T18:51:47.400] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=695 uid 1003
[2024-05-16T18:51:47.400] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=696 uid 1003
[2024-05-16T18:51:47.408] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=697 uid 1003
[2024-05-16T18:51:47.414] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=698 uid 1003
[2024-05-16T18:51:47.421] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=699 uid 1003
[2024-05-16T18:51:47.431] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=700 uid 1003
[2024-05-16T18:51:58.105] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=188 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=186 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=187 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=190 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=189 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=192 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=193 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=195 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=194 uid 1002
[2024-05-16T18:51:58.106] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=191 uid 1002
[2024-05-16T18:51:58.129] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=196 uid 1002
[2024-05-16T18:51:58.143] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=197 uid 1002
[2024-05-16T18:51:58.143] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=198 uid 1002
[2024-05-16T18:51:58.154] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=199 uid 1002
[2024-05-16T18:51:58.163] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=201 uid 1002
[2024-05-16T18:51:58.171] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=202 uid 1002
[2024-05-16T18:51:58.180] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=204 uid 1002
[2024-05-16T18:51:58.189] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=206 uid 1002
[2024-05-16T18:51:58.197] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=208 uid 1002
[2024-05-16T18:51:58.205] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=210 uid 1002
[2024-05-16T18:51:58.214] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=219 uid 1002
[2024-05-16T18:51:58.222] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=223 uid 1002
[2024-05-16T18:51:58.229] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=225 uid 1002
[2024-05-16T18:51:58.243] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=228 uid 1002
[2024-05-16T18:51:58.255] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=230 uid 1002
[2024-05-16T18:51:58.263] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=232 uid 1002
[2024-05-16T18:51:58.280] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=234 uid 1002
[2024-05-16T18:51:58.289] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=236 uid 1002
[2024-05-16T18:51:58.297] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=238 uid 1002
[2024-05-16T18:51:58.304] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=240 uid 1002
[2024-05-16T18:51:58.305] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=241 uid 1002
[2024-05-16T18:51:58.323] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=243 uid 1002
[2024-05-16T18:51:58.332] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=245 uid 1002
[2024-05-16T18:51:58.340] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=246 uid 1002
[2024-05-16T18:51:58.349] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=248 uid 1002
[2024-05-16T18:51:58.357] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=250 uid 1002
[2024-05-16T18:51:58.367] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=252 uid 1002
[2024-05-16T18:51:58.377] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=254 uid 1002
[2024-05-16T18:51:58.384] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=255 uid 1002
[2024-05-16T18:51:58.392] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=257 uid 1002
[2024-05-16T18:51:58.402] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=259 uid 1002
[2024-05-16T18:51:58.413] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=260 uid 1002
[2024-05-16T18:51:58.419] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=262 uid 1002
[2024-05-16T18:51:58.427] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=264 uid 1002
[2024-05-16T18:51:58.434] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=266 uid 1002
[2024-05-16T18:51:58.442] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=276 uid 1002
[2024-05-16T18:51:58.449] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=279 uid 1002
[2024-05-16T18:51:58.458] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=282 uid 1002
[2024-05-16T18:51:58.468] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=284 uid 1002
[2024-05-16T18:51:58.478] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=286 uid 1002
[2024-05-16T18:51:58.486] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=288 uid 1002
[2024-05-16T18:51:58.496] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=290 uid 1002
[2024-05-16T18:51:58.504] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=292 uid 1002
[2024-05-16T18:51:58.515] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=293 uid 1002
[2024-05-16T18:51:58.525] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=294 uid 1002
[2024-05-16T18:51:58.532] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=296 uid 1002
[2024-05-16T18:51:58.540] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=297 uid 1002
[2024-05-16T18:51:58.547] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=299 uid 1002
[2024-05-16T18:51:58.554] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=301 uid 1002
[2024-05-16T18:51:58.561] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=302 uid 1002
[2024-05-16T18:51:58.568] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=304 uid 1002
[2024-05-16T18:51:58.590] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=305 uid 1002
[2024-05-16T18:51:58.597] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=307 uid 1002
[2024-05-16T18:51:58.604] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=313 uid 1002
[2024-05-16T18:51:58.611] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=318 uid 1002
[2024-05-16T18:51:58.619] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=320 uid 1002
[2024-05-16T18:51:58.627] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=322 uid 1002
[2024-05-16T18:51:58.636] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=324 uid 1002
[2024-05-16T18:51:58.647] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=326 uid 1002
[2024-05-16T18:51:58.656] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=328 uid 1002
[2024-05-16T18:51:58.665] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=330 uid 1002
[2024-05-16T18:51:58.672] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=332 uid 1002
[2024-05-16T18:51:58.679] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=334 uid 1002
[2024-05-16T18:51:58.686] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=354 uid 1002
[2024-05-16T18:51:58.694] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=359 uid 1002
[2024-05-16T18:51:58.705] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=363 uid 1002
[2024-05-16T18:51:58.712] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=364 uid 1002
[2024-05-16T18:51:58.721] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=367 uid 1002
[2024-05-16T18:51:58.728] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=371 uid 1002
[2024-05-16T18:51:58.735] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=374 uid 1002
[2024-05-16T18:51:58.742] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=378 uid 1002
[2024-05-16T18:51:58.749] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=381 uid 1002
[2024-05-16T18:51:58.756] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=384 uid 1002
[2024-05-16T18:51:58.774] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=385 uid 1002
[2024-05-16T18:51:58.774] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=388 uid 1002
[2024-05-16T18:51:58.783] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=389 uid 1002
[2024-05-16T18:51:58.791] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=391 uid 1002
[2024-05-16T18:51:58.800] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=393 uid 1002
[2024-05-16T18:51:58.809] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=394 uid 1002
[2024-05-16T18:51:58.816] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=396 uid 1002
[2024-05-16T18:51:58.826] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=399 uid 1002
[2024-05-16T18:51:58.834] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=401 uid 1002
[2024-05-16T18:51:58.841] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=403 uid 1002
[2024-05-16T18:51:58.848] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=404 uid 1002
[2024-05-16T18:51:58.855] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=405 uid 1002
[2024-05-16T18:51:58.863] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=407 uid 1002
[2024-05-16T18:51:58.870] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=410 uid 1002
[2024-05-16T18:51:58.877] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=412 uid 1002
[2024-05-16T18:51:58.885] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=414 uid 1002
[2024-05-16T18:51:58.892] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=416 uid 1002
[2024-05-16T18:51:58.900] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=418 uid 1002
[2024-05-16T18:51:58.908] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=420 uid 1002
[2024-05-16T18:51:58.917] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=421 uid 1002
[2024-05-16T18:51:58.926] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=423 uid 1002
[2024-05-16T18:51:58.934] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=426 uid 1002
[2024-05-16T18:51:58.943] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=428 uid 1002
[2024-05-16T18:51:58.950] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=436 uid 1002
[2024-05-16T18:51:58.958] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=444 uid 1002
[2024-05-16T18:51:58.965] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=451 uid 1002
[2024-05-16T18:51:58.972] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=456 uid 1002
[2024-05-16T18:51:58.979] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=459 uid 1002
[2024-05-16T18:51:58.986] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=462 uid 1002
[2024-05-16T18:51:58.994] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=466 uid 1002
[2024-05-16T18:51:59.001] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=469 uid 1002
[2024-05-16T18:51:59.008] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=471 uid 1002
[2024-05-16T18:51:59.016] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=474 uid 1002
[2024-05-16T18:51:59.023] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=476 uid 1002
[2024-05-16T18:51:59.030] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=479 uid 1002
[2024-05-16T18:51:59.039] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=480 uid 1002
[2024-05-16T18:51:59.047] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=482 uid 1002
[2024-05-16T18:51:59.056] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=483 uid 1002
[2024-05-16T18:51:59.057] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=484 uid 1002
[2024-05-16T18:51:59.076] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=487 uid 1002
[2024-05-16T18:51:59.085] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=490 uid 1002
[2024-05-16T18:51:59.096] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=492 uid 1002
[2024-05-16T18:51:59.104] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=494 uid 1002
[2024-05-16T18:51:59.111] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=496 uid 1002
[2024-05-16T18:51:59.118] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=498 uid 1002
[2024-05-16T18:51:59.125] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=500 uid 1002
[2024-05-16T18:51:59.132] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=502 uid 1002
[2024-05-16T18:51:59.140] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=503 uid 1002
[2024-05-16T18:51:59.147] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=506 uid 1002
[2024-05-16T18:51:59.154] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=508 uid 1002
[2024-05-16T18:51:59.161] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=510 uid 1002
[2024-05-16T18:51:59.168] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=511 uid 1002
[2024-05-16T18:51:59.176] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=513 uid 1002
[2024-05-16T18:51:59.184] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=515 uid 1002
[2024-05-16T18:51:59.193] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=517 uid 1002
[2024-05-16T18:51:59.202] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=518 uid 1002
[2024-05-16T18:51:59.211] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=520 uid 1002
[2024-05-16T18:51:59.219] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=525 uid 1002
[2024-05-16T18:51:59.227] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=527 uid 1002
[2024-05-16T18:51:59.234] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=530 uid 1002
[2024-05-16T18:51:59.242] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=532 uid 1002
[2024-05-16T18:51:59.249] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=533 uid 1002
[2024-05-16T18:51:59.256] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=535 uid 1002
[2024-05-16T18:51:59.263] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=537 uid 1002
[2024-05-16T18:51:59.270] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=538 uid 1002
[2024-05-16T18:51:59.277] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=541 uid 1002
[2024-05-16T18:51:59.284] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=542 uid 1002
[2024-05-16T18:51:59.291] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=548 uid 1002
[2024-05-16T18:51:59.300] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=551 uid 1002
[2024-05-16T18:51:59.307] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=553 uid 1002
[2024-05-16T18:51:59.315] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=554 uid 1002
[2024-05-16T18:51:59.324] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=556 uid 1002
[2024-05-16T18:51:59.333] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=558 uid 1002
[2024-05-16T18:51:59.341] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=560 uid 1002
[2024-05-16T18:51:59.350] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=565 uid 1002
[2024-05-16T18:51:59.359] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=568 uid 1002
[2024-05-16T18:51:59.366] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=570 uid 1002
[2024-05-16T18:51:59.373] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=571 uid 1002
[2024-05-16T18:51:59.380] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=584 uid 1002
[2024-05-16T18:51:59.388] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=586 uid 1002
[2024-05-16T18:51:59.395] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=589 uid 1002
[2024-05-16T18:51:59.402] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=591 uid 1002
[2024-05-16T18:51:59.409] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=593 uid 1002
[2024-05-16T18:51:59.429] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=595 uid 1002
[2024-05-16T18:51:59.438] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=598 uid 1002
[2024-05-16T18:51:59.445] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=599 uid 1002
[2024-05-16T18:51:59.452] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=601 uid 1002
[2024-05-16T18:51:59.460] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=610 uid 1002
[2024-05-16T18:51:59.468] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=612 uid 1002
[2024-05-16T18:51:59.477] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=614 uid 1002
[2024-05-16T18:51:59.486] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=616 uid 1002
[2024-05-16T18:51:59.495] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=619 uid 1002
[2024-05-16T18:51:59.503] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=621 uid 1002
[2024-05-16T18:51:59.511] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=622 uid 1002
[2024-05-16T18:51:59.518] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=624 uid 1002
[2024-05-16T18:51:59.525] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=626 uid 1002
[2024-05-16T18:51:59.535] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=628 uid 1002
[2024-05-16T18:51:59.542] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=630 uid 1002
[2024-05-16T18:51:59.549] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=632 uid 1002
[2024-05-16T18:51:59.620] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=185 uid 1002
[2024-05-16T18:52:07.296] _slurm_rpc_submit_batch_job: JobId=701 InitPrio=110 usec=1341
[2024-05-16T18:52:07.821] _slurm_rpc_submit_batch_job: JobId=702 InitPrio=110 usec=1377
[2024-05-16T18:52:08.017] sched: Allocate JobId=701 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:52:08.029] sched: Allocate JobId=702 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:52:08.419] _slurm_rpc_submit_batch_job: JobId=703 InitPrio=110 usec=415
[2024-05-16T18:52:08.825] _slurm_rpc_submit_batch_job: JobId=704 InitPrio=110 usec=405
[2024-05-16T18:52:11.108] sched: Allocate JobId=703 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:52:11.117] sched: Allocate JobId=704 NodeList=compute[00-01] #CPUs=4 Partition=fair
[2024-05-16T18:52:20.650] _slurm_rpc_submit_batch_job: JobId=705 InitPrio=10100 usec=382
[2024-05-16T18:52:21.417] _slurm_rpc_submit_batch_job: JobId=706 InitPrio=10100 usec=406
[2024-05-16T18:52:22.195] _slurm_rpc_submit_batch_job: JobId=707 InitPrio=10100 usec=382
[2024-05-16T18:53:05.779] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:53:06.038] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T18:58:05.297] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T18:58:05.533] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:03:05.919] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:03:06.102] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:04:18.552] _slurm_rpc_submit_batch_job: JobId=708 InitPrio=10100 usec=379
[2024-05-16T19:04:34.322] _slurm_rpc_submit_batch_job: JobId=709 InitPrio=10100 usec=348
[2024-05-16T19:05:53.479] _slurm_rpc_submit_batch_job: JobId=710 InitPrio=10100 usec=358
[2024-05-16T19:05:55.629] _slurm_rpc_submit_batch_job: JobId=711 InitPrio=10100 usec=331
[2024-05-16T19:07:09.616] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=701 uid 0
[2024-05-16T19:07:09.628] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=703 uid 0
[2024-05-16T19:07:09.628] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=702 uid 0
[2024-05-16T19:07:09.628] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=704 uid 0
[2024-05-16T19:07:09.740] cleanup_completing: JobId=703 completion process took 62 seconds
[2024-05-16T19:07:11.512] sched: Allocate JobId=705 NodeList=compute[00-01] #CPUs=4 Partition=urgent
[2024-05-16T19:07:42.252] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=705 uid 0
[2024-05-16T19:07:42.500] sched: Allocate JobId=706 NodeList=compute[00-01] #CPUs=4 Partition=urgent
[2024-05-16T19:08:05.685] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:08:05.896] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:09:45.114] _slurm_rpc_submit_batch_job: JobId=712 InitPrio=10100 usec=358
[2024-05-16T19:09:46.444] _slurm_rpc_submit_batch_job: JobId=713 InitPrio=10100 usec=327
[2024-05-16T19:13:05.382] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:13:05.588] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:15:18.030] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=706 uid 0
[2024-05-16T19:15:18.192] sched: Allocate JobId=707 NodeList=compute[00-01] #CPUs=4 Partition=urgent
[2024-05-16T19:15:29.920] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=711 uid 0
[2024-05-16T19:15:29.920] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=712 uid 0
[2024-05-16T19:15:29.921] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=708 uid 0
[2024-05-16T19:15:29.973] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=710 uid 0
[2024-05-16T19:15:29.974] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=709 uid 0
[2024-05-16T19:15:29.974] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=713 uid 0
[2024-05-16T19:15:30.007] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=707 uid 0
[2024-05-16T19:18:05.384] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:18:05.388] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:23:05.365] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:23:05.368] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:28:05.306] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:28:05.309] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:33:06.064] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:33:06.080] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:38:05.281] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:38:05.286] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:43:06.013] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:43:06.017] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:48:05.408] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:48:05.409] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:53:05.436] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T19:53:05.447] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:58:05.332] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T19:58:05.342] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T20:03:05.283] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T20:03:05.303] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T20:08:06.066] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T20:08:06.068] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T20:13:06.038] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T20:13:06.047] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T20:18:05.420] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T20:18:05.427] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T20:23:05.280] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T20:23:05.287] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T20:28:05.269] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T20:28:05.277] error: _find_node_record(753): lookup failure for compute01.hpcnet
[2024-05-16T20:33:05.578] error: _find_node_record(753): lookup failure for compute00.hpcnet
[2024-05-16T20:33:05.579] error: _find_node_record(753): lookup failure for compute01.hpcnet
